{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxmDMK4yupqg"
      },
      "source": [
        "# Antarctic Prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4XGxDrCkeip"
      },
      "source": [
        "## Setup Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__YRa0SfPwU8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go_MtluVHTSi"
      },
      "source": [
        "## Antarctic Prompt Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfJG_APKHTSi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "antarctic_number_of_captions = 1#@param {type: 'number'}\n",
        "\n",
        "!git clone https://github.com/dzryk/antarctic-captions.git\n",
        "%cd antarctic-captions/\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip3 install gdown\n",
        "!pip3 install ftfy\n",
        "!pip3 install transformers\n",
        "#!pip3 install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "!pip install torch pytorch-lightning\n",
        "# Download models and cache\n",
        "\n",
        "!wget -m -np -c -U \"eye02\" -w 2 -P \"/content/drive/MyDrive/AI/models/antarctic-captions/\" -R \"index.html*\" \"https://the-eye.eu/public/AI/models/antarctic-captions/\"\n",
        "import argparse\n",
        "import io\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import requests\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TVTFF\n",
        "\n",
        "from CLIP import clip\n",
        "from PIL import Image\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import model\n",
        "import utils\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "# Helper functions\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def load_image(img, preprocess):\n",
        "    img = Image.open(fetch(img))\n",
        "    return img, preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "def show(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = TVTFF.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "def display_grid(imgs):\n",
        "    reshaped = [TVTFF.to_tensor(x.resize((256, 256))) for x in imgs]\n",
        "    show(make_grid(reshaped))\n",
        "    \n",
        "def clip_rescoring(args, net, candidates, x):\n",
        "    textemb = net.perceiver.encode_text(\n",
        "        clip.tokenize(candidates).to(args.device)).float()\n",
        "    textemb /= textemb.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * x @ textemb.T).softmax(dim=-1)\n",
        "    _, indices = similarity[0].topk(args.num_return_sequences)\n",
        "    return [candidates[idx] for idx in indices[0]]\n",
        "\n",
        "def loader(args):\n",
        "    cache = []\n",
        "    with open(args.textfile) as f:\n",
        "        for line in f:\n",
        "            cache.append(line.strip())\n",
        "    cache_emb = np.load(args.embfile)\n",
        "    net = utils.load_ckpt(args)\n",
        "    net.cache = cache\n",
        "    net.cache_emb = torch.tensor(cache_emb).to(args.device)\n",
        "    preprocess = clip.load(args.clip_model, jit=False)[1]\n",
        "    return net, preprocess\n",
        "    \n",
        "def caption_image(path, args, net, preprocess):\n",
        "    print('in caption_image')\n",
        "    captions = []\n",
        "    img, mat = load_image(path, preprocess)\n",
        "    table, x = utils.build_table(mat.to(device), \n",
        "                        perceiver=net.perceiver,\n",
        "                        cache=net.cache,\n",
        "                        cache_emb=net.cache_emb,\n",
        "                        topk=args.topk,\n",
        "                        return_images=True)\n",
        "    table = net.tokenizer.encode(table[0], return_tensors='pt').to(device)\n",
        "    out = net.model.generate(table,\n",
        "                            do_sample=args.do_sample,\n",
        "                            num_beams=args.num_beams,\n",
        "                            temperature=args.temperature,\n",
        "                            top_p=args.top_p,\n",
        "                            num_return_sequences=args.num_return_sequences)\n",
        "    candidates = []\n",
        "    for seq in out:\n",
        "        candidates.append(net.tokenizer.decode(seq, skip_special_tokens=True))\n",
        "    captions = clip_rescoring(args, net, candidates, x[None,:])\n",
        "    for c in captions[:args.display]:\n",
        "        print(c)\n",
        "    #display_grid([img])\n",
        "    return captions\n",
        "# Settings\n",
        "antarctic_filedir='/content/drive/MyDrive/AI/models/antarctic-captions/the-eye.eu/public/AI/models/antarctic-captions'\n",
        "antarctic_args = argparse.Namespace(\n",
        "    ckpt=f'{antarctic_filedir}/-epoch=05-vloss=2.163.ckpt',\n",
        "    textfile=f'{antarctic_filedir}/postcache.txt',\n",
        "    embfile=f'{antarctic_filedir}/postcache.npy',\n",
        "    clip_model='ViT-B/16',\n",
        "    topk=10,\n",
        "    num_return_sequences=1000,\n",
        "    num_beams=1,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    display=antarctic_number_of_captions,\n",
        "    do_sample=True,\n",
        "    device=device\n",
        ")\n",
        "# Load checkpoint and preprocessor\n",
        "antarctic_net, antarctic_preprocess = loader(antarctic_args)\n",
        "%cd ..\n",
        "\n",
        "antarctic_prompt_request_directory = '/content/drive/MyDrive/AI/antarctic_prompt/request/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anartic request loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1zMJvsaa7SZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import sys \n",
        "timeout_minutes=60 #@param {type:\"integer\"}\n",
        "#how to count the time that spends on while loop\n",
        "start_time = time.time()\n",
        "while True:\n",
        "  while len(os.listdir(antarctic_prompt_request_directory)) == 0:\n",
        "      time.sleep(1)\n",
        "      if (time.time() - start_time) > timeout_minutes*60:\n",
        "          print('timeout')\n",
        "          sys.exit()\n",
        "\n",
        "  if len(os.listdir(antarctic_prompt_request_directory)) > 0:\n",
        "    for file in os.listdir(antarctic_prompt_request_directory):\n",
        "      if file.endswith('.png'):\n",
        "        init_image = antarctic_prompt_request_directory + 'init_image.png'\n",
        "        new_antarctic_prompts = caption_image(init_image, antarctic_args, antarctic_net, antarctic_preprocess)[:antarctic_number_of_captions]\n",
        "        #create a text file in the antarctic_prompt_request_directory with the new antarctic prompts\n",
        "        #join new_anarctic_prompts with a ; and print it\n",
        "        print(';'.join(new_antarctic_prompts))\n",
        "        with open(antarctic_prompt_request_directory + 'antarctic_prompts.txt', 'w') as f:\n",
        "          for prompt in new_antarctic_prompts:\n",
        "            f.write(prompt)\n",
        "        os.remove(antarctic_prompt_request_directory + 'init_image.png')\n",
        "        #testing\n",
        "        start_time = time.time()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Object_detection.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
