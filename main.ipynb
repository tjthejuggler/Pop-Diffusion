{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 1.1 Check GPU Status\n",
        "import subprocess\n",
        "simple_nvidia_smi_display = False#@param {type:\"boolean\"}\n",
        "nvidiasmi_output=\"\"\n",
        "if simple_nvidia_smi_display:\n",
        "  #!nvidia-smi\n",
        "  nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8') \n",
        "  print(nvidiasmi_output) \n",
        "else:\n",
        "  #!nvidia-smi -i 0 -e 0\n",
        "  nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print(nvidiasmi_output)\n",
        "  nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print('heloooo',nvidiasmi_ecc_note)\n",
        "gpu_name=nvidiasmi_output.split('|   0')[1].split('Off')[0].strip()\n",
        "print(gpu_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjthejuggler/Pop-Diffusion/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TitleTop"
      },
      "source": [
        "# Disco Diffusion v5.3 - Now with Symmetry\n",
        "\n",
        "In case of confusion, Disco is the name of this notebook edit. The diffusion model in use is Katherine Crowson's fine-tuned 512x512 model\n",
        "\n",
        "For issues, join the [Disco Diffusion Discord](https://discord.gg/msEZBy4HxA) or message us on twitter at [@somnai_dreams](https://twitter.com/somnai_dreams) or [@gandamu](https://twitter.com/gandamu_ml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CreditsChTop"
      },
      "source": [
        "### Credits & Changelog ⬇️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Credits"
      },
      "source": [
        "#### Credits\n",
        "\n",
        "Original notebook by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). It uses either OpenAI's 256x256 unconditional ImageNet or Katherine Crowson's fine-tuned 512x512 diffusion model (https://github.com/openai/guided-diffusion), together with CLIP (https://github.com/openai/CLIP) to connect text prompts with images.\n",
        "\n",
        "Modified by Daniel Russell (https://github.com/russelldc, https://twitter.com/danielrussruss) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.\n",
        "\n",
        "Further improvements from Dango233 and nsheppard helped improve the quality of diffusion in general, and especially so for shorter runs like this notebook aims to achieve.\n",
        "\n",
        "Vark added code to load in multiple Clip models at once, which all prompts are evaluated against, which may greatly improve accuracy.\n",
        "\n",
        "The latest zoom, pan, rotation, and keyframes features were taken from Chigozie Nri's VQGAN Zoom Notebook (https://github.com/chigozienri, https://twitter.com/chigozienri)\n",
        "\n",
        "Advanced DangoCutn Cutout method is also from Dango223.\n",
        "\n",
        "--\n",
        "\n",
        "Disco:\n",
        "\n",
        "Somnai (https://twitter.com/Somnai_dreams) added Diffusion Animation techniques, QoL improvements and various implementations of tech and techniques, mostly listed in the changelog below.\n",
        "\n",
        "3D animation implementation added by Adam Letts (https://twitter.com/gandamu_ml) in collaboration with Somnai. Creation of disco.py and ongoing maintenance.\n",
        "\n",
        "Turbo feature by Chris Allen (https://twitter.com/zippy731)\n",
        "\n",
        "Improvements to ability to run on local systems, Windows support, and dependency installation by HostsServer (https://twitter.com/HostsServer)\n",
        "\n",
        "VR Mode by Tom Mason (https://twitter.com/nin_artificial)\n",
        "\n",
        "Horizontal and Vertical symmetry functionality by nsheppard. Symmetry transformation_steps by huemin (https://twitter.com/huemin_art). Symmetry integration into Disco Diffusion by Dmitrii Tochilkin (https://twitter.com/cut_pow)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LicenseTop"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "License"
      },
      "source": [
        "Licensed under the MIT License\n",
        "\n",
        "Copyright (c) 2021 Katherine Crowson \n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in\n",
        "all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "THE SOFTWARE.\n",
        "\n",
        "--\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Intel ISL (Intel Intelligent Systems Lab)\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "\n",
        "--\n",
        "\n",
        "Licensed under the MIT License\n",
        "\n",
        "Copyright (c) 2021 Maxwell Ingham\n",
        "\n",
        "Copyright (c) 2022 Adam Letts \n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in\n",
        "all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChangelogTop"
      },
      "source": [
        "#### Changelog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Changelog"
      },
      "outputs": [],
      "source": [
        "#@title <- View Changelog\n",
        "skip_for_run_all = True #@param {type: 'boolean'}\n",
        "\n",
        "if skip_for_run_all == False:\n",
        "  print(\n",
        "      '''\n",
        "  v1 Update: Oct 29th 2021 - Somnai\n",
        "\n",
        "      QoL improvements added by Somnai (@somnai_dreams), including user friendly UI, settings+prompt saving and improved google drive folder organization.\n",
        "\n",
        "  v1.1 Update: Nov 13th 2021 - Somnai\n",
        "\n",
        "      Now includes sizing options, intermediate saves and fixed image prompts and perlin inits. unexposed batch option since it doesn't work\n",
        "\n",
        "  v2 Update: Nov 22nd 2021 - Somnai\n",
        "\n",
        "      Initial addition of Katherine Crowson's Secondary Model Method (https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR)\n",
        "\n",
        "      Noticed settings were saving with the wrong name so corrected it. Let me know if you preferred the old scheme.\n",
        "\n",
        "  v3 Update: Dec 24th 2021 - Somnai\n",
        "\n",
        "      Implemented Dango's advanced cutout method\n",
        "\n",
        "      Added SLIP models, thanks to NeuralDivergent\n",
        "\n",
        "      Fixed issue with NaNs resulting in black images, with massive help and testing from @Softology\n",
        "\n",
        "      Perlin now changes properly within batches (not sure where this perlin_regen code came from originally, but thank you)\n",
        "\n",
        "  v4 Update: Jan 2021 - Somnai\n",
        "\n",
        "      Implemented Diffusion Zooming\n",
        "\n",
        "      Added Chigozie keyframing\n",
        "\n",
        "      Made a bunch of edits to processes\n",
        "  \n",
        "  v4.1 Update: Jan  14th 2021 - Somnai\n",
        "\n",
        "      Added video input mode\n",
        "\n",
        "      Added license that somehow went missing\n",
        "\n",
        "      Added improved prompt keyframing, fixed image_prompts and multiple prompts\n",
        "\n",
        "      Improved UI\n",
        "\n",
        "      Significant under the hood cleanup and improvement\n",
        "\n",
        "      Refined defaults for each mode\n",
        "\n",
        "      Added latent-diffusion SuperRes for sharpening\n",
        "\n",
        "      Added resume run mode\n",
        "\n",
        "  v4.9 Update: Feb 5th 2022 - gandamu / Adam Letts\n",
        "\n",
        "      Added 3D\n",
        "\n",
        "      Added brightness corrections to prevent animation from steadily going dark over time\n",
        "\n",
        "  v4.91 Update: Feb 19th 2022 - gandamu / Adam Letts\n",
        "\n",
        "      Cleaned up 3D implementation and made associated args accessible via Colab UI elements\n",
        "\n",
        "  v4.92 Update: Feb 20th 2022 - gandamu / Adam Letts\n",
        "\n",
        "      Separated transform code\n",
        "\n",
        "  v5.01 Update: Mar 10th 2022 - gandamu / Adam Letts\n",
        "\n",
        "      IPython magic commands replaced by Python code\n",
        "\n",
        "  v5.1 Update: Mar 30th 2022 - zippy / Chris Allen and gandamu / Adam Letts\n",
        "\n",
        "      Integrated Turbo+Smooth features from Disco Diffusion Turbo -- just the implementation, without its defaults.\n",
        "\n",
        "      Implemented resume of turbo animations in such a way that it's now possible to resume from different batch folders and batch numbers.\n",
        "\n",
        "      3D rotation parameter units are now degrees (rather than radians)\n",
        "\n",
        "      Corrected name collision in sampling_mode (now diffusion_sampling_mode for plms/ddim, and sampling_mode for 3D transform sampling)\n",
        "\n",
        "      Added video_init_seed_continuity option to make init video animations more continuous\n",
        "\n",
        "  v5.1 Update: Apr 4th 2022 - MSFTserver aka HostsServer\n",
        "\n",
        "      Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion\n",
        "\n",
        "      Remove Super Resolution\n",
        "\n",
        "      Remove SLIP Models\n",
        "\n",
        "      Update for crossplatform support\n",
        "\n",
        "  v5.2 Update: Apr 10th 2022 - nin_artificial / Tom Mason\n",
        "\n",
        "      VR Mode\n",
        "\n",
        "  v5.3 Update: Jun 10th 2022 - nsheppard, huemin, cut_pow\n",
        "\n",
        "      Horizontal and Vertical symmetry\n",
        "\n",
        "      Addition of ViT-L/14@336px model (requires high VRAM)\n",
        "    '''\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TutorialTop"
      },
      "source": [
        "### Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiffusionSet"
      },
      "source": [
        "**Diffusion settings (Defaults are heavily outdated)**\n",
        "---\n",
        "Disco Diffusion is complex, and continually evolving with new features.  The most current documentation on on Disco Diffusion settings can be found in the unofficial guidebook:\n",
        "\n",
        "[Zippy's Disco Diffusion Cheatsheet](https://docs.google.com/document/d/1l8s7uS2dGqjztYSjPpzlmXLjl5PM3IGkRWI3IiCuK7g/edit)\n",
        "\n",
        "We also encourage users to join the [Disco Diffusion User Discord](https://discord.gg/XGZrFFCRfN) to learn from the active user community.\n",
        "\n",
        "This section below is outdated as of v2\n",
        "\n",
        "Setting | Description | Default\n",
        "--- | --- | ---\n",
        "**Your vision:**\n",
        "`text_prompts` | A description of what you'd like the machine to generate. Think of it like writing the caption below your image on a website. | N/A\n",
        "`image_prompts` | Think of these images more as a description of their contents. | N/A\n",
        "**Image quality:**\n",
        "`clip_guidance_scale`  | Controls how much the image should look like the prompt. | 1000\n",
        "`tv_scale` | Controls the smoothness of the final output. | 150\n",
        "`range_scale` | Controls how far out of range RGB values are allowed to be. | 150\n",
        "`sat_scale` | Controls how much saturation is allowed. From nshepperd's JAX notebook. | 0\n",
        "`cutn` | Controls how many crops to take from the image. | 16\n",
        "`cutn_batches` | Accumulate CLIP gradient from multiple batches of cuts. | 2\n",
        "**Init settings:**\n",
        "`init_image` | URL or local path | None\n",
        "`init_scale` | This enhances the effect of the init image, a good value is 1000 | 0\n",
        "`skip_steps` | Controls the starting point along the diffusion timesteps | 0\n",
        "`perlin_init` | Option to start with random perlin noise | False\n",
        "`perlin_mode` | ('gray', 'color') | 'mixed'\n",
        "**Advanced:**\n",
        "`skip_augs` | Controls whether to skip torchvision augmentations | False\n",
        "`randomize_class` | Controls whether the imagenet class is randomly changed each iteration | True\n",
        "`clip_denoised` | Determines whether CLIP discriminates a noisy or denoised image | False\n",
        "`clamp_grad` | Experimental: Using adaptive clip grad in the cond_fn | True\n",
        "`seed`  | Choose a random seed and print it at end of run for reproduction | random_seed\n",
        "`fuzzy_prompt` | Controls whether to add multiple noisy prompts to the prompt losses | False\n",
        "`rand_mag` | Controls the magnitude of the random noise | 0.1\n",
        "`eta` | DDIM hyperparameter | 0.5\n",
        "\n",
        "..\n",
        "\n",
        "**Model settings**\n",
        "---\n",
        "\n",
        "Setting | Description | Default\n",
        "--- | --- | ---\n",
        "**Diffusion:**\n",
        "`timestep_respacing` | Modify this value to decrease the number of timesteps. | ddim100\n",
        "`diffusion_steps` || 1000\n",
        "**Diffusion:**\n",
        "`clip_models` | Models of CLIP to load. Typically the more, the better but they all come at a hefty VRAM cost. | ViT-B/32, ViT-B/16, RN50x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SetupTop"
      },
      "source": [
        "## 1. Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrepFolders"
      },
      "outputs": [],
      "source": [
        "#@title 1.2 Prepare Folders\n",
        "import subprocess, os, sys, ipykernel\n",
        "\n",
        "def gitclone(url):\n",
        "  res = subprocess.run(['git', 'clone', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print(res)\n",
        "\n",
        "def pipi(modulestr):\n",
        "  res = subprocess.run(['pip', 'install', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print(res)\n",
        "\n",
        "def pipie(modulestr):\n",
        "  res = subprocess.run(['git', 'install', '-e', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print(res)\n",
        "\n",
        "def wget(url, outputdir):\n",
        "  res = subprocess.run(['wget', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print(res)\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"Google Colab detected. Using Google Drive.\")\n",
        "    is_colab = True\n",
        "    #@markdown If you connect your Google Drive, you can save the final image of each run on your drive.\n",
        "    google_drive = True #@param {type:\"boolean\"}\n",
        "    #@markdown Click here if you'd like to save the diffusion model checkpoint file to (and/or load from) your Google Drive:\n",
        "    save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
        "except:\n",
        "    is_colab = False\n",
        "    google_drive = False\n",
        "    save_models_to_google_drive = False\n",
        "    print(\"Google Colab not detected.\")\n",
        "\n",
        "mounted_at_beginning = False\n",
        "\n",
        "if is_colab:\n",
        "    if google_drive is True:\n",
        "        drive.mount('/content/drive')\n",
        "        mounted_at_beginning = True\n",
        "        root_path = '/content/drive/MyDrive/AI/Disco_Diffusion'\n",
        "    else:\n",
        "        root_path = '/content'\n",
        "else:\n",
        "    root_path = os.getcwd()\n",
        "\n",
        "import os\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "initDirPath = f'{root_path}/init_images'\n",
        "createPath(initDirPath)\n",
        "outDirPath = f'{root_path}/images_out'\n",
        "createPath(outDirPath)\n",
        "\n",
        "if is_colab:\n",
        "    if google_drive and not save_models_to_google_drive or not google_drive:\n",
        "        model_path = '/content/models'\n",
        "        createPath(model_path)\n",
        "    if google_drive and save_models_to_google_drive:\n",
        "        model_path = f'{root_path}/models'\n",
        "        createPath(model_path)\n",
        "else:\n",
        "    model_path = f'{root_path}/models'\n",
        "    createPath(model_path)\n",
        "\n",
        "# libraries = f'{root_path}/libraries'\n",
        "# createPath(libraries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwfk5fM_gw-2"
      },
      "source": [
        "## 4. Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwJZlhQwgw-2"
      },
      "outputs": [],
      "source": [
        "#@title ### 1.3 Install and import dependencies\n",
        "\n",
        "import pathlib, shutil, os, sys\n",
        "import time\n",
        "\n",
        "if not is_colab:\n",
        "  # If running locally, there's a good chance your env will need this in order to not crash upon np.matmul() or similar operations.\n",
        "  os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
        "\n",
        "PROJECT_DIR = os.path.abspath(os.getcwd())\n",
        "USE_ADABINS = True\n",
        "\n",
        "if is_colab:\n",
        "  if google_drive is not True:\n",
        "    root_path = f'/content'\n",
        "    model_path = '/content/models' \n",
        "else:\n",
        "  root_path = os.getcwd()\n",
        "  model_path = f'{root_path}/models'\n",
        "\n",
        "model_256_downloaded = False\n",
        "model_512_downloaded = False\n",
        "model_secondary_downloaded = False\n",
        "\n",
        "multipip_res = subprocess.run(['pip', 'install', 'lpips', 'datetime', 'timm', 'ftfy', 'einops', 'pytorch-lightning', 'omegaconf'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "print(multipip_res)\n",
        "\n",
        "if is_colab:\n",
        "  subprocess.run(['apt', 'install', 'imagemagick'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "try:\n",
        "  from CLIP import clip\n",
        "except:\n",
        "  if not os.path.exists(\"CLIP\"):\n",
        "    gitclone(\"https://github.com/openai/CLIP\")\n",
        "  sys.path.append(f'{PROJECT_DIR}/CLIP')\n",
        "\n",
        "try:\n",
        "  from guided_diffusion.script_util import create_model_and_diffusion\n",
        "except:\n",
        "  if not os.path.exists(\"guided-diffusion\"):\n",
        "    #gitclone(\"https://github.com/crowsonkb/guided-diffusion\")\n",
        "    gitclone(\"https://github.com/kostarion/guided-diffusion\")\n",
        "  sys.path.append(f'{PROJECT_DIR}/guided-diffusion')\n",
        "\n",
        "try:\n",
        "  from resize_right import resize\n",
        "except:\n",
        "  if not os.path.exists(\"ResizeRight\"):\n",
        "    gitclone(\"https://github.com/assafshocher/ResizeRight.git\")\n",
        "  sys.path.append(f'{PROJECT_DIR}/ResizeRight')\n",
        "\n",
        "try:\n",
        "  import py3d_tools\n",
        "except:\n",
        "  if not os.path.exists('pytorch3d-lite'):\n",
        "    gitclone(\"https://github.com/MSFTserver/pytorch3d-lite.git\")\n",
        "  sys.path.append(f'{PROJECT_DIR}/pytorch3d-lite')\n",
        "\n",
        "try:\n",
        "  from midas.dpt_depth import DPTDepthModel\n",
        "except:\n",
        "  if not os.path.exists('MiDaS'):\n",
        "    gitclone(\"https://github.com/isl-org/MiDaS.git\")\n",
        "  if not os.path.exists('MiDaS/midas_utils.py'):\n",
        "    shutil.move('MiDaS/utils.py', 'MiDaS/midas_utils.py')\n",
        "  if not os.path.exists(f'{model_path}/dpt_large-midas-2f21e586.pt'):\n",
        "    wget(\"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\", model_path)\n",
        "  sys.path.append(f'{PROJECT_DIR}/MiDaS')\n",
        "\n",
        "try:\n",
        "  sys.path.append(PROJECT_DIR)\n",
        "  import disco_xform_utils as dxf\n",
        "except:\n",
        "  if not os.path.exists(\"disco-diffusion\"):\n",
        "    gitclone(\"https://github.com/alembics/disco-diffusion.git\")\n",
        "  if os.path.exists('disco_xform_utils.py') is not True:\n",
        "    shutil.move('disco-diffusion/disco_xform_utils.py', 'disco_xform_utils.py')\n",
        "  sys.path.append(PROJECT_DIR)\n",
        "\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import timm\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "from glob import glob\n",
        "import json\n",
        "from types import SimpleNamespace\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "from CLIP import clip\n",
        "from resize_right import resize\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from ipywidgets import Output\n",
        "import hashlib\n",
        "from functools import partial\n",
        "if is_colab:\n",
        "  os.chdir('/content')\n",
        "  from google.colab import files\n",
        "else:\n",
        "  os.chdir(f'{PROJECT_DIR}')\n",
        "from IPython.display import Image as ipyimg\n",
        "from numpy import asarray\n",
        "from einops import rearrange, repeat\n",
        "import torch, torchvision\n",
        "import time\n",
        "from omegaconf import OmegaConf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# AdaBins stuff\n",
        "if USE_ADABINS:\n",
        "  try:\n",
        "    from infer import InferenceHelper\n",
        "  except:\n",
        "    if os.path.exists(\"AdaBins\") is not True:\n",
        "      gitclone(\"https://github.com/shariqfarooq123/AdaBins.git\")\n",
        "    if not os.path.exists(f'{PROJECT_DIR}/pretrained/AdaBins_nyu.pt'):\n",
        "      createPath(f'{PROJECT_DIR}/pretrained')\n",
        "      wget(\"https://cloudflare-ipfs.com/ipfs/Qmd2mMnDLWePKmgfS8m6ntAg4nhV5VkUyAydYBp8cWWeB7/AdaBins_nyu.pt\", f'{PROJECT_DIR}/pretrained')\n",
        "    sys.path.append(f'{PROJECT_DIR}/AdaBins')\n",
        "  from infer import InferenceHelper\n",
        "  MAX_ADABINS_AREA = 500000\n",
        "\n",
        "import torch\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', DEVICE)\n",
        "device = DEVICE # At least one of the modules expects this name..\n",
        "\n",
        "if torch.cuda.get_device_capability(DEVICE) == (8,0): ## A100 fix thanks to Emad\n",
        "  print('Disabling CUDNN for A100 gpu', file=sys.stderr)\n",
        "  torch.backends.cudnn.enabled = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygaQirgDgw-3"
      },
      "source": [
        "## 5. Define Midas functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKNK8-Ucgw-3"
      },
      "outputs": [],
      "source": [
        "#@title ### 1.4 Define Midas functions\n",
        "\n",
        "from midas.dpt_depth import DPTDepthModel\n",
        "from midas.midas_net import MidasNet\n",
        "from midas.midas_net_custom import MidasNet_small\n",
        "from midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
        "\n",
        "# Initialize MiDaS depth model.\n",
        "# It remains resident in VRAM and likely takes around 2GB VRAM.\n",
        "# You could instead initialize it for each frame (and free it after each frame) to save VRAM.. but initializing it is slow.\n",
        "default_models = {\n",
        "    \"midas_v21_small\": f\"{model_path}/midas_v21_small-70d6b9c8.pt\",\n",
        "    \"midas_v21\": f\"{model_path}/midas_v21-f6b98070.pt\",\n",
        "    \"dpt_large\": f\"{model_path}/dpt_large-midas-2f21e586.pt\",\n",
        "    \"dpt_hybrid\": f\"{model_path}/dpt_hybrid-midas-501f0c75.pt\",\n",
        "    \"dpt_hybrid_nyu\": f\"{model_path}/dpt_hybrid_nyu-2ce69ec7.pt\",}\n",
        "\n",
        "def init_midas_depth_model(midas_model_type=\"dpt_large\", optimize=True):\n",
        "    midas_model = None\n",
        "    net_w = None\n",
        "    net_h = None\n",
        "    resize_mode = None\n",
        "    normalization = None\n",
        "\n",
        "    print(f\"Initializing MiDaS '{midas_model_type}' depth model...\")\n",
        "    # load network\n",
        "    midas_model_path = default_models[midas_model_type]\n",
        "\n",
        "    if midas_model_type == \"dpt_large\": # DPT-Large\n",
        "        midas_model = DPTDepthModel(\n",
        "            path=midas_model_path,\n",
        "            backbone=\"vitl16_384\",\n",
        "            non_negative=True,\n",
        "        )\n",
        "        net_w, net_h = 384, 384\n",
        "        resize_mode = \"minimal\"\n",
        "        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    elif midas_model_type == \"dpt_hybrid\": #DPT-Hybrid\n",
        "        midas_model = DPTDepthModel(\n",
        "            path=midas_model_path,\n",
        "            backbone=\"vitb_rn50_384\",\n",
        "            non_negative=True,\n",
        "        )\n",
        "        net_w, net_h = 384, 384\n",
        "        resize_mode=\"minimal\"\n",
        "        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    elif midas_model_type == \"dpt_hybrid_nyu\": #DPT-Hybrid-NYU\n",
        "        midas_model = DPTDepthModel(\n",
        "            path=midas_model_path,\n",
        "            backbone=\"vitb_rn50_384\",\n",
        "            non_negative=True,\n",
        "        )\n",
        "        net_w, net_h = 384, 384\n",
        "        resize_mode=\"minimal\"\n",
        "        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    elif midas_model_type == \"midas_v21\":\n",
        "        midas_model = MidasNet(midas_model_path, non_negative=True)\n",
        "        net_w, net_h = 384, 384\n",
        "        resize_mode=\"upper_bound\"\n",
        "        normalization = NormalizeImage(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    elif midas_model_type == \"midas_v21_small\":\n",
        "        midas_model = MidasNet_small(midas_model_path, features=64, backbone=\"efficientnet_lite3\", exportable=True, non_negative=True, blocks={'expand': True})\n",
        "        net_w, net_h = 256, 256\n",
        "        resize_mode=\"upper_bound\"\n",
        "        normalization = NormalizeImage(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    else:\n",
        "        print(f\"midas_model_type '{midas_model_type}' not implemented\")\n",
        "        assert False\n",
        "\n",
        "    midas_transform = T.Compose(\n",
        "        [\n",
        "            Resize(\n",
        "                net_w,\n",
        "                net_h,\n",
        "                resize_target=None,\n",
        "                keep_aspect_ratio=True,\n",
        "                ensure_multiple_of=32,\n",
        "                resize_method=resize_mode,\n",
        "                image_interpolation_method=cv2.INTER_CUBIC,\n",
        "            ),\n",
        "            normalization,\n",
        "            PrepareForNet(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    midas_model.eval()\n",
        "    \n",
        "    if optimize==True:\n",
        "        if DEVICE == torch.device(\"cuda\"):\n",
        "            midas_model = midas_model.to(memory_format=torch.channels_last)  \n",
        "            midas_model = midas_model.half()\n",
        "\n",
        "    midas_model.to(DEVICE)\n",
        "\n",
        "    print(f\"MiDaS '{midas_model_type}' depth model initialized.\")\n",
        "    return midas_model, midas_transform, net_w, net_h, resize_mode, normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA2tOnQSgw-4"
      },
      "source": [
        "## **1.5 Define necessary functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMIX_enCgw-4"
      },
      "outputs": [],
      "source": [
        "# https://gist.github.com/adefossez/0646dbe9ed4005480a2407c62aac8869\n",
        "\n",
        "import py3d_tools as p3dT\n",
        "import disco_xform_utils as dx\n",
        "import os\n",
        "\n",
        "\n",
        "stats={\n",
        "  \n",
        "    \"gpu_name\":gpu_name,\n",
        "    \"frame_durations\": [],\n",
        "    \"antarctic_prompt_durations\": [],\n",
        "    \"object_detection_durations\": [],\n",
        "    \"average_frame_duration\": '0',\n",
        "    \"average_antarctic_prompt_duration\": '0',\n",
        "    \"average_object_detection_duration\": '0',\n",
        "    \"interpolation_duration\": '0',\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "  \n",
        "antarctic_prompt_durations = []\n",
        "\n",
        "def interp(t):\n",
        "    return 3 * t**2 - 2 * t ** 3\n",
        "\n",
        "def perlin(width, height, scale=10, device=None):\n",
        "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
        "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
        "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
        "    wx = 1 - interp(xs)\n",
        "    wy = 1 - interp(ys)\n",
        "    dots = 0\n",
        "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
        "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
        "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
        "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
        "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
        "\n",
        "def perlin_ms(octaves, width, height, grayscale, device=device):\n",
        "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n",
        "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
        "    for i in range(1 if grayscale else 3):\n",
        "        scale = 2 ** len(octaves)\n",
        "        oct_width = width\n",
        "        oct_height = height\n",
        "        for oct in octaves:\n",
        "            p = perlin(oct_width, oct_height, scale, device)\n",
        "            out_array[i] += p * oct\n",
        "            scale //= 2\n",
        "            oct_width *= 2\n",
        "            oct_height *= 2\n",
        "    return torch.cat(out_array)\n",
        "\n",
        "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
        "    out = perlin_ms(octaves, width, height, grayscale)\n",
        "    if grayscale:\n",
        "        out = TF.resize(size=(side_y, side_x), img=out.unsqueeze(0))\n",
        "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n",
        "    else:\n",
        "        out = out.reshape(-1, 3, out.shape[0]//3, out.shape[1])\n",
        "        out = TF.resize(size=(side_y, side_x), img=out)\n",
        "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
        "\n",
        "    out = ImageOps.autocontrast(out)\n",
        "    return out\n",
        "\n",
        "def regen_perlin():\n",
        "    if perlin_mode == 'color':\n",
        "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
        "    elif perlin_mode == 'gray':\n",
        "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
        "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "    else:\n",
        "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "\n",
        "    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "    del init2\n",
        "    return init.expand(batch_size, -1, -1, -1)\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def read_image_workaround(path):\n",
        "    \"\"\"OpenCV reads images as BGR, Pillow saves them as RGB. Work around\n",
        "    this incompatibility to avoid colour inversions.\"\"\"\n",
        "    im_tmp = cv2.imread(path)\n",
        "    return cv2.cvtColor(im_tmp, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.reshape([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.reshape([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, skip_augs=False):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.skip_augs = skip_augs\n",
        "        self.augs = T.Compose([\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomGrayscale(p=0.15),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = T.Pad(input.shape[2]//4, fill=0)(input)\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "\n",
        "        cutouts = []\n",
        "        for ch in range(self.cutn):\n",
        "            if ch > self.cutn - self.cutn//4:\n",
        "                cutout = input.clone()\n",
        "            else:\n",
        "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
        "                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n",
        "                offsety = torch.randint(0, abs(sideY - size + 1), ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "\n",
        "            if not self.skip_augs:\n",
        "                cutout = self.augs(cutout)\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "            del cutout\n",
        "\n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        return cutouts\n",
        "\n",
        "cutout_debug = False\n",
        "padargs = {}\n",
        "\n",
        "class MakeCutoutsDango(nn.Module):\n",
        "    def __init__(self, cut_size,\n",
        "                 Overview=4, \n",
        "                 InnerCrop = 0, IC_Size_Pow=0.5, IC_Grey_P = 0.2\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.Overview = Overview\n",
        "        self.InnerCrop = InnerCrop\n",
        "        self.IC_Size_Pow = IC_Size_Pow\n",
        "        self.IC_Grey_P = IC_Grey_P\n",
        "        if args.animation_mode == 'None':\n",
        "          self.augs = T.Compose([\n",
        "              T.RandomHorizontalFlip(p=0.5),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomGrayscale(p=0.1),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "          ])\n",
        "        elif args.animation_mode == 'Video Input':\n",
        "          self.augs = T.Compose([\n",
        "              T.RandomHorizontalFlip(p=0.5),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomGrayscale(p=0.15),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "          ])\n",
        "        elif  args.animation_mode == '2D' or args.animation_mode == '3D':\n",
        "          self.augs = T.Compose([\n",
        "              T.RandomHorizontalFlip(p=0.4),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomGrayscale(p=0.1),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.3),\n",
        "          ])\n",
        "          \n",
        "\n",
        "    def forward(self, input):\n",
        "        cutouts = []\n",
        "        gray = T.Grayscale(3)\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        l_size = max(sideX, sideY)\n",
        "        output_shape = [1,3,self.cut_size,self.cut_size] \n",
        "        output_shape_2 = [1,3,self.cut_size+2,self.cut_size+2]\n",
        "        pad_input = F.pad(input,((sideY-max_size)//2,(sideY-max_size)//2,(sideX-max_size)//2,(sideX-max_size)//2), **padargs)\n",
        "        cutout = resize(pad_input, out_shape=output_shape)\n",
        "\n",
        "        if self.Overview>0:\n",
        "            if self.Overview<=4:\n",
        "                if self.Overview>=1:\n",
        "                    cutouts.append(cutout)\n",
        "                if self.Overview>=2:\n",
        "                    cutouts.append(gray(cutout))\n",
        "                if self.Overview>=3:\n",
        "                    cutouts.append(TF.hflip(cutout))\n",
        "                if self.Overview==4:\n",
        "                    cutouts.append(gray(TF.hflip(cutout)))\n",
        "            else:\n",
        "                cutout = resize(pad_input, out_shape=output_shape)\n",
        "                for _ in range(self.Overview):\n",
        "                    cutouts.append(cutout)\n",
        "\n",
        "            if cutout_debug:\n",
        "                if is_colab:\n",
        "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"/content/cutout_overview0.jpg\",quality=99)\n",
        "                else:\n",
        "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"cutout_overview0.jpg\",quality=99)\n",
        "\n",
        "                              \n",
        "        if self.InnerCrop >0:\n",
        "            for i in range(self.InnerCrop):\n",
        "                size = int(torch.rand([])**self.IC_Size_Pow * (max_size - min_size) + min_size)\n",
        "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "                offsety = torch.randint(0, sideY - size + 1, ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "                if i <= int(self.IC_Grey_P * self.InnerCrop):\n",
        "                    cutout = gray(cutout)\n",
        "                cutout = resize(cutout, out_shape=output_shape)\n",
        "                cutouts.append(cutout)\n",
        "            if cutout_debug:\n",
        "                if is_colab:\n",
        "                    TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"/content/cutout_InnerCrop.jpg\",quality=99)\n",
        "                else:\n",
        "                    TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"cutout_InnerCrop.jpg\",quality=99)\n",
        "        cutouts = torch.cat(cutouts)\n",
        "        if skip_augs is not True: cutouts=self.augs(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)     \n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input):\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
        "\n",
        "stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
        "TRANSLATION_SCALE = 1.0/200.0\n",
        "\n",
        "def do_3d_step(img_filepath, frame_num, midas_model, midas_transform):\n",
        "  if args.key_frames:\n",
        "    translation_x = args.translation_x_series[frame_num]\n",
        "    translation_y = args.translation_y_series[frame_num]\n",
        "    translation_z = args.translation_z_series[frame_num]\n",
        "    rotation_3d_x = args.rotation_3d_x_series[frame_num]\n",
        "    rotation_3d_y = args.rotation_3d_y_series[frame_num]\n",
        "    rotation_3d_z = args.rotation_3d_z_series[frame_num]\n",
        "    print(\n",
        "        f'translation_x: {translation_x}',\n",
        "        f'translation_y: {translation_y}',\n",
        "        f'translation_z: {translation_z}',\n",
        "        f'rotation_3d_x: {rotation_3d_x}',\n",
        "        f'rotation_3d_y: {rotation_3d_y}',\n",
        "        f'rotation_3d_z: {rotation_3d_z}',\n",
        "    )\n",
        "\n",
        "  translate_xyz = [-translation_x*TRANSLATION_SCALE, translation_y*TRANSLATION_SCALE, -translation_z*TRANSLATION_SCALE]\n",
        "  rotate_xyz_degrees = [rotation_3d_x, rotation_3d_y, rotation_3d_z]\n",
        "  print('translation:',translate_xyz)\n",
        "  print('rotation:',rotate_xyz_degrees)\n",
        "  rotate_xyz = [math.radians(rotate_xyz_degrees[0]), math.radians(rotate_xyz_degrees[1]), math.radians(rotate_xyz_degrees[2])]\n",
        "  rot_mat = p3dT.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n",
        "  print(\"rot_mat: \" + str(rot_mat))\n",
        "  next_step_pil = dxf.transform_image_3d(img_filepath, midas_model, midas_transform, DEVICE,\n",
        "                                          rot_mat, translate_xyz, math.floor(args.near_plane_series[frame_num]), math.floor(args.far_plane_series[frame_num]),\n",
        "                                          math.floor(args.fov_series[frame_num]), padding_mode=args.padding_mode,\n",
        "                                          sampling_mode=args.sampling_mode, midas_weight=args.midas_weight_series[frame_num])\n",
        "  return next_step_pil\n",
        "\n",
        "def soft_limit(x):\n",
        "  #zippy soft limiter\n",
        "  #value from -n to n, always return a value -1<x<1\n",
        "  #soft_limiter_knee set in params = 0.97 # where does compression start?\n",
        "  soft_sign = x/abs(x)\n",
        "  soft_overage = ((abs(x)-soft_limiter_knee)+(abs(abs(x)-soft_limiter_knee)))/2\n",
        "  soft_base = abs(x)-soft_overage\n",
        "  soft_limited_x = soft_base + torch.tanh(soft_overage/(1-soft_limiter_knee))*(1-soft_limiter_knee)\n",
        "  return soft_limited_x*soft_sign\n",
        "\n",
        "def symmetry_transformation_fn(x):\n",
        "  if args.use_horizontal_symmetry:\n",
        "    [n, c, h, w] = x.size()\n",
        "    x = torch.concat((x[:, :, :, :w//2], torch.flip(x[:, :, :, :w//2], [-1])), -1)\n",
        "    print(\"horizontal symmetry applied\")\n",
        "  if args.use_vertical_symmetry:\n",
        "    [n, c, h, w] = x.size()\n",
        "    x = torch.concat((x[:, :, :h//2, :], torch.flip(x[:, :, :h//2, :], [-2])), -2)\n",
        "    print(\"vertical symmetry applied\")\n",
        "  return x\n",
        "\n",
        "def determine_zoom_scale(box_width, box_height, image_width, image_height, num_iterations):\n",
        "  if image_width/box_width > image_height/box_height:\n",
        "    small_number = box_width\n",
        "    large_number = image_width\n",
        "  else:\n",
        "      small_number = box_height\n",
        "      large_number = image_height \n",
        "  percent_increase = (large_number / small_number) ** (1/num_iterations)\n",
        "  return percent_increase\n",
        "\n",
        "def calculate_box_center_pixel(box_coordinates):\n",
        "  box_center_pixel_horizontal = (box_coordinates[0] + box_coordinates[1])//2\n",
        "  box_center_pixel_vertical = (box_coordinates[2] + box_coordinates[3])//2\n",
        "  return box_center_pixel_horizontal, box_center_pixel_vertical\n",
        "\n",
        "def request_caption_image(init_image):\n",
        "  antarctic_prompt_request_directory = '/content/drive/MyDrive/AI/antarctic_prompt/request/'\n",
        "  #write init_image to antarctic_prompt_request_directory as a png\n",
        "  cv_init_image = cv2.imread(init_image)\n",
        "  print('cv_init_image', cv_init_image)\n",
        "  cv2.imwrite(antarctic_prompt_request_directory+\"/init_image.png\", cv_init_image)\n",
        " #it doesn't like this, maybe try making it as similar to ODs as possible, maybe check the types of each and compare, maybe need to do that other cv thing before doing this\n",
        "  antarctic_prompts = []\n",
        "  antarctic_prompt_start_time=time.time()\n",
        "  while len(antarctic_prompts) == 0:\n",
        "    for file in os.listdir(antarctic_prompt_request_directory):\n",
        "      if file.endswith('.txt'):\n",
        "        #open the file and put the lines into a list called antarctic_prompts\n",
        "        with open(antarctic_prompt_request_directory +\"/\"+ file, 'r') as f:\n",
        "          antarctic_prompts = f.readlines() \n",
        "        os.remove(antarctic_prompt_request_directory +\"/antarctic_prompts.txt\")      \n",
        "      else:\n",
        "        time.sleep(1)\n",
        "  antarctic_prompt_end_time=time.time()\n",
        "  antarctic_prompt_duration_time= antarctic_prompt_end_time-antarctic_prompt_start_time\n",
        "  antarctic_prompt_durations.append(antarctic_prompt_duration_time)\n",
        "  stats['antarctic_prompt_durations'] = antarctic_prompt_durations\n",
        "  average_antarctic_prompt_duration = sum(antarctic_prompt_durations) / len(antarctic_prompt_durations)\n",
        "  stats['average_antarctic_prompt_duration'] = average_antarctic_prompt_duration                            \n",
        "  with open(f\"{batchFolder}/{batch_name}({batchNum})_stats.txt\", \"w+\") as ant: \n",
        "    json.dump(stats, ant, ensure_ascii=False, indent=4)\n",
        "  return antarctic_prompts       \n",
        "\n",
        "#todo text file should be deleted after it gets used!!!!!!!!!!!!!\n",
        "def do_run():\n",
        "  global stats\n",
        "  global widget\n",
        "  if os.path.isfile(f\"{batchFolder}/{batch_name}({batchNum})_stats.txt\"):\n",
        "    with open(f'{batchFolder}/{batch_name}({batchNum})_stats.txt') as file:\n",
        "      stats = json.load(file)\n",
        "  seed = args.seed\n",
        "  print(range(args.start_frame, args.max_frames))\n",
        "  \n",
        "  updated_antarctic_text_prompts_series = args.prompts_series\n",
        "  #list_of_rot_mat_centers = (1*args.width_height[1]//2, 1*args.width_height[0]//2) * args.max_frames\n",
        "  list_of_rot_mat_centers = [() for _ in range(args.max_frames + 30)]\n",
        "  if (args.animation_mode == \"3D\") and (args.midas_weight > 0.0):\n",
        "      midas_model, midas_transform, midas_net_w, midas_net_h, midas_resize_mode, midas_normalization = init_midas_depth_model(args.midas_depth_model)\n",
        "  frame_durations = []\n",
        "  object_detection_durations=[]\n",
        "  antarctic_prompt_durations=[]\n",
        "  average_object_detection_duration = 0\n",
        "  average_antarctic_prompt_duration = 0\n",
        "  average_frame_duration = 0\n",
        "  previous_frame_time = round(time.time() * 1000)\n",
        "  print('args.translation_x_series', args.translation_x_series)\n",
        "  print('args.translation_y_series', args.translation_y_series)\n",
        "  # translation_x_series.update(box_zoom_variable_updates['translation_x_series'])\n",
        "  # translation_y_series.update(box_zoom_variable_updates['translation_y_series'])\n",
        "  # zoom_series.update(box_zoom_variable_updates['zoom_series'])\n",
        "  list_of_rot_mat_centers = box_zoom_variable_updates['rot_mat_center_series'].copy()\n",
        "  #we should be able to use update for the dictionaries, but i think rot_mat is a list of tuples so it may need to be different\n",
        "  for frame_num in range(args.start_frame, args.max_frames):\n",
        "\n",
        "      if stop_on_next_loop:\n",
        "        break\n",
        "\n",
        "      current_frame_time = round(time.time() * 1000)\n",
        "      frame_durations.append(current_frame_time-previous_frame_time)\n",
        "      stats['frame_durations'] = frame_durations                            \n",
        "      previous_frame_time = current_frame_time\n",
        "      average_frame_duration = sum(frame_durations) / len(frame_durations)\n",
        "      stats['average_frame_duration'] = average_frame_duration                            \n",
        "      with open(f\"{batchFolder}/{batch_name}({batchNum})_stats.txt\", \"w+\") as f: \n",
        "        json.dump(stats, f, ensure_ascii=False, indent=4)\n",
        "      \n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "      # Print Frame progress if animation mode is on\n",
        "      if args.animation_mode != \"None\":\n",
        "        batchBar = tqdm(range(args.max_frames), desc =\"Frames\")\n",
        "        batchBar.n = frame_num\n",
        "        batchBar.refresh()\n",
        "\n",
        "      \n",
        "      # Inits if not video frames\n",
        "      if args.animation_mode != \"Video Input\":\n",
        "        if args.init_image in ['','none', 'None', 'NONE']:\n",
        "          init_image = None\n",
        "        else:\n",
        "          init_image = args.init_image\n",
        "        init_scale = args.init_scale\n",
        "        #skip_steps = args.skip_steps + (frame_num * 10) #tj maybe try making a notification to see if we keep coming here\n",
        "        skip_steps = 0\n",
        "      if args.animation_mode == \"2D\":\n",
        "        if args.key_frames:\n",
        "          angle = args.angle_series[frame_num]\n",
        "          zoom = args.zoom_series[frame_num]\n",
        "          object_detection_zoom = args.object_detection_zoom\n",
        "          translation_x = args.translation_x_series[frame_num]\n",
        "          translation_y = args.translation_y_series[frame_num]\n",
        "          print(\n",
        "              f'angle: {angle}',\n",
        "              f'zoom: {zoom}',\n",
        "              f'object_detection_zoom: {object_detection_zoom}',\n",
        "              f'translation_x: {translation_x}',\n",
        "              f'translation_y: {translation_y}',\n",
        "              f'translation_x_series: {translation_x_series}',\n",
        "              f'translation_y_series: {translation_y_series}',\n",
        "              f'zoom_series: {zoom_series}',\n",
        "              f'new_frames_scale_series: {new_frames_scale_series}',\n",
        "              f'new_frames_skip_steps_series: {new_frames_skip_steps_series}',\n",
        "          )\n",
        "          print('next_frame', frame_num)\n",
        "          print('box_zoom_variable_updates',box_zoom_variable_updates)\n",
        "          #todo should be able to plug in the stuff in this function to make the zoom happen\n",
        "          if os.path.exists('./prevFrame.png'):\n",
        "            print('it exists', frame_num)\n",
        "            def encode_image(filepath):\n",
        "                with open('./prevFrame.png', 'rb') as f:\n",
        "                    image_bytes = f.read()\n",
        "                encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
        "                return \"data:image/jpg;base64,\"+encoded\n",
        "            guided_zoom_widget.image = encode_image('./prevFrame.png')\n",
        "            #widget.bboxes = []\n",
        "        \n",
        "        if frame_num > 0:\n",
        "          seed += 1\n",
        "          if resume_run and frame_num == start_frame:\n",
        "            img_0 = cv2.imread(batchFolder+f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\")\n",
        "          else:\n",
        "            img_0 = cv2.imread('prevFrame.png')\n",
        "          \n",
        "          #center = list_of_rot_mat_centers[frame_num]\n",
        "          center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
        "          if len(list_of_rot_mat_centers) > frame_num:\n",
        "            center = list_of_rot_mat_centers[frame_num]\n",
        "          if frame_num in object_detection_zoom:\n",
        "            OD_box_coordinates_string = ''\n",
        "            OD_request_folder = '/content/drive/MyDrive/AI/Object_Detection/request'\n",
        "            #save img_0 to OD_request_folder\n",
        "            cv2.imwrite(OD_request_folder+f\"/{batch_name}({batchNum})_{frame_num:04}.png\", img_0)\n",
        "            object_detection_start_time= time.time()\n",
        "            while OD_box_coordinates_string == '':             \n",
        "              for file in os.listdir(OD_request_folder):\n",
        "                if file.endswith('.txt'):\n",
        "                  #get box_coordinates from file\n",
        "                  with open(OD_request_folder+'/'+file, 'r') as f:\n",
        "                    OD_box_coordinates_string = f.read()\n",
        "                  os.remove(OD_request_folder+'/'+file)\n",
        "                else:\n",
        "                  time.sleep(1)\n",
        "            #TODO COPY THIS STUFF TO BOX ZOOM CELL\n",
        "            object_detection_end_time= time.time()\n",
        "            object_detection_duration_time= object_detection_end_time-object_detection_start_time\n",
        "            object_detection_durations.append(object_detection_duration_time)\n",
        "            stats['object_detection_durations'] = object_detection_durations\n",
        "            average_object_detection_duration = sum(object_detection_durations) / len(object_detection_durations)\n",
        "            stats['average_object_detection_duration'] = average_object_detection_duration                            \n",
        "            with open(f\"{batchFolder}/{batch_name}({batchNum})_stats.txt\", \"w+\") as obj: \n",
        "              json.dump(stats, obj, ensure_ascii=False, indent=4)\n",
        "            \n",
        "\n",
        "            num_iterations = 30 #todo this will eventually be a setting\n",
        "            OD_box_coordinates_list = OD_box_coordinates_string.split(',')\n",
        "            box_coordinates = []\n",
        "            for cord in OD_box_coordinates_list:\n",
        "              box_coordinates.append(int(float(cord)))\n",
        "            box_width = abs(box_coordinates[0] - box_coordinates[1]) * (args.width_height[0]/256)\n",
        "            box_height = abs(box_coordinates[2] - box_coordinates[3]) * (args.width_height[1]/256)\n",
        "            zoom_scale = determine_zoom_scale(box_width, box_height, args.width_height[0], args.width_height[1], num_iterations)\n",
        "            box_center_x, box_center_y = calculate_box_center_pixel(box_coordinates)\n",
        "            x_translate_per_iteration = int(((args.width_height[0]//2) - box_center_x) / num_iterations)\n",
        "            y_translate_per_iteration = int(((args.width_height[1]//2) - box_center_y) / num_iterations)\n",
        "            for frame in range(frame_num, frame_num+num_iterations):\n",
        "              iteration_number = frame + 1 - frame_num\n",
        "              translation_x_series[frame] = x_translate_per_iteration\n",
        "              translation_y_series[frame] = y_translate_per_iteration\n",
        "              list_of_rot_mat_centers[frame] = (box_center_x+(iteration_number*x_translate_per_iteration), box_center_y+(iteration_number*y_translate_per_iteration))\n",
        "              zoom_series[frame] = zoom_scale\n",
        "            #check to see if center is a tuple\n",
        "            center = list_of_rot_mat_centers[frame_num]\n",
        "            print('centertype', type(center))\n",
        "            translation_x = args.translation_x_series[frame_num]\n",
        "            translation_y = args.translation_y_series[frame_num]\n",
        "            zoom = args.zoom_series[frame_num]\n",
        "            #todo eventually make the number of frames that it takes to be zoomed in be based on the future zoom series(for now it is 30)\n",
        "          \n",
        "          trans_mat = np.float32(\n",
        "              [[1, 0, translation_x],\n",
        "              [0, 1, translation_y]]\n",
        "          )\n",
        "          if len(center) == 0:\n",
        "            center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
        "          rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n",
        "          trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "          rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "          transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
        "          img_0 = cv2.warpPerspective(\n",
        "              img_0,\n",
        "              transformation_matrix,\n",
        "              (img_0.shape[1], img_0.shape[0]),\n",
        "              borderMode=cv2.BORDER_WRAP\n",
        "          )\n",
        "\n",
        "          cv2.imwrite('prevFrameScaled.png', img_0)\n",
        "          init_image = 'prevFrameScaled.png'\n",
        "          init_scale = math.floor(args.new_frames_scale_series[frame_num])\n",
        "          # skip_steps = args.calc_frames_skip_steps#tj this is overriding my skip steps i think\n",
        "          # #skip_steps = args.skip_steps + (frame_num * 10)\n",
        "          # if len(args.calc_frames_skip_steps_series) > 0:\n",
        "          #   # print('args.calc_frames_skip_steps_series', args.calc_frames_skip_steps_series)\n",
        "          #   # print('args.new_cutn_batches_series', args.new_cutn_batches_series)\n",
        "          #   # print('args.new_clip_guidance_scale_series', args.new_clip_guidance_scale_series)\n",
        "          #   # print('args.new_range_scale_series',args.new_range_scale_series)\n",
        "          skip_steps = args.calc_frames_skip_steps_series[frame_num]\n",
        "      if args.animation_mode == \"3D\":\n",
        "        if frame_num > 0:\n",
        "          seed += 1    \n",
        "          if resume_run and frame_num == start_frame:\n",
        "            img_filepath = batchFolder+f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\"\n",
        "            if turbo_mode and frame_num > turbo_preroll:\n",
        "              shutil.copyfile(img_filepath, 'oldFrameScaled.png')\n",
        "          else:\n",
        "            img_filepath = '/content/prevFrame.png' if is_colab else 'prevFrame.png'\n",
        "\n",
        "          next_step_pil = do_3d_step(img_filepath, frame_num, midas_model, midas_transform)\n",
        "          next_step_pil.save('prevFrameScaled.png')\n",
        "\n",
        "          ### Turbo mode - skip some diffusions, use 3d morph for clarity and to save time\n",
        "          if turbo_mode:\n",
        "            if frame_num == turbo_preroll: #start tracking oldframe\n",
        "              next_step_pil.save('oldFrameScaled.png')#stash for later blending          \n",
        "            elif frame_num > turbo_preroll:\n",
        "              #set up 2 warped image sequences, old & new, to blend toward new diff image\n",
        "              old_frame = do_3d_step('oldFrameScaled.png', frame_num, midas_model, midas_transform)\n",
        "              old_frame.save('oldFrameScaled.png')\n",
        "              if frame_num % int(turbo_steps) != 0: \n",
        "                print('turbo skip this frame: skipping clip diffusion steps')\n",
        "                filename = f'{args.batch_name}({args.batchNum})_{frame_num:04}.png'\n",
        "                blend_factor = ((frame_num % int(turbo_steps))+1)/int(turbo_steps)\n",
        "                print('turbo skip this frame: skipping clip diffusion steps and saving blended frame')\n",
        "                newWarpedImg = cv2.imread('prevFrameScaled.png')#this is already updated..\n",
        "                oldWarpedImg = cv2.imread('oldFrameScaled.png')\n",
        "                blendedImage = cv2.addWeighted(newWarpedImg, blend_factor, oldWarpedImg,1-blend_factor, 0.0)\n",
        "                cv2.imwrite(f'{batchFolder}/{filename}',blendedImage)\n",
        "                next_step_pil.save(f'{img_filepath}') # save it also as prev_frame to feed next iteration\n",
        "                if vr_mode:\n",
        "                  generate_eye_views(TRANSLATION_SCALE,batchFolder,filename,frame_num,midas_model, midas_transform)\n",
        "                continue\n",
        "              else:\n",
        "                #if not a skip frame, will run diffusion and need to blend.\n",
        "                oldWarpedImg = cv2.imread('prevFrameScaled.png')\n",
        "                cv2.imwrite(f'oldFrameScaled.png',oldWarpedImg)#swap in for blending later \n",
        "                print('clip/diff this frame - generate clip diff image')\n",
        "          init_image = 'prevFrameScaled.png'\n",
        "          init_scale = math.floor(args.new_frames_scale_series[frame_num])\n",
        "          skip_steps = args.calc_frames_skip_steps\n",
        "      if  args.animation_mode == \"Video Input\":\n",
        "        if not video_init_seed_continuity:\n",
        "          seed += 1\n",
        "        init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'\n",
        "        init_scale = math.floor(args.new_frames_scale_series[frame_num])\n",
        "        skip_steps = args.calc_frames_skip_steps\n",
        "      loss_values = []  \n",
        "      if seed is not None:\n",
        "          np.random.seed(seed)\n",
        "          random.seed(seed)\n",
        "          torch.manual_seed(seed)\n",
        "          torch.cuda.manual_seed_all(seed)\n",
        "          torch.backends.cudnn.deterministic = True  \n",
        "      target_embeds, weights = [], []\n",
        "      if antarctic_prompt_frames:\n",
        "        should_use_antarctic_prompt = False\n",
        "        #if \"x\" is in antarctic_prompt_frames:\n",
        "        if \"x\" in antarctic_prompt_frames and frame_num != 0:\n",
        "          #number_in_antarctic_prompt_frames is antarctic_prompt_frames without the x\n",
        "          number_in_antarctic_prompt_frames = antarctic_prompt_frames.replace(\"x\",\"\")\n",
        "          if frame_num % int(number_in_antarctic_prompt_frames) == 0:\n",
        "            should_use_antarctic_prompt = True\n",
        "        elif str(frame_num) in antarctic_prompt_frames.split(\",\"):\n",
        "          should_use_antarctic_prompt = True\n",
        "        if should_use_antarctic_prompt:\n",
        "          global updated_antarctic_text_prompts\n",
        "          #new_antarctic_prompts = caption_image(init_image, antarctic_args, antarctic_net, antarctic_preprocess)[:antarctic_number_of_captions]\n",
        "          #trying new request method\n",
        "          new_antarctic_prompts = request_caption_image(init_image)#, antarctic_args, antarctic_net, antarctic_preprocess)[:antarctic_number_of_captions]\n",
        "          #every string in new_antarctic_prompts has antarctic_pre_string and antarctic_post_string prepended and appended to it\n",
        "          for i in range(len(new_antarctic_prompts)):\n",
        "            new_antarctic_prompts[i] = antarctic_pre_string + new_antarctic_prompts[i] + antarctic_post_string          \n",
        "          updated_antarctic_text_prompts[frame_num] = new_antarctic_prompts\n",
        "          updated_antarctic_text_prompts[frame_num].extend(antarctic_additional_prompts)\n",
        "          updated_antarctic_text_prompts = dict(sorted(updated_antarctic_text_prompts.items(), key=lambda x: int(x[0])))\n",
        "          #json dump dict to file\n",
        "          with open(f'{batchFolder}/antarctic_text_prompts.txt', 'w') as fp:\n",
        "            json.dump(updated_antarctic_text_prompts, fp, ensure_ascii=False, indent=4)\n",
        "          updated_antarctic_text_prompts_series = split_prompts(updated_antarctic_text_prompts)\n",
        "        if updated_antarctic_text_prompts_series is not None and frame_num >= len(updated_antarctic_text_prompts_series):\n",
        "          frame_prompt = updated_antarctic_text_prompts_series[-1]\n",
        "        elif updated_antarctic_text_prompts_series is not None:\n",
        "          frame_prompt = updated_antarctic_text_prompts_series[frame_num]\n",
        "        else:\n",
        "          frame_prompt = []        \n",
        "      else:\n",
        "        if args.prompts_series is not None and frame_num >= len(args.prompts_series):\n",
        "          frame_prompt = args.prompts_series[-1]\n",
        "        elif args.prompts_series is not None:\n",
        "          frame_prompt = args.prompts_series[frame_num]\n",
        "        else:\n",
        "          frame_prompt = []\n",
        "      print(args.image_prompts_series)\n",
        "      if args.image_prompts_series is not None and frame_num >= len(args.image_prompts_series):\n",
        "        image_prompt = args.image_prompts_series[-1]\n",
        "      elif args.image_prompts_series is not None:\n",
        "        image_prompt = args.image_prompts_series[frame_num]\n",
        "      else:\n",
        "        image_prompt = []\n",
        "\n",
        "      print(f'Frame {frame_num} Prompt: {frame_prompt}')\n",
        "\n",
        "      model_stats = []\n",
        "      for clip_model in clip_models:\n",
        "            cutn = 16\n",
        "            model_stat = {\"clip_model\":None,\"target_embeds\":[],\"make_cutouts\":None,\"weights\":[]}\n",
        "            model_stat[\"clip_model\"] = clip_model\n",
        "            for prompt in frame_prompt:\n",
        "                txt, weight = parse_prompt(prompt)\n",
        "                txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
        "                \n",
        "                if args.fuzzy_prompt:\n",
        "                    for i in range(25):\n",
        "                        model_stat[\"target_embeds\"].append((txt + torch.randn(txt.shape).cuda() * args.rand_mag).clamp(0,1))\n",
        "                        model_stat[\"weights\"].append(weight)\n",
        "                else:\n",
        "                    model_stat[\"target_embeds\"].append(txt)\n",
        "                    model_stat[\"weights\"].append(weight)\n",
        "        \n",
        "            if image_prompt:\n",
        "              model_stat[\"make_cutouts\"] = MakeCutouts(clip_model.visual.input_resolution, cutn, skip_augs=skip_augs) \n",
        "              for prompt in image_prompt:\n",
        "                  path, weight = parse_prompt(prompt)\n",
        "                  img = Image.open(fetch(path)).convert('RGB')\n",
        "                  img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n",
        "                  batch = model_stat[\"make_cutouts\"](TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n",
        "                  embed = clip_model.encode_image(normalize(batch)).float()\n",
        "                  if fuzzy_prompt:\n",
        "                      for i in range(25):\n",
        "                          model_stat[\"target_embeds\"].append((embed + torch.randn(embed.shape).cuda() * rand_mag).clamp(0,1))\n",
        "                          weights.extend([weight / cutn] * cutn)\n",
        "                  else:\n",
        "                      model_stat[\"target_embeds\"].append(embed)\n",
        "                      model_stat[\"weights\"].extend([weight / cutn] * cutn)\n",
        "        \n",
        "            model_stat[\"target_embeds\"] = torch.cat(model_stat[\"target_embeds\"])\n",
        "            model_stat[\"weights\"] = torch.tensor(model_stat[\"weights\"], device=device)\n",
        "            if model_stat[\"weights\"].sum().abs() < 1e-3:\n",
        "                raise RuntimeError('The weights must not sum to 0.')\n",
        "            model_stat[\"weights\"] /= model_stat[\"weights\"].sum().abs()\n",
        "            model_stats.append(model_stat)\n",
        "  \n",
        "      init = None\n",
        "      if init_image is not None:\n",
        "          init = Image.open(fetch(init_image)).convert('RGB')\n",
        "          init = init.resize((args.side_x, args.side_y), Image.LANCZOS)\n",
        "          init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "      \n",
        "      if args.perlin_init:\n",
        "          if args.perlin_mode == 'color':\n",
        "              init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "              init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
        "          elif args.perlin_mode == 'gray':\n",
        "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
        "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "          else:\n",
        "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "          # init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device)\n",
        "          init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "          del init2\n",
        "  \n",
        "      cur_t = None\n",
        "  \n",
        "      def cond_fn(x, t, y=None):\n",
        "          with torch.enable_grad():\n",
        "              x_is_NaN = False\n",
        "              x = x.detach().requires_grad_()\n",
        "              n = x.shape[0]\n",
        "              if use_secondary_model is True:\n",
        "                alpha = torch.tensor(diffusion.sqrt_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
        "                sigma = torch.tensor(diffusion.sqrt_one_minus_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
        "                cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
        "                out = secondary_model(x, cosine_t[None].repeat([n])).pred\n",
        "                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "                x_in = out * fac + x * (1 - fac)\n",
        "                x_in_grad = torch.zeros_like(x_in)\n",
        "              else:\n",
        "                my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
        "                out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n",
        "                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "                x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "                x_in_grad = torch.zeros_like(x_in)\n",
        "              for model_stat in model_stats:\n",
        "                #for i in range(args.cutn_batches): tj - this is how it was, my new line is the next line\n",
        "                for i in range(math.floor(args.new_cutn_batches_series[frame_num])):                  \n",
        "                    t_int = int(t.item())+1 #errors on last step without +1, need to find source\n",
        "                    #when using SLIP Base model the dimensions need to be hard coded to avoid AttributeError: 'VisionTransformer' object has no attribute 'input_resolution'\n",
        "                    try:\n",
        "                        input_resolution=model_stat[\"clip_model\"].visual.input_resolution\n",
        "                    except:\n",
        "                        input_resolution=224\n",
        "\n",
        "                    cuts = MakeCutoutsDango(input_resolution,\n",
        "                            Overview= args.cut_overview[1000-t_int], \n",
        "                            InnerCrop = args.cut_innercut[1000-t_int], IC_Size_Pow=args.new_cut_ic_pow_series[frame_num], IC_Grey_P = args.cut_icgray_p[1000-t_int]\n",
        "                            )\n",
        "                    clip_in = normalize(cuts(x_in.add(1).div(2)))\n",
        "                    image_embeds = model_stat[\"clip_model\"].encode_image(clip_in).float()\n",
        "                    dists = spherical_dist_loss(image_embeds.unsqueeze(1), model_stat[\"target_embeds\"].unsqueeze(0))\n",
        "                    dists = dists.view([args.cut_overview[1000-t_int]+args.cut_innercut[1000-t_int], n, -1])\n",
        "                    losses = dists.mul(model_stat[\"weights\"]).sum(2).mean(0)\n",
        "                    loss_values.append(losses.sum().item()) # log loss, probably shouldn't do per cutn_batch\n",
        "                    x_in_grad += torch.autograd.grad(losses.sum() * math.floor(args.new_clip_guidance_scale_series[frame_num]), x_in)[0] / math.floor(args.new_cutn_batches_series[frame_num])\n",
        "              tv_losses = tv_loss(x_in)\n",
        "              if use_secondary_model is True:\n",
        "                range_losses = range_loss(out)\n",
        "              else:\n",
        "                range_losses = range_loss(out['pred_xstart'])\n",
        "              sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()\n",
        "              loss = tv_losses.sum() * math.floor(args.new_tv_scale_series[frame_num]) + range_losses.sum() * math.floor(args.new_range_scale_series[frame_num]) + sat_losses.sum() * math.floor(args.new_sat_scale_series[frame_num])\n",
        "              if init is not None and args.init_scale:\n",
        "                  init_losses = lpips_model(x_in, init)\n",
        "                  loss = loss + init_losses.sum() * args.init_scale\n",
        "              x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
        "              if torch.isnan(x_in_grad).any()==False:\n",
        "                  grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
        "              else:\n",
        "                # print(\"NaN'd\")\n",
        "                x_is_NaN = True\n",
        "                grad = torch.zeros_like(x)\n",
        "          if args.clamp_grad and x_is_NaN == False:\n",
        "              magnitude = grad.square().mean().sqrt()\n",
        "              return grad * magnitude.clamp(max=args.new_clamp_max_series[frame_num]) / magnitude  #min=-0.02, min=-clamp_max, \n",
        "          return grad\n",
        "  \n",
        "      if args.diffusion_sampling_mode == 'ddim':\n",
        "          sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "      else:\n",
        "          sample_fn = diffusion.plms_sample_loop_progressive\n",
        "\n",
        "\n",
        "      image_display = Output()\n",
        "      for i in range(args.n_batches):\n",
        "          if args.animation_mode == 'None':\n",
        "            display.clear_output(wait=True)\n",
        "            batchBar = tqdm(range(args.n_batches), desc =\"Batches\")\n",
        "            batchBar.n = i\n",
        "            batchBar.refresh()\n",
        "          print('')\n",
        "          display.display(image_display)\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "          cur_t = diffusion.num_timesteps - skip_steps - 1\n",
        "          #tj cur_t = math.floor(args.anim_steps_series[frame_num] - skip_steps - 1)\n",
        "          \n",
        "          total_steps = cur_t\n",
        "\n",
        "          if perlin_init:\n",
        "              init = regen_perlin()\n",
        "\n",
        "          if args.diffusion_sampling_mode == 'ddim':\n",
        "              samples = sample_fn(\n",
        "                  model,\n",
        "                  (batch_size, 3, args.side_y, args.side_x),\n",
        "                  clip_denoised=clip_denoised,\n",
        "                  model_kwargs={},\n",
        "                  cond_fn=cond_fn,\n",
        "                  progress=True,\n",
        "                  skip_timesteps=skip_steps,\n",
        "                  init_image=init,\n",
        "                  randomize_class=randomize_class,\n",
        "                  eta=args.new_eta_series[frame_num],\n",
        "                  transformation_fn=symmetry_transformation_fn,\n",
        "                  transformation_percent=args.transformation_percent\n",
        "              )\n",
        "          else:\n",
        "              samples = sample_fn(\n",
        "                  model,\n",
        "                  (batch_size, 3, args.side_y, args.side_x),\n",
        "                  clip_denoised=clip_denoised,\n",
        "                  model_kwargs={},\n",
        "                  cond_fn=cond_fn,\n",
        "                  progress=True,\n",
        "                  skip_timesteps=skip_steps,\n",
        "                  init_image=init,\n",
        "                  randomize_class=randomize_class,\n",
        "                  order=2,\n",
        "              )\n",
        "          print('used_series: ', args.used_series)\n",
        "          print('frame_durations: ',frame_durations)\n",
        "          print('average_frame_duration: ' + str(average_frame_duration))\n",
        "          print('args.batch_name: ', args.batch_name)\n",
        "          for series in args.used_series:\n",
        "            print('this series', series)\n",
        "            if hasattr(args, series+\"_series\"):\n",
        "              print(\"args.\"+series+\"_series: \"+ str(getattr(args, series+\"_series\")[frame_num]))\n",
        "          if len(animations_with_keyframes_higher_than_max) > 0:\n",
        "            print('WARNING: animations_with_keyframes_higher_than_max: ', animations_with_keyframes_higher_than_max)\n",
        "          #if the largest prompt key in image_prompts_series is greater than max keyframes, then tell the user this\n",
        "          if int(args.prompts_series.keys()[-1]) > int(args.max_frames):\n",
        "            print('WARNING: there is a prompt higher than the max keyframes')\n",
        "\n",
        "          # with run_display:\n",
        "          # display.clear_output(wait=True)\n",
        "          for j, sample in enumerate(samples):    \n",
        "            cur_t -= 1\n",
        "            intermediateStep = False\n",
        "            if args.steps_per_checkpoint is not None:\n",
        "                if j % steps_per_checkpoint == 0 and j > 0:\n",
        "                  intermediateStep = True\n",
        "            elif j in args.intermediate_saves:\n",
        "              intermediateStep = True\n",
        "            with image_display:\n",
        "              if j % args.display_rate == 0 or cur_t == -1 or intermediateStep == True:\n",
        "                  for k, image in enumerate(sample['pred_xstart']):\n",
        "                      # tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
        "                      current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n",
        "                      percent = math.ceil(j/total_steps*100)\n",
        "                      if args.n_batches > 0:\n",
        "                        #if intermediates are saved to the subfolder, don't append a step or percentage to the name\n",
        "                        if cur_t == -1 and args.intermediates_in_subfolder is True:\n",
        "                          save_num = f'{frame_num:04}' if animation_mode != \"None\" else i\n",
        "                          filename = f'{args.batch_name}({args.batchNum})_{save_num}.png'\n",
        "                        else:\n",
        "                          #If we're working with percentages, append it\n",
        "                          if args.steps_per_checkpoint is not None:\n",
        "                            filename = f'{args.batch_name}({args.batchNum})_{i:04}-{percent:02}%.png'\n",
        "                          # Or else, iIf we're working with specific steps, append those\n",
        "                          else:\n",
        "                            filename = f'{args.batch_name}({args.batchNum})_{i:04}-{j:03}.png'\n",
        "                      #Optional Soft Limiter\n",
        "                      if args.soft_limiter_on == True:\n",
        "                          #zippy's soft limiter\n",
        "                          image=(soft_limit(image)+1)/2\n",
        "                          image = TF.to_pil_image(image)\n",
        "                      else:\n",
        "                          #default clamping behavior\n",
        "                          #clamp values to between 0 and 1\n",
        "                          image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n",
        "                      if j % args.display_rate == 0 or cur_t == -1:\n",
        "                        image.save('progress.png')\n",
        "                        display.clear_output(wait=True)\n",
        "                        display.display(display.Image('progress.png'))\n",
        "                      if args.steps_per_checkpoint is not None:\n",
        "                        if j % args.steps_per_checkpoint == 0 and j > 0:\n",
        "                          if args.intermediates_in_subfolder is True:\n",
        "                            image.save(f'{partialFolder}/{filename}')\n",
        "                          else:\n",
        "                            image.save(f'{batchFolder}/{filename}')\n",
        "                      else:\n",
        "                        if j in args.intermediate_saves:\n",
        "                          if args.intermediates_in_subfolder is True:\n",
        "                            image.save(f'{partialFolder}/{filename}')\n",
        "                          else:\n",
        "                            image.save(f'{batchFolder}/{filename}')\n",
        "                      if cur_t == -1:\n",
        "                        if frame_num == 0:\n",
        "                          save_settings()\n",
        "                        #if frame_num == args.max_frames-1:\n",
        "                        save_settings()\n",
        "                        if args.animation_mode != \"None\":\n",
        "                          image.save('prevFrame.png')\n",
        "                        image.save(f'{batchFolder}/{filename}')\n",
        "\n",
        "                        if args.animation_mode == \"3D\":\n",
        "                          # If turbo, save a blended image\n",
        "                          if turbo_mode and frame_num > 0:\n",
        "                            # Mix new image with prevFrameScaled\n",
        "                            blend_factor = (1)/int(turbo_steps)\n",
        "                            newFrame = cv2.imread('prevFrame.png') # This is already updated..\n",
        "                            prev_frame_warped = cv2.imread('prevFrameScaled.png')\n",
        "                            blendedImage = cv2.addWeighted(newFrame, blend_factor, prev_frame_warped, (1-blend_factor), 0.0)\n",
        "                            cv2.imwrite(f'{batchFolder}/{filename}',blendedImage)\n",
        "                          else:\n",
        "                            image.save(f'{batchFolder}/{filename}')\n",
        "\n",
        "                          if vr_mode:\n",
        "                            generate_eye_views(TRANSLATION_SCALE, batchFolder, filename, frame_num, midas_model, midas_transform)\n",
        "\n",
        "                        # if frame_num != args.max_frames-1:\n",
        "                        #   display.clear_output()\n",
        "        \n",
        "          plt.plot(np.array(loss_values), 'r')\n",
        "\n",
        "def generate_eye_views(trans_scale,batchFolder,filename,frame_num,midas_model, midas_transform):\n",
        "   for i in range(2):\n",
        "      theta = vr_eye_angle * (math.pi/180)\n",
        "      ray_origin = math.cos(theta) * vr_ipd / 2 * (-1.0 if i==0 else 1.0)\n",
        "      ray_rotation = (theta if i==0 else -theta)\n",
        "      translate_xyz = [-(ray_origin)*trans_scale, 0,0]\n",
        "      rotate_xyz = [0, (ray_rotation), 0]\n",
        "      rot_mat = p3dT.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n",
        "      transformed_image = dxf.transform_image_3d(f'{batchFolder}/{filename}', midas_model, midas_transform, DEVICE,\n",
        "                                                      rot_mat, translate_xyz, math.floor(args.near_plane_series[frame_num]), math.floor(args.far_plane_series[frame_num]),\n",
        "                                                      math.floor(args.fov_series[frame_num]), padding_mode=args.padding_mode,\n",
        "                                                      sampling_mode=args.sampling_mode, midas_weight=args.midas_weight_series[frame_num],spherical=True)\n",
        "      eye_file_path = batchFolder+f\"/frame_{frame_num:04}\" + ('_l' if i==0 else '_r')+'.png'\n",
        "      transformed_image.save(eye_file_path)\n",
        "\n",
        "def save_settings(save_to_project_folder=True):\n",
        "  setting_list = {\n",
        "    'text_prompts': text_prompts,\n",
        "    'image_prompts': image_prompts,\n",
        "    'updated_antarctic_text_prompts': updated_antarctic_text_prompts,\n",
        "    'overwrite_settings_file': overwrite_settings_file,\n",
        "    'batch_name': batch_name,\n",
        "    'absolute_frame_multiplier': absolute_frame_multiplier,\n",
        "    'antarctic_prompt_frames_input': antarctic_prompt_frames_input,\n",
        "    'antarctic_pre_string': antarctic_pre_string,\n",
        "    'antarctic_post_string': antarctic_post_string,\n",
        "    'antarctic_number_of_captions': antarctic_number_of_captions,\n",
        "    'diffusion_model': diffusion_model,\n",
        "    'use_secondary_model': str(use_secondary_model),\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "    'diffusion_sampling_mode': diffusion_sampling_mode,\n",
        "    'use_checkpoint': str(use_checkpoint),\n",
        "    'ViTB32': str(ViTB32),\n",
        "    'ViTB16': str(ViTB16),\n",
        "    'ViTL14': str(ViTL14),\n",
        "    'ViTL14_336px': str(ViTL14_336px),\n",
        "    'RN101': str(RN101),\n",
        "    'RN50': str(RN50),\n",
        "    'RN50x4': str(RN50x4),\n",
        "    'RN50x16': str(RN50x16),\n",
        "    'RN50x64': str(RN50x64),\n",
        "    'check_model_SHA': str(check_model_SHA),\n",
        "    'steps': steps,\n",
        "    'width': width_height[0],\n",
        "    'height': width_height[1],\n",
        "    'clip_guidance_scale_input': clip_guidance_scale_input,\n",
        "    'tv_scale_input': tv_scale_input,\n",
        "    'range_scale_input': range_scale_input,\n",
        "    'sat_scale_input': sat_scale_input,\n",
        "    # 'cutn': cutn,\n",
        "    'cutn_batches_input': cutn_batches_input,\n",
        "    'skip_augs': str(skip_augs),\n",
        "    'randomize_class': randomize_class,\n",
        "    'clip_denoised': clip_denoised,\n",
        "    'soft_limiter_on': str(soft_limiter_on),\n",
        "    'soft_limiter_knee': soft_limiter_knee,\n",
        "    'init_image': init_image,\n",
        "    'init_scale': init_scale,\n",
        "    'skip_steps': skip_steps,\n",
        "    'animation_mode': animation_mode,\n",
        "    'video_init_path':video_init_path,\n",
        "    'extract_nth_frame_input':extract_nth_frame_input,\n",
        "    'video_init_seed_continuity': str(video_init_seed_continuity),\n",
        "    'key_frames': str(key_frames),\n",
        "    'max_frames_input': max_frames_input,\n",
        "    'interp_spline': interp_spline,\n",
        "    'angle_input': angle_input,\n",
        "    'zoom_input': zoom_input,\n",
        "    'object_detection_zoom_input': object_detection_zoom_input,\n",
        "    #'anim' try printing anim_steps and the series\n",
        "    'translation_x_input': translation_x_input,\n",
        "    'translation_y_input': translation_y_input,\n",
        "    'translation_z_input': translation_z_input,\n",
        "    'rotation_3d_x_input': rotation_3d_x_input,\n",
        "    'rotation_3d_y_input': rotation_3d_y_input,\n",
        "    'rotation_3d_z_input': rotation_3d_z_input,\n",
        "    'midas_depth_model': midas_depth_model,\n",
        "    'midas_weight_input': midas_weight_input,\n",
        "    'near_plane_input': near_plane_input,\n",
        "    'far_plane_input': far_plane_input,\n",
        "    'fov_input': fov_input,\n",
        "    'padding_mode': padding_mode,\n",
        "    'sampling_mode': sampling_mode,\n",
        "    'turbo_mode':str(turbo_mode),\n",
        "    'turbo_steps':turbo_steps,\n",
        "    'turbo_preroll':turbo_preroll,\n",
        "    'frames_scale_input': frames_scale_input,\n",
        "    'frames_skip_steps_input': frames_skip_steps_input,\n",
        "    'vr_mode': str(vr_mode),\n",
        "    'vr_eye_angle': vr_eye_angle,\n",
        "    'vr_ipd': vr_ipd,\n",
        "    'intermediate_saves': intermediate_saves,\n",
        "    'intermediates_in_subfolder': str(intermediates_in_subfolder),\n",
        "    'perlin_init': str(perlin_init),\n",
        "    'perlin_mode': perlin_mode,\n",
        "    'set_seed': set_seed,\n",
        "    'seed': seed,\n",
        "    'fuzzy_prompt': fuzzy_prompt,\n",
        "    'rand_mag': rand_mag,\n",
        "    'eta': eta,\n",
        "    'clamp_grad': str(clamp_grad),\n",
        "    'clamp_max_input': clamp_max_input,\n",
        "    'cut_overview': str(cut_overview),\n",
        "    'cut_innercut': str(cut_innercut),\n",
        "    'cut_ic_pow': cut_ic_pow,\n",
        "    'cut_icgray_p': str(cut_icgray_p),\n",
        "    'use_vertical_symmetry':str(use_vertical_symmetry),\n",
        "    'use_horizontal_symmetry':str(use_horizontal_symmetry),\n",
        "    'transformation_percent':transformation_percent,\n",
        "    'skip_video_for_run_all':skip_video_for_run_all,\n",
        "    'make_regular_video':make_regular_video,\n",
        "    'make_info_video':make_info_video,\n",
        "    'make_unique_prompt_overlay_video':make_unique_prompt_overlay_video,\n",
        "    'make_prompt_overlay_video':make_prompt_overlay_video,\n",
        "    'make_interpolation_video':make_interpolation_video,\n",
        "    'fps':fps,\n",
        "    'mount_gdrive':mount_gdrive,\n",
        "    'frames_to_interpolate':frames_to_interpolate,\n",
        "    'gdrive_mountpoint':gdrive_mountpoint,\n",
        "    'gdrive_subdirectory':gdrive_subdirectory,\n",
        "    'number_of_interpolations':number_of_interpolations,\n",
        "    'output_video':output_video,\n",
        "    'output_video_fps':output_video_fps,\n",
        "    'should_create_gist':should_create_gist,\n",
        "    'github_username':github_username,\n",
        "    'ccmixter_search_url_or_keyword':ccmixter_search_url_or_keyword,\n",
        "    'finished_video_length':finished_video_length,    \n",
        "    'show_first_prompt_in_youtube_description':show_first_prompt_in_youtube_description,\n",
        " \n",
        "  }\n",
        "  file_path=f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\"\n",
        "  if not save_to_project_folder:\n",
        "    file_path=f\"/content/drive/MyDrive/AI/settings_files/{batch_name}({batchNum})_settings.txt\"\n",
        "  #remove one item from the dictionary\n",
        "  if not antarctic_prompt_frames_input:\n",
        "    del setting_list['updated_antarctic_text_prompts']\n",
        "  #if f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\" exists, then delete it\n",
        "  if os.path.exists(file_path):\n",
        "    os.remove(file_path)\n",
        "  # print('Settings:', setting_list)\n",
        "  with open(file_path, \"w+\") as f:   #save settings\n",
        "    json.dump(setting_list, f, ensure_ascii=False, indent=4)\n",
        "  #how to add every item from a list to another\n",
        "\n",
        "  \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KkXhQQrgw-7"
      },
      "source": [
        "## 6. Define the secondary diffusion model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVtH-otxgw-7"
      },
      "outputs": [],
      "source": [
        "#@title 1.6 Define the secondary diffusion model\n",
        "\n",
        "def append_dims(x, n):\n",
        "    return x[(Ellipsis, *(None,) * (n - x.ndim))]\n",
        "\n",
        "\n",
        "def expand_to_planes(x, shape):\n",
        "    return append_dims(x, len(shape)).repeat([1, 1, *shape[2:]])\n",
        "\n",
        "\n",
        "def alpha_sigma_to_t(alpha, sigma):\n",
        "    return torch.atan2(sigma, alpha) * 2 / math.pi\n",
        "\n",
        "\n",
        "def t_to_alpha_sigma(t):\n",
        "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DiffusionOutput:\n",
        "    v: torch.Tensor\n",
        "    pred: torch.Tensor\n",
        "    eps: torch.Tensor\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(c_in, c_out, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "\n",
        "class SkipBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
        "\n",
        "\n",
        "class FourierFeatures(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std=1.):\n",
        "        super().__init__()\n",
        "        assert out_features % 2 == 0\n",
        "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
        "\n",
        "    def forward(self, input):\n",
        "        f = 2 * math.pi * input @ self.weight.T\n",
        "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
        "\n",
        "\n",
        "class SecondaryDiffusionImageNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ConvBlock(3 + 16, c),\n",
        "            ConvBlock(c, c),\n",
        "            SkipBlock([\n",
        "                nn.AvgPool2d(2),\n",
        "                ConvBlock(c, c * 2),\n",
        "                ConvBlock(c * 2, c * 2),\n",
        "                SkipBlock([\n",
        "                    nn.AvgPool2d(2),\n",
        "                    ConvBlock(c * 2, c * 4),\n",
        "                    ConvBlock(c * 4, c * 4),\n",
        "                    SkipBlock([\n",
        "                        nn.AvgPool2d(2),\n",
        "                        ConvBlock(c * 4, c * 8),\n",
        "                        ConvBlock(c * 8, c * 4),\n",
        "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                    ]),\n",
        "                    ConvBlock(c * 8, c * 4),\n",
        "                    ConvBlock(c * 4, c * 2),\n",
        "                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                ]),\n",
        "                ConvBlock(c * 4, c * 2),\n",
        "                ConvBlock(c * 2, c),\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            ]),\n",
        "            ConvBlock(c * 2, c),\n",
        "            nn.Conv2d(c, 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
        "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
        "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "\n",
        "class SecondaryDiffusionImageNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "        self.down = nn.AvgPool2d(2)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ConvBlock(3 + 16, cs[0]),\n",
        "            ConvBlock(cs[0], cs[0]),\n",
        "            SkipBlock([\n",
        "                self.down,\n",
        "                ConvBlock(cs[0], cs[1]),\n",
        "                ConvBlock(cs[1], cs[1]),\n",
        "                SkipBlock([\n",
        "                    self.down,\n",
        "                    ConvBlock(cs[1], cs[2]),\n",
        "                    ConvBlock(cs[2], cs[2]),\n",
        "                    SkipBlock([\n",
        "                        self.down,\n",
        "                        ConvBlock(cs[2], cs[3]),\n",
        "                        ConvBlock(cs[3], cs[3]),\n",
        "                        SkipBlock([\n",
        "                            self.down,\n",
        "                            ConvBlock(cs[3], cs[4]),\n",
        "                            ConvBlock(cs[4], cs[4]),\n",
        "                            SkipBlock([\n",
        "                                self.down,\n",
        "                                ConvBlock(cs[4], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[4]),\n",
        "                                self.up,\n",
        "                            ]),\n",
        "                            ConvBlock(cs[4] * 2, cs[4]),\n",
        "                            ConvBlock(cs[4], cs[3]),\n",
        "                            self.up,\n",
        "                        ]),\n",
        "                        ConvBlock(cs[3] * 2, cs[3]),\n",
        "                        ConvBlock(cs[3], cs[2]),\n",
        "                        self.up,\n",
        "                    ]),\n",
        "                    ConvBlock(cs[2] * 2, cs[2]),\n",
        "                    ConvBlock(cs[2], cs[1]),\n",
        "                    self.up,\n",
        "                ]),\n",
        "                ConvBlock(cs[1] * 2, cs[1]),\n",
        "                ConvBlock(cs[1], cs[0]),\n",
        "                self.up,\n",
        "            ]),\n",
        "            ConvBlock(cs[0] * 2, cs[0]),\n",
        "            nn.Conv2d(cs[0], 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
        "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
        "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp6Vo4nbwhrK"
      },
      "source": [
        "# **Pre Video Settings Input**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsnaWFoXgw-8"
      },
      "source": [
        "## **Pre Diffuse Settings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "balObflYgw-8"
      },
      "source": [
        "### **Settings Overwrite**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6golH0RjQHGU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import display as dropdown_widget_display\n",
        "import ipywidgets as widgets\n",
        "settings_file_directory=\"/content/drive/MyDrive/AI/settings_files/\"\n",
        "folder_files = os.listdir(settings_file_directory) #You can also use full path.\n",
        "\n",
        "folder_files=[]\n",
        "folder_files.append('custom directory')\n",
        "for item in os.listdir(settings_file_directory) :\n",
        "  folder_files.append(item)\n",
        "scW = widgets.Select(options=folder_files)\n",
        "dropdown_widget_display(scW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRWyRIuFgw-9"
      },
      "outputs": [],
      "source": [
        "overwrite_settings_file_custom_directory = \"none\" #@param{type: 'string'}\n",
        "if scW.value=='custom_directory':\n",
        "    overwrite_settings_file = overwrite_settings_file_custom_directory\n",
        "else:\n",
        "    overwrite_settings_file = settings_file_directory + scW.value\n",
        "    print(overwrite_settings_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8AHqh4LkWed"
      },
      "source": [
        "### **Batch Name**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scql1UXTkWed"
      },
      "outputs": [],
      "source": [
        "batch_name = 'testing_many' #@param{type: 'string'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4pah79ogw-9"
      },
      "source": [
        "### **Frame Multiplier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDsbjuBEgw-9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@markdown This number is multiplied by every user input frame number\n",
        "absolute_frame_multiplier = 1 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtDmnCnhgw-9"
      },
      "source": [
        "### **Antarctic AI Prompts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qj74GFigw-9"
      },
      "outputs": [],
      "source": [
        "#@markdown comma seperate frames that should have automatically generated prompts based on the previous frame image\n",
        "#@markdown Example: 10,50,100,150\n",
        "#@markdown if left blank then it won't set up Antarctic at all\n",
        "antarctic_prompt_frames_input = \"\" #@param {type: 'string'}\n",
        "antarctic_prompt_frames = antarctic_prompt_frames_input\n",
        "#@markdown Example: pre and post strings will be placed after the generated caption\n",
        "antarctic_pre_string = \"\" #@param {type: 'string'}\n",
        "antarctic_post_string = \",trending on Artstation\"#@param {type: 'string'}\n",
        "antarctic_number_of_captions =  2#@param {type: 'number'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeKW7x0-gw-9"
      },
      "source": [
        "### **Diffusion and CLIP Model Settings**\n",
        "#### **Model Settings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrAU4pAkgw--"
      },
      "outputs": [],
      "source": [
        "\n",
        "diffusion_model = \"512x512_diffusion_uncond_finetune_008100\" #@param [\"256x256_diffusion_uncond\", \"512x512_diffusion_uncond_finetune_008100\"]\n",
        "use_secondary_model = True #@param {type: 'boolean'}\n",
        "diffusion_sampling_mode = 'ddim' #@param ['plms','ddim']  \n",
        "use_checkpoint = True #@param {type: 'boolean'}\n",
        "ViTB32 = True #@param{type:\"boolean\"}\n",
        "ViTB16 = False #@param{type:\"boolean\"}\n",
        "ViTL14 = False #@param{type:\"boolean\"}\n",
        "ViTL14_336px = False #@param{type:\"boolean\"}\n",
        "RN101 = False #@param{type:\"boolean\"}\n",
        "RN50 = False #@param{type:\"boolean\"}\n",
        "RN50x4 = False #@param{type:\"boolean\"}\n",
        "RN50x16 = False #@param{type:\"boolean\"}\n",
        "RN50x64 = False #@param{type:\"boolean\"}\n",
        "#@markdown If you're having issues with model downloads, check this to compare SHA's:\n",
        "check_model_SHA = False #@param{type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is3qrzXvgw--"
      },
      "source": [
        "### **Image/Frame Settings**\n",
        "#### **Basic Settings:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K43EaTvTgw--"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "steps =  20#@param [25,50,100,150,250,500,1000]{type: 'raw', allow-input: true}\n",
        "width_height = [500,500]#@param{type: 'raw'}\n",
        "clip_guidance_scale_input = \"30000\"#@param{type: 'string'}\n",
        "tv_scale_input = \"150000\"#@param{type: 'string'}\n",
        "range_scale_input = \"90\"#@param{type: 'string'}\n",
        "sat_scale_input = \"190000\"#@param{type: 'string'}\n",
        "cutn_batches_input = \"2\"#@param{type: 'string'}\n",
        "skip_augs= False#@param{type: 'boolean'}\n",
        "\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### **Soft Limiter (Use 0.97 - 0.995 range):**\n",
        "#@markdown *Experimental! ...may help mitigate color clipping.*\n",
        "soft_limiter_on = True#@param{type: 'boolean'}\n",
        "soft_limiter_knee = .98 #@param{type: 'number'}\n",
        "\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### **Init Settings:**\n",
        "init_image = \"None\" #@param{type: 'string'}\n",
        "init_scale = 1000 #@param{type: 'integer'}\n",
        "skip_steps =  15#@param{type: 'integer'}\n",
        "#@markdown *Make sure you set skip_steps to ~50% of your steps if you want to use an init image.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruta4zdxgw--"
      },
      "source": [
        "### **Animation Settings**\n",
        "#### **Animation Mode:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ_rMmA_gw--"
      },
      "outputs": [],
      "source": [
        "animation_mode = '2D' #@param ['None', '2D', '3D', 'Video Input'] {type:'string'}\n",
        "#@markdown *For animation, you probably want to turn `cutn_batches` to 1 to make it quicker.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#======= VR MODE\n",
        "#@markdown ---\n",
        "#@markdown ####**VR Mode (3D anim only):**\n",
        "#@markdown Enables stereo rendering of left/right eye views (supporting Turbo) which use a different (fish-eye) camera projection matrix.   \n",
        "#@markdown Note the images you're prompting will work better if they have some inherent wide-angle aspect\n",
        "#@markdown The generated images will need to be combined into left/right videos. These can then be stitched into the VR180 format.\n",
        "#@markdown Google made the VR180 Creator tool but subsequently stopped supporting it. It's available for download in a few places including https://www.patrickgrunwald.de/vr180-creator-download\n",
        "#@markdown The tool is not only good for stitching (videos and photos) but also for adding the correct metadata into existing videos, which is needed for services like YouTube to identify the format correctly.\n",
        "#@markdown Watching YouTube VR videos isn't necessarily the easiest depending on your headset. For instance Oculus have a dedicated media studio and store which makes the files easier to access on a Quest https://creator.oculus.com/manage/mediastudio/\n",
        "#@markdown \n",
        "#@markdown The command to get ffmpeg to concat your frames for each eye is in the form: `ffmpeg -framerate 15 -i frame_%4d_l.png l.mp4` (repeat for r)\n",
        "\n",
        "vr_mode = False #@param {type:\"boolean\"}\n",
        "#@markdown `vr_eye_angle` is the y-axis rotation of the eyes towards the center\n",
        "vr_eye_angle = 0.5 #@param{type:\"number\"}\n",
        "#@markdown interpupillary distance (between the eyes)\n",
        "vr_ipd = 5.0 #@param{type:\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lykYUQM0QHGZ"
      },
      "source": [
        "#### Video Input Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92YbPo1yQHGZ"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "\n",
        "#@markdown ###**Video Input Settings:**\n",
        "if is_colab:\n",
        "    video_init_path = \"/content/training.mp4\" #@param {type: 'string'}\n",
        "else:\n",
        "    video_init_path = \"training.mp4\" #@param {type: 'string'}\n",
        "extract_nth_frame_input = 2 #@param {type: 'number'}\n",
        "video_init_seed_continuity = False #@param {type: 'boolean'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tba9H7ccQHGZ"
      },
      "source": [
        "#### 2D Animation Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INvQ66dZQHGa"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "#@markdown ####**2D Animation Settings:**\n",
        "#@markdown `zoom` is a multiplier of dimensions, 1 is no zoom.\n",
        "#@markdown All rotations are provided in degrees.\n",
        "key_frames = True #@param {type:\"boolean\"}\n",
        "max_frames_input = 5#@param {type:\"number\"}\n",
        "\n",
        "interp_spline = 'Linear' #Do not change, currently will not look good. param ['Linear','Quadratic','Cubic']{type:\"string\"}\n",
        "angle_input = \"0: (0)\"#@param {type:\"string\"}\n",
        "zoom_input = \"0: (1)\"#@param {type:\"string\"}\n",
        "object_detection_zoom_input = []#@param{type: 'raw'}\n",
        "translation_x_input = \"0: (0)\"#@param {type:\"string\"}\n",
        "translation_y_input = \"0: (0)\"#@param {type:\"string\"}\n",
        "translation_z_input = \"0: (0)\"#@param {type:\"string\"}\n",
        "rotation_3d_x_input = \"0: (0)\"#@param {type:\"string\"}\n",
        "rotation_3d_y_input = \"0: (0)\"#@param {type:\"string\"}\n",
        "rotation_3d_z_input = \"0: (0), 100: (90)\"#@param {type:\"string\"}\n",
        "midas_depth_model = \"dpt_large\"#@param {type:\"string\"}\n",
        "midas_weight_input = \"0.3\"#@param{type: 'string'}\n",
        "near_plane_input = \"200\"#@param{type: 'string'}\n",
        "far_plane_input = \"10000\"#@param{type: 'string'}\n",
        "fov_input = \"120\"#@param{type: 'string'}\n",
        "padding_mode = 'border'#@param {type:\"string\"}\n",
        "sampling_mode = 'bicubic'#@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUY9dTkjQHGa"
      },
      "source": [
        "#### Turbo Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ78TfH5QHGa"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "#@markdown ####**Turbo Mode (3D anim only):**\n",
        "#@markdown (Starts after frame 10,) skips diffusion steps and just uses depth map to warp images for skipped frames.\n",
        "#@markdown Speeds up rendering by 2x-4x, and may improve image coherence between frames. frame_blend_mode smooths abrupt texture changes across 2 frames.\n",
        "#@markdown For different settings tuned for Turbo Mode, refer to the original Disco-Turbo Github: https://github.com/zippy731/disco-diffusion-turbo\n",
        "\n",
        "turbo_mode = True #@param {type:\"boolean\"}\n",
        "turbo_steps = \"6\" #@param [\"2\",\"3\",\"4\",\"5\",\"6\"] {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ####**Coherency Settings:**\n",
        "#@markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\n",
        "frames_scale_input =  \"1500\"#@param{type: 'string'}\n",
        "#@markdown `frames_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\n",
        "frames_skip_steps_input = \"1: (85), 50: (65), 100: (85)\" #@param{type: 'string'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8NUwSL9QHGb"
      },
      "source": [
        "#### VR MODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7KQxWt_QHGb"
      },
      "outputs": [],
      "source": [
        "#======= VR MODE\n",
        "#@markdown ---\n",
        "#@markdown ####**VR Mode (3D anim only):**\n",
        "#@markdown Enables stereo rendering of left/right eye views (supporting Turbo) which use a different (fish-eye) camera projection matrix.   \n",
        "#@markdown Note the images you're prompting will work better if they have some inherent wide-angle aspect\n",
        "#@markdown The generated images will need to be combined into left/right videos. These can then be stitched into the VR180 format.\n",
        "#@markdown Google made the VR180 Creator tool but subsequently stopped supporting it. It's available for download in a few places including https://www.patrickgrunwald.de/vr180-creator-download\n",
        "#@markdown The tool is not only good for stitching (videos and photos) but also for adding the correct metadata into existing videos, which is needed for services like YouTube to identify the format correctly.\n",
        "#@markdown Watching YouTube VR videos isn't necessarily the easiest depending on your headset. For instance Oculus have a dedicated media studio and store which makes the files easier to access on a Quest https://creator.oculus.com/manage/mediastudio/\n",
        "#@markdown \n",
        "#@markdown The command to get ffmpeg to concat your frames for each eye is in the form: `ffmpeg -framerate 15 -i frame_%4d_l.png l.mp4` (repeat for r)\n",
        "\n",
        "vr_mode = False #@param {type:\"boolean\"}\n",
        "#@markdown `vr_eye_angle` is the y-axis rotation of the eyes towards the center\n",
        "vr_eye_angle = 0.5 #@param{type:\"number\"}\n",
        "#@markdown interpupillary distance (between the eyes)\n",
        "vr_ipd = 5.0 #@param{type:\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1q7ostHgw-_"
      },
      "source": [
        "\n",
        "### **Extra Settings**\n",
        "#### **Saving:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiaWIIV5gSi-"
      },
      "outputs": [],
      "source": [
        "\n",
        "intermediate_saves = 0#@param{type: 'raw'}\n",
        "intermediates_in_subfolder = True #@param{type: 'boolean'}\n",
        "#@markdown Intermediate steps will save a copy at your specified intervals. You can either format it as a single integer or a list of specific steps \n",
        "\n",
        "#@markdown A value of `2` will save a copy at 33% and 66%. 0 will save none.\n",
        "\n",
        "#@markdown A value of `[5, 9, 34, 45]` will save at steps 5, 9, 34, and 45. (Make sure to include the brackets)\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "#@markdown ####**Advanced Settings:**\n",
        "#@markdown *There are a few extra advanced settings available if you double click this cell.*\n",
        "\n",
        "#@markdown *Perlin init will replace your init, so uncheck if using one.*\n",
        "\n",
        "perlin_init = False  #@param{type: 'boolean'}\n",
        "perlin_mode = 'mixed' #@param ['mixed', 'color', 'gray']\n",
        "set_seed = 'random_seed' #@param{type: 'string'}\n",
        "eta_input = \"0.9\"#@param{type: 'string'}\n",
        "clamp_grad = True #@param{type: 'boolean'}\n",
        "clamp_max_input = \"0.06\"#@param{type: 'string'}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpyic3z5QHGc"
      },
      "source": [
        "#### Cutn Scheduling and Transformation Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e3MlS_qQHGc"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "\n",
        "#@markdown ####**Cutn Scheduling:**\n",
        "#@markdown Format: `[40]*400+[20]*600` = 40 cuts for the first 400 /1000 steps, then 20 for the last 600/1000\n",
        "\n",
        "#@markdown cut_overview and cut_innercut are cumulative for total cutn on any given step. Overview cuts see the entire image and are good for early structure, innercuts are your standard cutn.\n",
        "\n",
        "cut_overview = \"[20]*200+[16]*200+[10]*200+[8]*200+[2]*200\" #@param {type: 'string'}       \n",
        "cut_innercut =\"[2]*200+[4]*200+[6]*200+[8]*200+[4]*200\"#@param {type: 'string'}  \n",
        "cut_ic_pow_input = \"10\"#@param{type: 'string'} \n",
        "cut_icgray_p = \"[0.2]*400+[0]*800\"#@param {type: 'string'}\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ####**Transformation Settings:**\n",
        "use_vertical_symmetry = False #@param {type:\"boolean\"}\n",
        "use_horizontal_symmetry = False #@param {type:\"boolean\"}\n",
        "transformation_percent = [0.09] #@param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PromptsTop"
      },
      "source": [
        "## Prompts\n",
        "`animation_mode: None` will only use the first set. `animation_mode: 2D / Video` will run through them per the set frames and hold on the last one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1Iy9N2pQHGd"
      },
      "outputs": [],
      "source": [
        "#this comment is used to indicate the beginning of prompt creation DONT CHANGE!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prompts"
      },
      "outputs": [],
      "source": [
        "text_prompts = {\n",
        "    0: [\n",
        "            \"girl detailed portrait of an adorable egyptian baby girl by Nick Silva, Shin JeongHo, Wandah Kurniawan, Symmetrical composition with people centered, trending on artstation, colorized:4\",\n",
        "            \"text:-2\",\n",
        "            \"glasses:-1\",\n",
        "            \"color:1\"\n",
        "        ],\n",
        "    1: [\n",
        "            \"DÜNYA DÖNÜYOR detailed portrait of an adorable egyptian baby girl by Nick Silva, Shin JeongHo, Wandah Kurniawan, Symmetrical composition with people centered, trending on artstation, colorized:4\",\n",
        "            \"text:-2\",\n",
        "            \"glasses:-1\",\n",
        "            \"color:1\"\n",
        "        ],\n",
        "    2: [\n",
        "            \"guy detailed portrait of an adorable egyptian baby girl by Nick Silva, Shin JeongHo, Wandah Kurniawan, Symmetrical composition with people centered, trending on artstation, colorized:4\",\n",
        "            \"text:-2\",\n",
        "            \"glasses:-1\",\n",
        "            \"color:1\"\n",
        "        ]\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "image_prompts = {\n",
        "    # 0:['ImagePromptsWorkButArentVeryGood.png:2',],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IN2HaPwQHGd"
      },
      "outputs": [],
      "source": [
        "#this comment is used to indicate the end of prompt creation DONT CHANGE!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7BKPsDPQHGe"
      },
      "source": [
        "### Prompt Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqVmMD9nQHGe"
      },
      "outputs": [],
      "source": [
        "#add every item of a list to each prompt\n",
        "# for key in text_prompts:\n",
        "#   text_prompts[key].extend([\"text:-5\",\"words:-5\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-tCVA_6xnj3"
      },
      "source": [
        "## Diffuse Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fHO6WfRxlOI"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Diffuse!**\n",
        "#@markdown ###**Do the Run!**\n",
        "#@markdown `n_batches` ignored with animation modes.\n",
        "\n",
        "display_rate =  250 #@param{type: 'number'}\n",
        "n_batches =  200 #@param{type: 'number'}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "resume_run = True #@param{type: 'boolean'}\n",
        "run_to_resume = '-1' #@param{type: 'string'}\n",
        "resume_from_frame = 'latest' #@param{type: 'string'}\n",
        "retain_overwritten_frames = False #@param{type: 'boolean'}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KrIW8rsQHGf"
      },
      "source": [
        "## Settings Overwrite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsIVlWOoQHGf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "if 'settings.txt' in overwrite_settings_file:\n",
        "  json_file=open(overwrite_settings_file)\n",
        "  data = json.load(json_file)\n",
        "  if 'text_prompts' in data:\n",
        "    text_prompts=data['text_prompts']\n",
        "    text_prompts = {int(k):v for k,v in text_prompts.items()}\n",
        "  if 'absolute_frame_multiplier' in data: absolute_frame_multiplier=data['absolute_frame_multiplier']\n",
        "  if 'antarctic_prompt_frames_input' in data: antarctic_prompt_frames_input=data['antarctic_prompt_frames_input']\n",
        "  if 'antarctic_pre_string' in data: antarctic_pre_string=data['antarctic_pre_string']\n",
        "  if 'antarctic_post_string' in data: antarctic_post_string=data['antarctic_post_string']\n",
        "  if 'antarctic_number_of_captions' in data: antarctic_number_of_captions=data['antarctic_number_of_captions']\n",
        "  if 'diffusion_model' in data: diffusion_model=data['diffusion_model']\n",
        "  if 'use_secondary_model' in data:\n",
        "    if data['use_secondary_model'] == 'True': use_secondary_model = True\n",
        "    else: use_secondary_model = False    \n",
        "  if 'diffusion_sampling_mode' in data: diffusion_sampling_mode=data['diffusion_sampling_mode']\n",
        "  if 'use_checkpoint' in data:\n",
        "    if data['use_checkpoint'] == 'True': use_checkpoint = True\n",
        "    else: use_checkpoint = False\n",
        "  if 'ViTB32' in data:\n",
        "    if data['ViTB32'] == 'True': ViTB32 = True\n",
        "    else: ViTB32 = False\n",
        "  if 'ViTB16' in data:\n",
        "    if data['ViTB16'] == 'True': ViTB16 = True\n",
        "    else: ViTB16 = False\n",
        "  if 'ViTL14' in data:\n",
        "    if data['ViTL14'] == 'True': ViTL14 = True\n",
        "    else: ViTL14 = False\n",
        "  if 'ViTL14_336px' in data:\n",
        "    if data['ViTL14_336px'] == 'True': ViTL14_336px = True\n",
        "    else: ViTL14_336px = False\n",
        "  if 'RN101' in data:\n",
        "    if data['RN101'] == 'True': RN101 = True\n",
        "    else: RN101 = False\n",
        "  if 'RN50' in data:\n",
        "    if data['RN50'] == 'True': RN50 = True\n",
        "    else: RN50 = False\n",
        "  if 'RN50x4' in data:\n",
        "    if data['RN50x4'] == 'True': RN50x4 = True\n",
        "    else: RN50x4 = False    \n",
        "  if 'RN50x16' in data:\n",
        "    if data['RN50x16'] == 'True': RN50x16 = True\n",
        "    else: RN50x16 = False\n",
        "  if 'RN50x64' in data:\n",
        "    if data['RN50x64'] == 'True': RN50x64 = True\n",
        "    else: RN50x64 = False\n",
        "  if 'check_model_SHA' in data: check_model_SHA=data['check_model_SHA']\n",
        "  if 'steps' in data: steps=data['steps']\n",
        "  if 'diffusion_steps' in data: diffusion_steps=data['diffusion_steps']\n",
        "  if 'width' in data and 'height' in data: width_height=[data['width'],data['height']]\n",
        "  if 'clip_guidance_scale_input' in data: clip_guidance_scale_input=data['clip_guidance_scale_input']\n",
        "  if 'tv_scale_input' in data: tv_scale_input=data['tv_scale_input']\n",
        "  if 'range_scale_input' in data: range_scale_input=data['range_scale_input']\n",
        "  if 'sat_scale_input' in data: sat_scale_input=data['sat_scale_input']\n",
        "  if 'cutn_batches_input' in data: cutn_batches_input=data['cutn_batches_input']\n",
        "  if 'skip_augs' in data:\n",
        "    if data['skip_augs'] == 'True': skip_augs = True\n",
        "    else: skip_augs = False\n",
        "  if 'soft_limiter_on' in data:\n",
        "    if data['soft_limiter_on'] == 'True': soft_limiter_on = True\n",
        "    else: soft_limiter_on = False\n",
        "  if 'soft_limiter_knee' in data: soft_limiter_knee=data['soft_limiter_knee']\n",
        "  if 'init_image' in data: init_image=data['init_image']\n",
        "  if 'init_scale' in data: init_scale=data['init_scale']\n",
        "  if 'skip_steps' in data: skip_steps=data['skip_steps']\n",
        "  if 'animation_mode' in data: animation_mode=data['animation_mode']\n",
        "  if 'video_init_path' in data: video_init_path=data['video_init_path']\n",
        "  if 'extract_nth_frame_input' in data: extract_nth_frame_input=data['extract_nth_frame_input']\n",
        "  if 'video_init_seed_continuity' in data:\n",
        "    if data['video_init_seed_continuity'] == 'True': video_init_seed_continuity = True\n",
        "    else: video_init_seed_continuity = False\n",
        "  if 'key_frames' in data:\n",
        "    if data['key_frames'] == 'True': key_frames = True\n",
        "    else: key_frames = False\n",
        "  if 'max_frames_input' in data: max_frames_input=data['max_frames_input']\n",
        "  if 'interp_spline' in data: interp_spline=data['interp_spline']\n",
        "  if 'angle_input' in data: angle_input=data['angle_input']\n",
        "  if 'zoom_input' in data: zoom_input=data['zoom_input']\n",
        "  if 'object_detection_zoom_input' in data: object_detection_zoom_input=data['object_detection_zoom_input']\n",
        "  if 'translation_x_input' in data: translation_x_input=data['translation_x_input']\n",
        "  if 'translation_y_input' in data: translation_y_input=data['translation_y_input']\n",
        "  if 'translation_z_input' in data: translation_z_input=data['translation_z_input']\n",
        "  if 'rotation_3d_x_input' in data: rotation_3d_x_input=data['rotation_3d_x_input']\n",
        "  if 'rotation_3d_y_input' in data: rotation_3d_y_input=data['rotation_3d_y_input']\n",
        "  if 'rotation_3d_z_input' in data: rotation_3d_z_input=data['rotation_3d_z_input']\n",
        "  if 'midas_depth_model' in data: midas_depth_model=data['midas_depth_model']\n",
        "  if 'midas_weight_input' in data: midas_weight_input=data['midas_weight_input']\n",
        "  if 'near_plane_input' in data: near_plane_input=data['near_plane_input']\n",
        "  if 'far_plane_input' in data: far_plane_input=data['far_plane_input']\n",
        "  if 'fov_input' in data: fov_input=data['fov_input']\n",
        "  if 'padding_mode' in data: padding_mode=data['padding_mode']\n",
        "  if 'sampling_mode' in data: sampling_mode=data['sampling_mode'] \n",
        "  if 'turbo_mode' in data:\n",
        "    if data['turbo_mode'] == 'True': turbo_mode = True\n",
        "    else: turbo_mode = False\n",
        "  if 'turbo_steps' in data: turbo_steps=data['turbo_steps']\n",
        "  if 'turbo_preroll' in data: turbo_preroll=data['turbo_preroll']\n",
        "  if 'frames_scale_input' in data: frames_scale_input=data['frames_scale_input']\n",
        "  if 'frames_skip_steps_input' in data: frames_skip_steps_input=data['frames_skip_steps_input']\n",
        "  if 'vr_mode' in data:\n",
        "    if data['vr_mode'] == 'True': vr_mode = True\n",
        "    else: vr_mode = False\n",
        "  if 'vr_eye_angle' in data: vr_eye_angle=data['vr_eye_angle']\n",
        "  if 'vr_ipd' in data: vr_ipd=data['vr_ipd']\n",
        "  if 'intermediate_saves' in data: intermediate_saves=data['intermediate_saves']\n",
        "  if 'intermediates_in_subfolder' in data:\n",
        "    if data['intermediates_in_subfolder'] == 'True': intermediates_in_subfolder = True\n",
        "    else: intermediates_in_subfolder = False   \n",
        "  if 'perlin_init' in data:\n",
        "    if data['perlin_init'] == 'True': perlin_init = True\n",
        "    else: perlin_init = False\n",
        "  if 'perlin_mode' in data: perlin_mode=data['perlin_mode']\n",
        "  if 'set_seed' in data: set_seed=data['set_seed']\n",
        "  if 'fuzzy_prompt' in data: fuzzy_prompt=data['fuzzy_prompt']\n",
        "  if 'rand_mag' in data: rand_mag=data['rand_mag']\n",
        "  if 'eta' in data: eta=data['eta']\n",
        "  if 'randomize_class' in data: randomize_class=data['randomize_class']\n",
        "  if 'clip_denoised' in data: clip_denoised=data['clip_denoised']\n",
        "  if 'clamp_grad' in data:\n",
        "    if data['clamp_grad'] == 'True': clamp_grad = True\n",
        "    else: clamp_grad = False    \n",
        "  if 'clamp_max_input' in data: clamp_max_input=data['clamp_max_input']\n",
        "  if 'cut_overview' in data: cut_overview=data['cut_overview']\n",
        "  if 'cut_innercut' in data: cut_innercut=data['cut_innercut']\n",
        "  if 'cut_ic_pow' in data: cut_ic_pow=data['cut_ic_pow']\n",
        "  if 'cut_icgray_p' in data: cut_icgray_p=data['cut_icgray_p']\n",
        "  if 'use_horizontal_symmetry' in data:\n",
        "    if data['use_horizontal_symmetry'] == 'True': use_horizontal_symmetry = True\n",
        "    else: use_horizontal_symmetry = False\n",
        "  if 'use_vertical_symmetry' in data:\n",
        "    if data['use_vertical_symmetry'] == 'True': use_vertical_symmetry = True\n",
        "    else: use_vertical_symmetry = False\n",
        "  if 'transformation_percent' in data: transformation_percent=data['transformation_percent']\n",
        "  \n",
        "  #absolute_frame_multiplier = \"\"\n",
        "  #antarctic_prompt_frames_input = \"\"\n",
        "  #antarctic_pre_string =\"\"\n",
        "  #antarctic_post_string =\"\"\n",
        "  #antarctic_number_of_captions =\"\"\n",
        "  #diffusion_model =\"\"\n",
        "  #use_secondary_model = True\n",
        "  #diffusion_sampling_mode=\"\"\n",
        "  #use_checkpoint = True\n",
        "  #ViTB32 = True\n",
        "  #ViTB16 = True\n",
        "  #ViTL14 = True\n",
        "  #ViTL14_336px = True\n",
        "  #RN101 = True\n",
        "  #RN50 = True\n",
        "  #RN50x4 = True\n",
        "  #RN50x16 = True\n",
        "  #RN50x64 = True\n",
        "  #check_model_SHA=\"\"\n",
        "  #steps=\"\"\n",
        "  #diffusion_steps=\"\"\n",
        "  #width_height=[]\n",
        "  #clip_guidance_scale_input=\"\"\n",
        "  #tv_scale_input=\"\"\n",
        "  #range_scale_input=\"\"\n",
        "  #sat_scale_input=\"\"\n",
        "  #cutn_batches_input=\"\"\n",
        "  #skip_augs = True\n",
        "  #soft_limiter_on = True\n",
        "  #soft_limiter_knee=\"\"\n",
        "  #init_image=\"\"\n",
        "  #init_scale=\"\"\n",
        "  #skip_steps=\"\"\n",
        "  #animation_mode=\"\"\n",
        "  #video_init_path=\"\"\n",
        "  #extract_nth_frame_input=\"\"\n",
        "  #video_init_seed_continuity = True\n",
        "  #max_frames_input=\"\"\n",
        "  #interp_spline=\"\"\n",
        "  #angle_input=\"\"\n",
        "  #zoom_input=\"\"\n",
        "  #object_detection_zoom_input=\"\"\n",
        "  #translation_x_input=\"\"\n",
        "  #translation_y_input=\"\"\n",
        "  #translation_z_input=\"\"\n",
        "  #rotation_3d_x_input=\"\"\n",
        "  #rotation_3d_y_input=\"\"\n",
        "  #rotation_3d_z_input=\"\"\n",
        "  #midas_depth_model=\"\"\n",
        "  #midas_weight_input=\"\"\n",
        "  #near_plane_input=\"\"\n",
        "  #far_plane_input=\"\"\n",
        "  #fov_input=\"\"\n",
        "  #padding_mode=\"\"\n",
        "  #sampling_mode=\"\" \n",
        "  #turbo_mode = True\n",
        "  #turbo_steps=\"\"\n",
        "  #turbo_preroll=\"\"\n",
        "  #frames_scale_input=\"\"\n",
        "  #frames_skip_steps_input=\"\"\n",
        "  #vr_mode = True\n",
        "  #vr_eye_angle=\"\"\n",
        "  #vr_ipd=\"\"\n",
        "  #intermediate_saves=\"\"\n",
        "  #intermediates_in_subfolder = True\n",
        "  #perlin_init = True\n",
        "  #perlin_mode=\"\"\n",
        "  #set_seed=\"\"\n",
        "  #fuzzy_prompt=\"\"\n",
        "  #rand_mag=\"\"\n",
        "  #eta=\"\"\n",
        "  #randomize_class=\"\"\n",
        "  #clip_denoised=\"\"\n",
        "  #clamp_grad = True\n",
        "  #clamp_max_input=\"\"\n",
        "  #cut_overview=\"\"\n",
        "  #cut_innercut=\"\"\n",
        "  #cut_ic_pow=\"\"\n",
        "  #cut_icgray_p=\"\"\n",
        "  #use_horizontal_symmetry = True\n",
        "  #use_vertical_symmetry = True\n",
        "  #transformation_percent=\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjIsWQ1tkWeh"
      },
      "source": [
        "# Generation Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5-zDikyFl7u"
      },
      "source": [
        "## 3. Antarctic Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrQEuymVwv0C"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "\n",
        "\n",
        "antarctic_additional_prompts = [\"text:-2\"]\n",
        "#antarctic_prompt_frames is antarctic_prompt_frames_input with every number multiplied by absolute_frame_multiplier\n",
        "#if there is a , in antarctic_prompt_frames_input then it will split it up and multiply each number by absolute_frame_multiplier\n",
        "#if there is no , then it will just multiply the number by absolute_frame_multiplier\n",
        "if antarctic_prompt_frames_input:\n",
        "  if 'x' in antarctic_prompt_frames_input:\n",
        "      antarctic_prompt_frames_num = antarctic_prompt_frames_input.replace('x', '')\n",
        "      antarctic_prompt_frames_mult = int(antarctic_prompt_frames_num) * absolute_frame_multiplier\n",
        "      antarctic_prompt_frames = \"x\"+str(antarctic_prompt_frames_mult)\n",
        "  elif \",\" in antarctic_prompt_frames_input:\n",
        "      antarctic_prompt_frames = [int(int(i) * absolute_frame_multiplier) for i in antarctic_prompt_frames_input.split(\",\")]\n",
        "      antarctic_prompt_frames = \",\".join(str(i) for i in antarctic_prompt_frames)\n",
        "  else:\n",
        "      antarctic_prompt_frames = [int(int(antarctic_prompt_frames_input)) * absolute_frame_multiplier]\n",
        "      antarctic_prompt_frames = \",\".join(str(i) for i in antarctic_prompt_frames)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiffClipSetTop"
      },
      "source": [
        "## 7. Diffusion and CLIP model settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ModelSettings"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model_256_SHA = '983e3de6f95c88c81b2ca7ebb2c217933be1973b1ff058776b970f901584613a'\n",
        "model_512_SHA = '9c111ab89e214862b76e1fa6a1b3f1d329b1a88281885943d2cdbe357ad57648'\n",
        "model_secondary_SHA = '983e3de6f95c88c81b2ca7ebb2c217933be1973b1ff058776b970f901584613a'\n",
        "\n",
        "model_256_link = 'https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt'\n",
        "model_512_link = 'https://v-diffusion.s3.us-west-2.amazonaws.com/512x512_diffusion_uncond_finetune_008100.pt'\n",
        "model_secondary_link = 'https://v-diffusion.s3.us-west-2.amazonaws.com/secondary_model_imagenet_2.pth'\n",
        "\n",
        "model_256_path = f'{model_path}/256x256_diffusion_uncond.pt'\n",
        "model_512_path = f'{model_path}/512x512_diffusion_uncond_finetune_008100.pt'\n",
        "model_secondary_path = f'{model_path}/secondary_model_imagenet_2.pth'\n",
        "\n",
        "# Download the diffusion model\n",
        "if diffusion_model == '256x256_diffusion_uncond':\n",
        "  if os.path.exists(model_256_path) and check_model_SHA:\n",
        "    print('Checking 256 Diffusion File')\n",
        "    with open(model_256_path,\"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest();\n",
        "    if hash == model_256_SHA:\n",
        "      print('256 Model SHA matches')\n",
        "      model_256_downloaded = True\n",
        "    else: \n",
        "      print(\"256 Model SHA doesn't match, redownloading...\")\n",
        "      wget(model_256_link, model_path)\n",
        "      model_256_downloaded = True\n",
        "  elif os.path.exists(model_256_path) and not check_model_SHA or model_256_downloaded == True:\n",
        "    print('256 Model already downloaded, check check_model_SHA if the file is corrupt')\n",
        "  else:  \n",
        "    wget(model_256_link, model_path)\n",
        "    model_256_downloaded = True\n",
        "elif diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n",
        "  if os.path.exists(model_512_path) and check_model_SHA:\n",
        "    print('Checking 512 Diffusion File')\n",
        "    with open(model_512_path,\"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest();\n",
        "    if hash == model_512_SHA:\n",
        "      print('512 Model SHA matches')\n",
        "      model_512_downloaded = True\n",
        "    else:  \n",
        "      print(\"512 Model SHA doesn't match, redownloading...\")\n",
        "      wget(model_512_link, model_path)\n",
        "      model_512_downloaded = True\n",
        "  elif os.path.exists(model_512_path) and not check_model_SHA or model_512_downloaded == True:\n",
        "    print('512 Model already downloaded, check check_model_SHA if the file is corrupt')\n",
        "  else:  \n",
        "    wget(model_512_link, model_path)\n",
        "    model_512_downloaded = True\n",
        "\n",
        "\n",
        "# Download the secondary diffusion model v2\n",
        "if use_secondary_model == True:\n",
        "  if os.path.exists(model_secondary_path) and check_model_SHA:\n",
        "    print('Checking Secondary Diffusion File')\n",
        "    with open(model_secondary_path,\"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest();\n",
        "    if hash == model_secondary_SHA:\n",
        "      print('Secondary Model SHA matches')\n",
        "      model_secondary_downloaded = True\n",
        "    else:  \n",
        "      print(\"Secondary Model SHA doesn't match, redownloading...\")\n",
        "      wget(model_secondary_link, model_path)\n",
        "      model_secondary_downloaded = True\n",
        "  elif os.path.exists(model_secondary_path) and not check_model_SHA or model_secondary_downloaded == True:\n",
        "    print('Secondary Model already downloaded, check check_model_SHA if the file is corrupt')\n",
        "  else:  \n",
        "    wget(model_secondary_link, model_path)\n",
        "    model_secondary_downloaded = True\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "if diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n",
        "        'image_size': 512,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 256,\n",
        "        'num_head_channels': 64,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })\n",
        "elif diffusion_model == '256x256_diffusion_uncond':\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 256,\n",
        "        'num_head_channels': 64,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })\n",
        "\n",
        "model_default = model_config['image_size']\n",
        "\n",
        "\n",
        "\n",
        "if use_secondary_model:\n",
        "    secondary_model = SecondaryDiffusionImageNet2()\n",
        "    secondary_model.load_state_dict(torch.load(f'{model_path}/secondary_model_imagenet_2.pth', map_location='cpu'))\n",
        "    secondary_model.eval().requires_grad_(False).to(device)\n",
        "\n",
        "clip_models = []\n",
        "if ViTB32: clip_models.append(clip.load('ViT-B/32', jit=False)[0].eval().requires_grad_(False).to(device))\n",
        "if ViTB16: clip_models.append(clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
        "if ViTL14: clip_models.append(clip.load('ViT-L/14', jit=False)[0].eval().requires_grad_(False).to(device))\n",
        "if ViTL14_336px: clip_models.append(clip.load('ViT-L/14@336px', jit=False)[0].eval().requires_grad_(False).to(device))\n",
        "if RN50: clip_models.append(clip.load('RN50', jit=False)[0].eval().requires_grad_(False).to(device))\n",
        "if RN50x4: clip_models.append(clip.load('RN50x4', jit=False)[0].eval().requires_grad_(False).to(device))\n",
        "if RN50x16: clip_models.append(clip.load('RN50x16', jit=False)[0].eval().requires_grad_(False).to(device))\n",
        "if RN50x64: clip_models.append(clip.load('RN50x64', jit=False)[0].eval().requires_grad_(False).to(device))\n",
        "if RN101: clip_models.append(clip.load('RN101', jit=False)[0].eval().requires_grad_(False).to(device))\n",
        "\n",
        "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SettingsTop"
      },
      "source": [
        "## 8. Image/Frame Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BasicSettings"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def absolute_key_frame_multiplier(original_string):\n",
        "    #if there is no colon in the string, return the original string\n",
        "    if not re.search(':', original_string):\n",
        "        return original_string\n",
        "    #if original_string doesn't include decimals\n",
        "    if not re.search('\\.', original_string):\n",
        "        multiplied_string = re.sub(r'\\d+', lambda x: str(int(x.group(0)) * absolute_frame_multiplier), original_string)\n",
        "        #print('mult',multiplied_string)\n",
        "    #divide all numbers in parentheses by absolute_frame_multiplier\n",
        "    # this brings them back to the original number since we only want to change frame numbers\n",
        "        return re.sub(r'\\((\\d+)\\)', lambda x: '(' + str(int(x.group(1)) // absolute_frame_multiplier) + ')', multiplied_string)\n",
        "    else:\n",
        "        #get all non-float numbers in the string\n",
        "        non_floats = re.compile(\"(?<![\\.\\d])[0-9]+(?![\\.\\d])\")\n",
        "        #sub all non-floats with themselves times absolute_frame_multiplier\n",
        "        return re.sub(non_floats, lambda x: str(int(x.group(0)) * absolute_frame_multiplier), original_string)\n",
        "\n",
        "clip_guidance_scale = absolute_key_frame_multiplier(clip_guidance_scale_input)\n",
        "\n",
        "tv_scale = absolute_key_frame_multiplier(tv_scale_input)\n",
        "\n",
        "range_scale = absolute_key_frame_multiplier(range_scale_input)\n",
        "\n",
        "sat_scale = absolute_key_frame_multiplier(sat_scale_input)\n",
        "\n",
        "cutn_batches = absolute_key_frame_multiplier(cutn_batches_input)\n",
        "\n",
        "if soft_limiter_knee < 0.5 or soft_limiter_knee > .999:\n",
        "  soft_limiter_knee = .98\n",
        "  print('soft_limiter_knee out of range. Automatically reset to 0.98')\n",
        "\n",
        "\n",
        "\n",
        "#Get corrected sizes\n",
        "side_x = (width_height[0]//64)*64;\n",
        "side_y = (width_height[1]//64)*64;\n",
        "if side_x != width_height[0] or side_y != width_height[1]:\n",
        "  print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')\n",
        "\n",
        "#Update Model Settings\n",
        "timestep_respacing = f'ddim{steps}'\n",
        "diffusion_steps = (1000//steps)*steps if steps < 1000 else steps\n",
        "model_config.update({\n",
        "    'timestep_respacing': timestep_respacing,\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "})\n",
        "\n",
        "#Make folder for batch\n",
        "batchFolder = f'{outDirPath}/{batch_name}'\n",
        "createPath(batchFolder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnimSetTop"
      },
      "source": [
        "### 8.1. Animation Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnimSettings"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "extract_nth_frame = extract_nth_frame_input * absolute_frame_multiplier\n",
        "\n",
        "\n",
        "if animation_mode == \"Video Input\":\n",
        "  if is_colab:\n",
        "      videoFramesFolder = f'/content/videoFrames'\n",
        "  else:\n",
        "      videoFramesFolder = f'videoFrames'\n",
        "  createPath(videoFramesFolder)\n",
        "  print(f\"Exporting Video Frames (1 every {extract_nth_frame})...\")\n",
        "  try:\n",
        "    for f in pathlib.Path(f'{videoFramesFolder}').glob('*.jpg'):\n",
        "      f.unlink()\n",
        "  except:\n",
        "    print('')\n",
        "  vf = f'select=not(mod(n\\,{extract_nth_frame}))'\n",
        "  subprocess.run(['ffmpeg', '-i', f'{video_init_path}', '-vf', f'{vf}', '-vsync', 'vfr', '-q:v', '2', '-loglevel', 'error', '-stats', f'{videoFramesFolder}/%04d.jpg'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  #!ffmpeg -i {video_init_path} -vf {vf} -vsync vfr -q:v 2 -loglevel error -stats {videoFramesFolder}/%04d.jpg\n",
        "\n",
        "\n",
        "max_frames = max_frames_input * absolute_frame_multiplier\n",
        "\n",
        "if animation_mode == \"Video Input\":\n",
        "  max_frames = len(glob(f'{videoFramesFolder}/*.jpg'))\n",
        "\n",
        "\n",
        "angle = absolute_key_frame_multiplier(angle_input)\n",
        "zoom = absolute_key_frame_multiplier(zoom_input)\n",
        "object_detection_zoom = object_detection_zoom_input\n",
        "translation_x = absolute_key_frame_multiplier(translation_x_input)\n",
        "translation_y = absolute_key_frame_multiplier(translation_y_input)\n",
        "translation_z = absolute_key_frame_multiplier(translation_z_input)\n",
        "rotation_3d_x = absolute_key_frame_multiplier(rotation_3d_x_input)\n",
        "rotation_3d_y = absolute_key_frame_multiplier(rotation_3d_y_input)\n",
        "rotation_3d_z = absolute_key_frame_multiplier(rotation_3d_z_input)\n",
        "midas_weight = absolute_key_frame_multiplier(midas_weight_input)\n",
        "near_plane = absolute_key_frame_multiplier(near_plane_input)\n",
        "far_plane = absolute_key_frame_multiplier(far_plane_input)\n",
        "fov = absolute_key_frame_multiplier(fov_input)\n",
        "\n",
        "#======= TURBO MODE\n",
        "\n",
        "turbo_preroll = 10 # frames\n",
        "\n",
        "#insist turbo be used only w 3d anim.\n",
        "if turbo_mode and animation_mode != '3D':\n",
        "  print('=====')\n",
        "  print('Turbo mode only available with 3D animations. Disabling Turbo.')\n",
        "  print('=====')\n",
        "  turbo_mode = False\n",
        "\n",
        "\n",
        "frames_scale = absolute_key_frame_multiplier(frames_scale_input)\n",
        "\n",
        "frames_skip_steps = absolute_key_frame_multiplier(frames_skip_steps_input)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#insist VR be used only w 3d anim.\n",
        "if vr_mode and animation_mode != '3D':\n",
        "  print('=====')\n",
        "  print('VR mode only available with 3D animations. Disabling VR.')\n",
        "  print('=====')\n",
        "  vr_mode = False\n",
        "\n",
        "def parse_key_frames(string, prompt_parser=None):\n",
        "    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n",
        "    return a dictionary with the frame numbers as keys and the parameter values as the values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    string: string\n",
        "        Frame numbers paired with parameter values at that frame number, in the format\n",
        "        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n",
        "    prompt_parser: function or None, optional\n",
        "        If provided, prompt_parser will be applied to each string of parameter values.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Frame numbers as keys, parameter values at that frame number as values\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    RuntimeError\n",
        "        If the input string does not match the expected format.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n",
        "    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n",
        "\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n",
        "    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n",
        "    \"\"\"\n",
        "    #import re\n",
        "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
        "    frames = dict()\n",
        "    for match_object in re.finditer(pattern, string):\n",
        "        frame = int(match_object.groupdict()['frame'])\n",
        "        param = match_object.groupdict()['param']\n",
        "        if prompt_parser:\n",
        "            frames[frame] = prompt_parser(param)\n",
        "        else:\n",
        "            frames[frame] = param\n",
        "\n",
        "    if frames == {} and len(string) != 0:\n",
        "        raise RuntimeError('Key Frame string not correctly formatted')\n",
        "    return frames\n",
        "    \n",
        "animations_with_keyframes_higher_than_max = []\n",
        "def get_inbetweens(name, key_frames, between_max_frames, integer=False):\n",
        "    \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n",
        "    return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n",
        "    Any values not provided in the input dict are calculated by linear interpolation between\n",
        "    the values of the previous and next provided frames. If there is no previous provided frame, then\n",
        "    the value is equal to the value of the next provided frame, or if there is no next provided frame,\n",
        "    then the value is equal to the value of the previous provided frame. If no frames are provided,\n",
        "    all frame values are NaN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    key_frames: dict\n",
        "        A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n",
        "    integer: Bool, optional\n",
        "        If True, the values of the output series are converted to integers.\n",
        "        Otherwise, the values are floats.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        A Series with length max_frames representing the parameter values for each frame.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> max_frames = 5\n",
        "    >>> get_inbetweens({1: 5, 3: 6})\n",
        "    0    5.0\n",
        "    1    5.0\n",
        "    2    5.5\n",
        "    3    6.0\n",
        "    4    6.0\n",
        "    dtype: float64\n",
        "\n",
        "    >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n",
        "    0    5\n",
        "    1    5\n",
        "    2    5\n",
        "    3    6\n",
        "    4    6\n",
        "    dtype: int64\n",
        "    \"\"\"\n",
        "    key_frame_series = pd.Series([np.nan for a in range(between_max_frames)])\n",
        "    \n",
        "    for i, value in key_frames.items():\n",
        "        key_frame_series[i] = value\n",
        "        if i>between_max_frames:\n",
        "          animations_with_keyframes_higher_than_max.append(name)\n",
        "    key_frame_series = key_frame_series.astype(float)\n",
        "    \n",
        "    interp_method = interp_spline\n",
        "\n",
        "    if interp_method == 'Cubic' and len(key_frames.items()) <=3:\n",
        "      interp_method = 'Quadratic'\n",
        "    \n",
        "    if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n",
        "      interp_method = 'Linear'      \n",
        "    \n",
        "    key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n",
        "    key_frame_series[between_max_frames-1] = key_frame_series[key_frame_series.last_valid_index()]\n",
        "    # key_frame_series = key_frame_series.interpolate(method=intrp_method,order=1, limit_direction='both')\n",
        "    key_frame_series = key_frame_series.interpolate(method=interp_method.lower(),limit_direction='both')\n",
        "    if integer:\n",
        "        return key_frame_series.astype(int)\n",
        "    return key_frame_series\n",
        "\n",
        "def split_prompts(prompts):\n",
        "  prompt_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "  for i, prompt in prompts.items():\n",
        "    prompt_series[i] = prompt\n",
        "  # prompt_series = prompt_series.astype(str)\n",
        "  prompt_series = prompt_series.ffill().bfill()\n",
        "  return prompt_series\n",
        "\n",
        "used_series = []\n",
        "\n",
        "def try_to_get_inbetweens(name, string, between_max_frames):\n",
        "    used_series.append(name) #needs work\n",
        "    # if name == \"new_eta\" or name == \"new_clamp_max\":\n",
        "    #   #import re\n",
        "    #   #print('string1',string)\n",
        "    #   string = re.sub(r\"[-+]?(?:\\d*\\.\\d+)\", lambda x: str(int(float(x.group()) * 1000)), string)\n",
        "    #   #print('string',string)\n",
        "    #   try:\n",
        "    #       in_betweens = get_inbetweens(name, parse_key_frames(string))\n",
        "    #       #print('in_betweens', in_betweens)\n",
        "    #       to_return = in_betweens.divide(other = 1000)\n",
        "    #       #print('to_return', to_return)\n",
        "    #       return to_return\n",
        "    #   except RuntimeError as e:\n",
        "    #       print(\n",
        "    #           \"WARNING: You have selected to use key frames, but you have not \"\n",
        "    #           \"formatted `\"+name+\"` correctly for key frames.\\n\"\n",
        "    #           \"Attempting to interpret `\"+name+\"` as \"\n",
        "    #           f'\"0: ({string})\"\\n'\n",
        "    #           \"Please read the instructions to find out how to use key frames \"\n",
        "    #           \"correctly.\\n\"\n",
        "    #       )\n",
        "    #       string = f\"0: ({string})\"\n",
        "    #       in_betweens = get_inbetweens(name, parse_key_frames(string))\n",
        "    #       to_return = in_betweens.divide(other = 1000)\n",
        "    #       return to_return    \n",
        "    # else:\n",
        "    try:\n",
        "        return get_inbetweens(name, parse_key_frames(string), between_max_frames)\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `\"+name+\"` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `\"+name+\"` as \"\n",
        "            f'\"0: ({string})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        string = f\"0: ({string})\"\n",
        "        return get_inbetweens(name, parse_key_frames(string), between_max_frames)\n",
        "\n",
        "def con_try_to_get_inbetweens(name, string, between_max_frames):\n",
        "    if \":\" not in str(string): #tj test this\n",
        "      string = \"0: (\"+str(string)+\")\"\n",
        "      #print(string)\n",
        "    else:\n",
        "      used_series.append(name)\n",
        "    if name == \"new_eta\" or name == \"new_clamp_max\":\n",
        "      import re\n",
        "      #print('string1',string)\n",
        "      string = re.sub(r\"[-+]?(?:\\d*\\.\\d+)\", lambda x: str(int(float(x.group()) * 1000)), string)\n",
        "      #print('string',string)\n",
        "      try:\n",
        "          in_betweens = get_inbetweens(name, parse_key_frames(string), between_max_frames)\n",
        "          #print('in_betweens', in_betweens)\n",
        "          to_return = in_betweens.divide(other = 1000)\n",
        "          #print('to_return', to_return)\n",
        "          return to_return\n",
        "      except RuntimeError as e:\n",
        "          print(\n",
        "              \"WARNING: You have selected to use key frames, but you have not \"\n",
        "              \"formatted `\"+name+\"` correctly for key frames.\\n\"\n",
        "              \"Attempting to interpret `\"+name+\"` as \"\n",
        "              f'\"0: ({string})\"\\n'\n",
        "              \"Please read the instructions to find out how to use key frames \"\n",
        "              \"correctly.\\n\"\n",
        "          )\n",
        "          string = f\"0: ({string})\"\n",
        "          in_betweens = get_inbetweens(name, parse_key_frames(string), between_max_frames)\n",
        "          to_return = in_betweens.divide(other = 1000)\n",
        "          return to_return    \n",
        "    else:\n",
        "      try:\n",
        "          return get_inbetweens(name, parse_key_frames(string), between_max_frames)\n",
        "      except RuntimeError as e:\n",
        "          print(\n",
        "              \"WARNING: You have selected to use key frames, but you have not \"\n",
        "              \"formatted `\"+name+\"` correctly for key frames.\\n\"\n",
        "              \"Attempting to interpret `\"+name+\"` as \"\n",
        "              f'\"0: ({string})\"\\n'\n",
        "              \"Please read the instructions to find out how to use key frames \"\n",
        "              \"correctly.\\n\"\n",
        "          )\n",
        "          string = f\"0: ({string})\"\n",
        "          return get_inbetweens(name, parse_key_frames(string), between_max_frames)\n",
        "\n",
        "if key_frames:\n",
        "    angle_series = try_to_get_inbetweens(\"angle\", angle, max_frames)\n",
        "    zoom_series = try_to_get_inbetweens(\"zoom\", zoom, max_frames)\n",
        "    translation_x_series = try_to_get_inbetweens(\"translation_x\", translation_x, max_frames)\n",
        "    translation_y_series = try_to_get_inbetweens(\"translation_y\", translation_y, max_frames)\n",
        "    translation_z_series = try_to_get_inbetweens(\"translation_z\", translation_z, max_frames)\n",
        "    rotation_3d_x_series = try_to_get_inbetweens(\"rotation_3d_x\", rotation_3d_x, max_frames)\n",
        "    rotation_3d_y_series = try_to_get_inbetweens(\"rotation_3d_y\", rotation_3d_y, max_frames)\n",
        "    rotation_3d_z_series = try_to_get_inbetweens(\"rotation_3d_z\", rotation_3d_z, max_frames)\n",
        "    new_cutn_batches_series = con_try_to_get_inbetweens(\"new_cutn_batches\", cutn_batches, max_frames)\n",
        "    new_clip_guidance_scale_series = con_try_to_get_inbetweens(\"new_clip_guidance_scale\", clip_guidance_scale, max_frames)\n",
        "    new_tv_scale_series = con_try_to_get_inbetweens(\"new_tv_scale\", tv_scale, max_frames)\n",
        "    new_range_scale_series = con_try_to_get_inbetweens(\"new_range_scale\", range_scale, max_frames)\n",
        "    new_sat_scale_series = con_try_to_get_inbetweens(\"new_sat_scale\", sat_scale, max_frames)\n",
        "    new_midas_weight_series = con_try_to_get_inbetweens(\"new_midas_weight\", midas_weight, max_frames)\n",
        "    new_near_plane_series = con_try_to_get_inbetweens(\"new_near_plane\", near_plane, max_frames)\n",
        "    new_far_plane_series = con_try_to_get_inbetweens(\"new_far_plane\", far_plane, max_frames)\n",
        "    new_fov_series = con_try_to_get_inbetweens(\"new_fov\", fov, max_frames)\n",
        "    new_frames_scale_series = con_try_to_get_inbetweens(\"new_frames_scale\", frames_scale, max_frames)\n",
        "    new_frames_skip_steps_series = con_try_to_get_inbetweens(\"new_frames_skip_steps\", frames_skip_steps, max_frames)\n",
        "#make a note of how to do thisss shit! https://stackoverflow.com/questions/23227927/how-do-i-move-master-back-several-commits-in-git\n",
        "else:\n",
        "    angle = float(angle)\n",
        "    zoom = float(zoom)\n",
        "    translation_x = float(translation_x)\n",
        "    translation_y = float(translation_y)\n",
        "    translation_z = float(translation_z)\n",
        "    rotation_3d_x = float(rotation_3d_x)\n",
        "    rotation_3d_y = float(rotation_3d_y)\n",
        "    rotation_3d_z = float(rotation_3d_z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExtraSetTop"
      },
      "source": [
        "### 8.2. Extra Settings\n",
        " Partial Saves, Advanced Settings, Cutn Scheduling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExtraSettings"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "if type(intermediate_saves) is not list:\n",
        "  if intermediate_saves:\n",
        "    steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))\n",
        "    steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1\n",
        "    print(f'Will save every {steps_per_checkpoint} steps')\n",
        "  else:\n",
        "    steps_per_checkpoint = steps+10\n",
        "else:\n",
        "  steps_per_checkpoint = None\n",
        "\n",
        "if intermediate_saves and intermediates_in_subfolder is True:\n",
        "  partialFolder = f'{batchFolder}/partials'\n",
        "  createPath(partialFolder)\n",
        "\n",
        "eta = absolute_key_frame_multiplier(eta_input)\n",
        "\n",
        "clamp_max = absolute_key_frame_multiplier(clamp_max_input)\n",
        "\n",
        "### EXTRA ADVANCED SETTINGS:\n",
        "randomize_class = True\n",
        "clip_denoised = False\n",
        "fuzzy_prompt = False\n",
        "rand_mag = 0.05\n",
        "\n",
        "\n",
        "cut_ic_pow = absolute_key_frame_multiplier(cut_ic_pow_input)\n",
        "\n",
        "\n",
        "if key_frames:\n",
        "  new_eta_series = con_try_to_get_inbetweens(\"new_eta\", eta, max_frames)\n",
        "  new_clamp_max_series = con_try_to_get_inbetweens(\"new_clamp_max\",  clamp_max, max_frames)\n",
        "  new_cut_ic_pow_series = con_try_to_get_inbetweens(\"new_cut_ic_pow\",  cut_ic_pow, max_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fmJsoSjgw_E"
      },
      "source": [
        "## Box Zoom Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkpVVP2qgw_E"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "!pip install jupyter_bbox_widget\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "import ipywidgets as widgets\n",
        "import json\n",
        "import os\n",
        "import base64\n",
        "guided_zoom_widget = BBoxWidget(\n",
        "image='/content/drive/MyDrive/AI/Disco_Diffusion/init_images/TimeToDisco(1)_0.png',\n",
        "classes=['zoom'],\n",
        ")#todo delete the stuff out of here that we dont actually need\n",
        "box_zoom_variable_updates = {\n",
        "  'translation_x_series': {},\n",
        "  'translation_y_series': {},\n",
        "  'zoom_series': {},\n",
        "  'rot_mat_center_series': [],\n",
        "  'zoom_series_list': [],\n",
        "  'frames_scale_series_list': [],\n",
        "  'frames_skip_steps_series_list': [],\n",
        "}\n",
        "\n",
        "def key_frame_multiplierx(original_string):#add next frame to these as an argument!!!!!!!!!!!!!!!\n",
        "    #if there is no colon in the string, return the original string\n",
        "    if not re.search(':', original_string):\n",
        "        return original_string\n",
        "    #if original_string doesn't include decimals\n",
        "    if not re.search('\\.', original_string):\n",
        "        multiplied_string = re.sub(r'\\d+', lambda x: str(int(x.group(0)) * absolute_frame_multiplier), original_string)\n",
        "        #print('mult',multiplied_string)\n",
        "    #divide all numbers in parentheses by absolute_frame_multiplier\n",
        "    # this brings them back to the original number since we only want to change frame numbers\n",
        "        return re.sub(r'\\((\\d+)\\)', lambda x: '(' + str(int(x.group(1)) // absolute_frame_multiplier) + ')', multiplied_string)\n",
        "    else:\n",
        "        #get all non-float numbers in the string\n",
        "        non_floats = re.compile(\"(?<![\\.\\d])[0-9]+(?![\\.\\d])\")\n",
        "        #sub all non-floats with themselves times absolute_frame_multiplier\n",
        "        return re.sub(non_floats, lambda x: str(int(x.group(0)) * absolute_frame_multiplier), original_string)\n",
        "\n",
        "def guided_widget_button_clicked(button):\n",
        "  global box_zoom_variable_updates\n",
        "  should_endless_zoom = False\n",
        "  translate_box_to_center = True\n",
        "  guided_zoom_widget.bboxes\n",
        "  frames_to_center = 30\n",
        "  if button == 'skip':\n",
        "    should_endless_zoom = guided_zoom_widget.bboxes[0]['guided_skip_btn_endless_zoom_input']\n",
        "    translate_box_to_center = guided_zoom_widget.bboxes[0]['guided_skip_btn_translate_box_to_center_input']\n",
        "    frames_to_center = int(guided_zoom_widget.bboxes[0]['guided_skip_btn_frames_to_center_input'])\n",
        "    guided_zoom_input = guided_zoom_widget.bboxes[0]['guided_skip_btn_zoom_input']\n",
        "    guided_frames_scale_input = guided_zoom_widget.bboxes[0]['guided_skip_btn_frames_scale_input']\n",
        "    guided_frames_skip_steps_input = guided_zoom_widget.bboxes[0]['guided_skip_btn_frames_skip_steps_input']\n",
        "  elif button == 'submit':\n",
        "    should_endless_zoom = guided_zoom_widget.bboxes[0]['guided_submit_btn_endless_zoom_input']\n",
        "    translate_box_to_center = guided_zoom_widget.bboxes[0]['guided_submit_btn_translate_box_to_center_input']\n",
        "    frames_to_center = int(guided_zoom_widget.bboxes[0]['guided_submit_btn_frames_to_center_input'])\n",
        "    guided_zoom_input = guided_zoom_widget.bboxes[0]['guided_submit_btn_zoom_input']\n",
        "    guided_frames_scale_input = guided_zoom_widget.bboxes[0]['guided_submit_btn_frames_scale_input']\n",
        "    guided_frames_skip_steps_input = guided_zoom_widget.bboxes[0]['guided_submit_btn_frames_skip_steps_input']\n",
        "  next_frame = len(glob(batchFolder+f\"/{batch_name}({batchNum})_*.png\"))\n",
        "  # if should_endless_zoom:\n",
        "  #   box_zoom_variable_updates['zoom_series_list'] = try_to_get_inbetweens('guided_zoom_input', guided_zoom_input, max_frames).tolist()\n",
        "  # else:\n",
        "  #   box_zoom_variable_updates['zoom_series_list'] = try_to_get_inbetweens('guided_zoom_input', guided_zoom_input, translate_box_to_center).tolist()\n",
        "  #zoom_series.update(pd.Series(box_zoom_variable_updates['zoom_series_list'], index = list(range(next_frame, next_frame+frames_to_center))))\n",
        "\n",
        "  #guided_frames_scale_input = \"0:(1.0),100:(1.05)\"\n",
        "  # zoom_series = try_to_get_inbetweens(\"zoom\", \"0:(1.0),100:(1.05)\", max_frames)\n",
        "  # print('zoom_series', zoom_series.tolist())\n",
        "  # print('guided_frames_scale_input', guided_frames_scale_input)\n",
        "\n",
        "  # guided_frames_scale_list = try_to_get_inbetweens('guided_frames_scale_input', guided_frames_scale_input, max_frames).tolist()\n",
        "  # print('guided_frames_scale_list', guided_frames_scale_list)\n",
        "  \n",
        "  print('guided_frames_scale_input', guided_frames_scale_input)\n",
        "  guided_frames_scale_list = try_to_get_inbetweens('guided_frames_scale_input', guided_frames_scale_input, frames_to_center).tolist()\n",
        "  print('guided_frames_scale_list', guided_frames_scale_list)\n",
        "  new_frames_scale_series.update(pd.Series(guided_frames_scale_list, index = list(range(next_frame, next_frame+len(guided_frames_scale_list)))))\n",
        "  guided_frames_skip_steps_series_list = try_to_get_inbetweens('guided_frames_skip_steps_input', guided_frames_skip_steps_input, frames_to_center).tolist()\n",
        "  new_frames_skip_steps_series.update(pd.Series(guided_frames_skip_steps_series_list, index = list(range(next_frame, next_frame+len(guided_frames_skip_steps_series_list)))))\n",
        "  # guided_zoom_widget.bboxes\n",
        "  # print(guided_zoom_widget.bboxes)\n",
        "  box_x = guided_zoom_widget.bboxes[0]['x']\n",
        "  box_y = guided_zoom_widget.bboxes[0]['y']\n",
        "  box_width = guided_zoom_widget.bboxes[0]['width']\n",
        "  box_height = guided_zoom_widget.bboxes[0]['height']\n",
        "  print('box dims', box_x, box_y, box_width, box_height)\n",
        "  zoom_scale = determine_zoom_scale(box_width, box_height, box_x, box_y, frames_to_center)\n",
        "  zoom_scale_list = [zoom_scale for _ in range(frames_to_center)]\n",
        "  zoom_series.update(pd.Series(zoom_scale_list, index = list(range(next_frame, next_frame+len(zoom_scale_list)))))\n",
        "  #zoom_scale only works if we are zooming in and not doing endless zoom because it decides what the zoom should be \n",
        "  # to make the box area be equal to the area of the whole image. \n",
        "  box_center_x = box_x + box_width/2\n",
        "  box_center_y = box_y + box_height/2\n",
        "  x_translate_per_iteration = int(((args.width_height[0]//2) - box_center_x) / frames_to_center)\n",
        "  y_translate_per_iteration = int(((args.width_height[1]//2) - box_center_y) / frames_to_center)  \n",
        "  guided_translation_x_list = [x_translate_per_iteration for _ in range(frames_to_center)]\n",
        "  translation_x_series.update(pd.Series(guided_translation_x_list, index = list(range(next_frame, next_frame+len(guided_translation_x_list)))))\n",
        "  guided_translation_y_list = [y_translate_per_iteration for _ in range(frames_to_center)]\n",
        "  translation_y_series.update(pd.Series(guided_translation_y_list, index = list(range(next_frame, next_frame+len(guided_translation_y_list)))))  \n",
        "  box_zoom_variable_updates['rot_mat_center_series'] = [() for _ in range(max_frames + 30)]\n",
        "  #find of if this is the code to make a list of a bunch of ints that are all the same so we can add it to the pd series\n",
        "  for frame in range(next_frame, next_frame+frames_to_center):\n",
        "    iteration_number = frame + 1 - next_frame\n",
        "    # box_zoom_variable_updates['translation_x_series'][frame] = x_translate_per_iteration\n",
        "    # box_zoom_variable_updates['translation_y_series'][frame] = y_translate_per_iteration\n",
        "    # box_zoom_variable_updates['zoom_series'][frame] = zoom_scale\n",
        "    box_zoom_variable_updates['rot_mat_center_series'][frame] = (box_center_x+(iteration_number*x_translate_per_iteration), box_center_y+(iteration_number*y_translate_per_iteration))\n",
        "\n",
        "def on_submit():\n",
        "  guided_widget_button_clicked('submit')\n",
        "\n",
        "def on_skip():\n",
        "  guided_widget_button_clicked('skip')\n",
        "\n",
        "guided_zoom_widget.on_submit(on_submit)\n",
        "guided_zoom_widget.on_skip(on_skip)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkWzPClDkWel"
      },
      "source": [
        "# Box Zoom Widget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0l86N_ngw_F"
      },
      "outputs": [],
      "source": [
        "empty_label = widgets.Label(value=\" \")\n",
        "skip_header_label = widgets.Label(value=\"SKIP\")\n",
        "submit_header_label = widgets.Label(value=\"SUBMIT\")\n",
        "guided_skip_btn_endless_zoom_checkbox = widgets.Checkbox(value=False,description='Endless zoom',disabled=False,indent=False)\n",
        "guided_zoom_widget.attach(guided_skip_btn_endless_zoom_checkbox, name='guided_skip_btn_endless_zoom_input')\n",
        "guided_submit_btn_endless_zoom_checkbox = widgets.Checkbox(value=False,description='Endless zoom',disabled=False,indent=False)\n",
        "guided_zoom_widget.attach(guided_submit_btn_endless_zoom_checkbox, name='guided_submit_btn_endless_zoom_input')\n",
        "guided_skip_btn_translate_box_to_center_checkbox = widgets.Checkbox(value=True,description='Translate to center',disabled=False,indent=False)\n",
        "guided_zoom_widget.attach(guided_skip_btn_translate_box_to_center_checkbox, name='guided_skip_btn_translate_box_to_center_input')\n",
        "guided_submit_btn_translate_box_to_center_checkbox = widgets.Checkbox(value=True,description='Translate to center',disabled=False,indent=False)\n",
        "guided_zoom_widget.attach(guided_submit_btn_translate_box_to_center_checkbox, name='guided_submit_btn_translate_box_to_center_input')\n",
        "guided_skip_btn_frames_to_center_textbox = widgets.Text(value='30',placeholder='Type something',description='Frames to center:',disabled=False,style= {'description_width': 'initial'})\n",
        "guided_zoom_widget.attach(guided_skip_btn_frames_to_center_textbox, name='guided_skip_btn_frames_to_center_input')\n",
        "guided_submit_btn_frames_to_center_textbox = widgets.Text(value='30',placeholder='Type something',description='Frames to center:',disabled=False,style= {'description_width': 'initial'})\n",
        "guided_zoom_widget.attach(guided_submit_btn_frames_to_center_textbox, name='guided_submit_btn_frames_to_center_input')\n",
        "guided_skip_btn_zoom_textbox = widgets.Text(value='0: (1.0), 30: (1.01)', placeholder='Type something', description='Guided zoom input:', disabled=False,style= {'description_width': 'initial'})\n",
        "guided_zoom_widget.attach(guided_skip_btn_zoom_textbox, name='guided_skip_btn_zoom_input')\n",
        "guided_submit_btn_zoom_textbox = widgets.Text(value='0: (1.0), 30: (1.01)', placeholder='Type something', description='Guided zoom input:', disabled=False,style= {'description_width': 'initial'})\n",
        "guided_zoom_widget.attach(guided_submit_btn_zoom_textbox, name='guided_submit_btn_zoom_input')\n",
        "guided_skip_btn_frames_scale_textbox = widgets.Text(value='0: (1000)', placeholder='Type something', description='Guided frames scale input:', disabled=False,style= {'description_width': 'initial'})\n",
        "guided_zoom_widget.attach(guided_skip_btn_frames_scale_textbox, name='guided_skip_btn_frames_scale_input')\n",
        "guided_submit_btn_frames_scale_textbox = widgets.Text(value='0: (1000)', placeholder='Type something', description='Guided frames scale input:', disabled=False,style= {'description_width': 'initial'})\n",
        "guided_zoom_widget.attach(guided_submit_btn_frames_scale_textbox, name='guided_submit_btn_frames_scale_input')\n",
        "guided_skip_btn_frames_skip_steps_textbox = widgets.Text(value='0: (60)', placeholder='Type something', description='Guided frames skip steps input:', disabled=False,style= {'description_width': 'initial'})\n",
        "guided_zoom_widget.attach(guided_skip_btn_frames_skip_steps_textbox, name='guided_skip_btn_frames_skip_steps_input')\n",
        "guided_submit_btn_frames_skip_steps_textbox = widgets.Text(value='0: (60)', placeholder='Type something', description='Guided frames skip steps input:', disabled=False,style= {'description_width': 'initial'})\n",
        "guided_zoom_widget.attach(guided_submit_btn_frames_skip_steps_textbox, name='guided_submit_btn_frames_skip_steps_input')\n",
        "items = [guided_zoom_widget, empty_label, skip_header_label,submit_header_label,guided_skip_btn_endless_zoom_checkbox, guided_submit_btn_endless_zoom_checkbox,guided_skip_btn_translate_box_to_center_checkbox, guided_submit_btn_translate_box_to_center_checkbox,\n",
        "        guided_skip_btn_frames_to_center_textbox,guided_submit_btn_frames_to_center_textbox, guided_skip_btn_zoom_textbox,guided_submit_btn_zoom_textbox,\n",
        "        guided_skip_btn_frames_scale_textbox,guided_submit_btn_frames_scale_textbox ,guided_skip_btn_frames_skip_steps_textbox, guided_submit_btn_frames_skip_steps_textbox]\n",
        "guided_zoom_widget_box = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(2, 600px)\"), disabled =False)\n",
        "guided_zoom_widget_box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiffuseTop"
      },
      "source": [
        "# Diffuse Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoTheRun"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#Update Model Settings\n",
        "timestep_respacing = f'ddim{steps}'\n",
        "diffusion_steps = (1000//steps)*steps if steps < 1000 else steps\n",
        "model_config.update({\n",
        "    'timestep_respacing': timestep_respacing,\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "})\n",
        "\n",
        "batch_size = 1 \n",
        "\n",
        "def move_files(start_num, end_num, old_folder, new_folder):\n",
        "    for i in range(start_num, end_num):\n",
        "        old_file = old_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n",
        "        new_file = new_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n",
        "        os.rename(old_file, new_file)\n",
        "\n",
        "\n",
        "if retain_overwritten_frames is True:\n",
        "  retainFolder = f'{batchFolder}/retained'\n",
        "  createPath(retainFolder)\n",
        "\n",
        "\n",
        "#skip_step_ratio = int(frames_skip_steps.rstrip(\"%\")) / 100\n",
        "#calc_frames_skip_steps = math.floor(steps * skip_step_ratio)\n",
        "\n",
        "# calc_frames_skip_steps_series = []\n",
        "# if len(anim_steps_series) > 0:\n",
        "#   for anim_steps_item in anim_steps_series:\n",
        "#     calc_frames_skip_steps_series.append(math.floor(anim_steps_item * skip_step_ratio))\n",
        "\n",
        "\n",
        "calc_frames_skip_steps_series = []\n",
        "if len(new_frames_skip_steps_series) > 0:\n",
        "  for frames_skip_steps_item in new_frames_skip_steps_series:\n",
        "    skip_step_ratio = int(frames_skip_steps_item) / 100\n",
        "    calc_frames_skip_steps_series.append(math.floor(steps * skip_step_ratio))\n",
        "\n",
        "\n",
        "\n",
        "# if steps <= calc_frames_skip_steps:\n",
        "#   sys.exit(\"ERROR: You can't skip more steps than your total steps\")\n",
        "\n",
        "if resume_run:\n",
        "  if run_to_resume == 'latest':\n",
        "    try:\n",
        "      batchNum\n",
        "    except:\n",
        "      batchNum = len(glob(f\"{batchFolder}/{batch_name}(*)_settings.txt\"))-1\n",
        "  else:\n",
        "    batchNum = int(run_to_resume)\n",
        "  if resume_from_frame == 'latest':\n",
        "    start_frame = len(glob(batchFolder+f\"/{batch_name}({batchNum})_*.png\"))\n",
        "    if animation_mode != '3D' and turbo_mode == True and start_frame > turbo_preroll and start_frame % int(turbo_steps) != 0:\n",
        "      start_frame = start_frame - (start_frame % int(turbo_steps))\n",
        "  else:\n",
        "    start_frame = int(resume_from_frame)+1\n",
        "    if animation_mode != '3D' and turbo_mode == True and start_frame > turbo_preroll and start_frame % int(turbo_steps) != 0:\n",
        "      start_frame = start_frame - (start_frame % int(turbo_steps))\n",
        "    if retain_overwritten_frames is True:\n",
        "      existing_frames = len(glob(batchFolder+f\"/{batch_name}({batchNum})_*.png\"))\n",
        "      frames_to_save = existing_frames - start_frame\n",
        "      print(f'Moving {frames_to_save} frames to the Retained folder')\n",
        "      move_files(start_frame, existing_frames, batchFolder, retainFolder)\n",
        "else:\n",
        "  start_frame = 0\n",
        "  batchNum = len(glob(batchFolder+\"/*.txt\"))\n",
        "  while os.path.isfile(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\") is True or os.path.isfile(f\"{batchFolder}/{batch_name}-{batchNum}_settings.txt\") is True:\n",
        "    batchNum += 1\n",
        "\n",
        "print(f'Starting Run: {batch_name}({batchNum}) at frame {start_frame}')\n",
        "\n",
        "if set_seed == 'random_seed':\n",
        "    random.seed()\n",
        "    seed = random.randint(0, 2**32)\n",
        "    # print(f'Using seed: {seed}')\n",
        "else:\n",
        "    seed = int(set_seed)\n",
        "updated_antarctic_text_prompts = text_prompts\n",
        "#if f'{batchFolder}/antarctic_text_prompts.json' exists, use it\n",
        "if os.path.isfile(f'{batchFolder}/antarctic_text_prompts.json'):\n",
        "    with open(f'{batchFolder}/antarctic_text_prompts.json') as f:\n",
        "        updated_antarctic_text_prompts = json.load(f)\n",
        "args = {\n",
        "    'batchNum': batchNum,\n",
        "    'text_prompts': text_prompts,\n",
        "    'prompts_series':split_prompts(text_prompts) if text_prompts else None,\n",
        "    'image_prompts_series':split_prompts(image_prompts) if image_prompts else None,\n",
        "    'seed': seed,\n",
        "    'display_rate':display_rate,\n",
        "    'n_batches':n_batches if animation_mode == 'None' else 1,\n",
        "    'batch_size':batch_size,\n",
        "    'batch_name': batch_name,\n",
        "    'steps': steps,\n",
        "    'diffusion_sampling_mode': diffusion_sampling_mode,\n",
        "    'width_height': width_height,\n",
        "    'clip_guidance_scale': clip_guidance_scale,\n",
        "    'tv_scale': tv_scale,\n",
        "    'range_scale': range_scale,\n",
        "    'sat_scale': sat_scale,\n",
        "    'cutn_batches': cutn_batches,\n",
        "    'soft_limiter_on': soft_limiter_on,\n",
        "    'soft_limiter_knee': soft_limiter_knee,\n",
        "    'init_image': init_image,\n",
        "    'init_scale': init_scale,\n",
        "    'skip_steps': skip_steps,\n",
        "    'side_x': side_x,\n",
        "    'side_y': side_y,\n",
        "    'timestep_respacing': timestep_respacing,\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "    'animation_mode': animation_mode,\n",
        "    'video_init_path': video_init_path,\n",
        "    'extract_nth_frame': extract_nth_frame,\n",
        "    'video_init_seed_continuity': video_init_seed_continuity,\n",
        "    'key_frames': key_frames,\n",
        "    'max_frames': max_frames if animation_mode != \"None\" else 1,\n",
        "    'interp_spline': interp_spline,\n",
        "    'start_frame': start_frame,\n",
        "    'angle': angle,\n",
        "    'zoom': zoom,\n",
        "    'object_detection_zoom': object_detection_zoom,\n",
        "    'translation_x': translation_x,\n",
        "    'translation_y': translation_y,\n",
        "    'translation_z': translation_z,\n",
        "    'rotation_3d_x': rotation_3d_x,\n",
        "    'rotation_3d_y': rotation_3d_y,\n",
        "    'rotation_3d_z': rotation_3d_z,\n",
        "    'midas_depth_model': midas_depth_model,\n",
        "    'midas_weight': midas_weight,\n",
        "    'near_plane': near_plane,\n",
        "    'far_plane': far_plane,\n",
        "    'fov': fov,\n",
        "    'padding_mode': padding_mode,\n",
        "    'sampling_mode': sampling_mode,\n",
        "    'angle_series':angle_series,\n",
        "    'zoom_series':zoom_series,\n",
        "    'translation_x_series':translation_x_series,\n",
        "    'translation_y_series':translation_y_series,\n",
        "    'translation_z_series':translation_z_series,\n",
        "    'rotation_3d_x_series':rotation_3d_x_series,\n",
        "    'rotation_3d_y_series':rotation_3d_y_series,\n",
        "    'rotation_3d_z_series':rotation_3d_z_series,\n",
        "    'frames_scale': frames_scale,\n",
        "    #'calc_frames_skip_steps': calc_frames_skip_steps,\n",
        "    'skip_step_ratio': skip_step_ratio,\n",
        "    # 'new_cutn_batches': new_cutn_batches,\n",
        "    # 'new_clip_guidance_scale': new_clip_guidance_scale,\n",
        "    # 'new_tv_scale': new_tv_scale,\n",
        "    # 'new_range_scale': new_range_scale,\n",
        "    # 'new_sat_scale': new_sat_scale,\n",
        "    # 'new_midas_weight': new_midas_weight,\n",
        "    # 'new_near_plane': new_near_plane,\n",
        "    # 'new_far_plane': new_far_plane,\n",
        "    # 'new_fov': new_fov,\n",
        "    # 'new_frames_scale': new_frames_scale,\n",
        "    # 'new_frames_skip_steps': new_frames_skip_steps,\n",
        "    # 'new_eta': new_eta,\n",
        "    # 'new_clamp_max': new_clamp_max,\n",
        "    'cut_ic_pow': cut_ic_pow,\n",
        "    'new_cutn_batches_series': new_cutn_batches_series,\n",
        "    'new_clip_guidance_scale_series': new_clip_guidance_scale_series,\n",
        "    'new_tv_scale_series': new_tv_scale_series,\n",
        "    'new_range_scale_series': new_range_scale_series,\n",
        "    'new_sat_scale_series': new_sat_scale_series,\n",
        "    'new_midas_weight_series': new_midas_weight_series,\n",
        "    'new_near_plane_series': new_near_plane_series,\n",
        "    'new_far_plane_series': new_far_plane_series,\n",
        "    'new_fov_series': new_fov_series,\n",
        "    'new_frames_scale_series': new_frames_scale_series,\n",
        "    'new_eta_series': new_eta_series,\n",
        "    'new_clamp_max_series':new_clamp_max_series,\n",
        "    'new_cut_ic_pow_series':new_cut_ic_pow_series,\n",
        "    #'calc_frames_skip_steps': calc_frames_skip_steps,\n",
        "    'calc_frames_skip_steps_series': calc_frames_skip_steps_series,\n",
        "    'text_prompts': text_prompts,\n",
        "    'image_prompts': image_prompts,\n",
        "    'cut_overview': eval(cut_overview),\n",
        "    'cut_innercut': eval(cut_innercut),\n",
        "    'cut_ic_pow': cut_ic_pow,\n",
        "    'cut_icgray_p': eval(cut_icgray_p),\n",
        "    'intermediate_saves': intermediate_saves,\n",
        "    'intermediates_in_subfolder': intermediates_in_subfolder,\n",
        "    'steps_per_checkpoint': steps_per_checkpoint,\n",
        "    'perlin_init': perlin_init,\n",
        "    'perlin_mode': perlin_mode,\n",
        "    'set_seed': set_seed,\n",
        "    'eta': eta,\n",
        "    'clamp_grad': clamp_grad,\n",
        "    'clamp_max': clamp_max,\n",
        "    'skip_augs': skip_augs,\n",
        "    'randomize_class': randomize_class,\n",
        "    'clip_denoised': clip_denoised,\n",
        "    'fuzzy_prompt': fuzzy_prompt,\n",
        "    'rand_mag': rand_mag,\n",
        "    'use_vertical_symmetry': use_vertical_symmetry,\n",
        "    'use_horizontal_symmetry': use_horizontal_symmetry,\n",
        "    'transformation_percent': transformation_percent,\n",
        "    'used_series': used_series,\n",
        "    'animations_with_keyframes_higher_than_max': animations_with_keyframes_higher_than_max,\n",
        "    'box_zoom_variable_updates': box_zoom_variable_updates,\n",
        "}\n",
        "\n",
        "args = SimpleNamespace(**args)\n",
        "\n",
        "print('Prepping model...')\n",
        "model, diffusion = create_model_and_diffusion(**model_config)\n",
        "model.load_state_dict(torch.load(f'{model_path}/{diffusion_model}.pt', map_location='cpu'))\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "for name, param in model.named_parameters():\n",
        "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "        param.requires_grad_()\n",
        "if model_config['use_fp16']:\n",
        "    model.convert_to_fp16()\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdzSQIL82KGN"
      },
      "source": [
        "# Video Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASQWrJXagw_G"
      },
      "source": [
        "## **Create the video**\n",
        "### **Create video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BecaISWkgw_G"
      },
      "outputs": [],
      "source": [
        "#@markdown Video file will save in the same folder as your images.\n",
        "\n",
        "skip_video_for_run_all = False #@param {type: 'boolean'}\n",
        "make_regular_video = False #@param {type: 'boolean'}\n",
        "make_info_video = False #@param {type: 'boolean'}\n",
        "make_custom_text_overlay_video = True #@param {type: 'boolean'}\n",
        "use_unique_prompt_as_custom_overlay = True #@param {type: 'boolean'}\n",
        "make_prompt_overlay_video = False #@param {type: 'boolean'}\n",
        "make_interpolation_video = True #@param {type: 'boolean'}\n",
        "folder = batch_name #@param\n",
        "latest_run=batchNum\n",
        "run = latest_run #@param\n",
        "init_frame = 1#@param {type:\"number\"} This is the frame where the video will start\n",
        "final_frame=\"final_frame\"\n",
        "last_frame = final_frame#@param {type:\"number\"} You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "fps = 12#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "# view_video_in_cell = True #@param {type: 'boolean'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Custom Overlay Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "famous_characters_text=[\n",
        "    \"Menes (3000 BC) Egyptian pharaoh who founded the First Dynasty\",\n",
        "    \"Moses (1391-1271 BC) Key figure of Jewish history\",\n",
        "    \"Zoroaster (1200 BC) Iranian prophet founded Zoroastrianism\",\n",
        "    \"Homer (928 BC) Greek poet who wrote Iliad and Odyssey\",\n",
        "    \"Cyrus the Great (602-530 BC) Founder of the Achaemenid Empire\",\n",
        "    \"Mahavira (601 BC) Principal figure of Jainism.\",\n",
        "    \"Lao Tzu (600 BC) Author of Tao Te Ching, founder of Taoism\",\n",
        "    \"Buddha (563-483 BC) Founder of Buddhism.\",\n",
        "    \"Confucius (551-479 BC) Chinese philosopher.\",\n",
        "    \"Plato (424-348 BC) Greek philosopher.\",\n",
        "    \"Aristotle (384-322 BC) Greek philosopher and polymath\",\n",
        "    \"Alexander the Great (356-323 BC) King of Macedonia and military leader\",\n",
        "    \"Euclid (325-265 BC) Greek mathematician\",\n",
        "    \"King Ashoka (260-232 BC) Indian King\",\n",
        "    \"Shih Huang Ti (259-210 BC) King of the state of Qin\",\n",
        "    \"Julius Caesar (100 BC-44 BC) Roman ruler\",\n",
        "    \"Augustus Caesar (63 BC-14) First Emperor of Rome\",\n",
        "    \"Mencius (38-03BC) Chinese philosopher\",\n",
        "    \"Jesus of Nazareth (5 BC-30 AD) Central figure of Christianity\",\n",
        "    \"St. Paul (5-67) Christian missionary\",\n",
        "    \"Ts'ai Lun (50-121) Inventor of paper\",\n",
        "    \"Mani (216) Iranian founder of Manichaeism\",\n",
        "    \"Constantine the Great (272-337) Roman Emperor\",\n",
        "    \"St. Augustine (354-430) Influential Christian saint\",\n",
        "    \"Justinian I  (482-565) Emperor of Eastern Roman Empire\",\n",
        "    \"Emperor Wen of Sui (541-604) Founder of China's Sui Dynasty\",\n",
        "    \"Muhammad (570-632) Prophet of Islam\",\n",
        "    \"Umar ibn al-Khattab (584-644) Powerful Muslim Caliphate\",\n",
        "    \"Charlemagne (742-814) First western Emperor since the fall of Rome\",\n",
        "    \"William the Conqueror (1028-1087) First Norman King of England\",\n",
        "    \"Pope Urban II (1042-1099) Influential Pope who\",\n",
        "    \"Genghis Kahn (1162-1227) Leader of the Mongols\",\n",
        "    \"Johann Gutenberg (1395-1468) Inventor of the printing press\",\n",
        "    \"Christopher Columbus (1451-1506) Italian explorer landed in America\",\n",
        "    \"Queen Isabella I (1451-1504) Queen of Castille\",\n",
        "    \"Vasco da Gama (1460s-524) Portuguese explorer\",\n",
        "    \"Nicoli Machiavelli (1469-1527) Italian diplomat, father of political science\",\n",
        "    \"Francisco Pizarro (1471-1541) Spanish Conquistador\",\n",
        "    \"Nicolaus Copernicus (1473-1543) Renaissance mathematician and astronomer\",\n",
        "    \"Michelangelo (1475-1564) Renaissance sculptor, painter and architect\",\n",
        "    \"Martin Luther (1483-1546) Sought to reform the Roman Catholic Church\",\n",
        "    \"Hernando Cortes (1485-1547) Spanish Conquistador\",\n",
        "    \"John Calvin (1509-27 May 1564) Christian theologian\",\n",
        "    \"Queen Elizabeth I (1533-1603) Queen of England\",\n",
        "    \"Francis Bacon (1561- 1626) Creator of the scientific method\",\n",
        "    \"Galileo Galilei (1564-1642) Italian scientist\",\n",
        "    \"William Shakespeare (1564-1616) English poet and playwright\",\n",
        "    \"Johannes Kepler (1571-1630) German mathematician and astronomer, created laws of planetary motion\",\n",
        "    \"William Harvey (1578-1657) English physician, blood circulation\",\n",
        "    \"Rene Descartes (1596-1650) French philosopher and mathematician\",\n",
        "    \"Oliver Cromwell (1599-1658) Leader of Parliamentarians in English civil war\",\n",
        "    \"Antony van Leeuwenhoek (1632-1723) Dutch chemist, founder of microbiology\",\n",
        "    \"John Locke (1632-1704) English political philosopher, theory of liberal democracy\",\n",
        "    \"Isaac Newton (1642-1727) British mathematician and scientist\",\n",
        "    \"Johann Sebastian Bach (1685-1750) Composer and organist\",\n",
        "    \"Voltaire (1694-1778) Key figure of European Enlightenment, satirist\",\n",
        "    \"Leonhard Euler (1707-1783) Swiss mathematician, calculus and graph theory\",\n",
        "    \"Jean-Jacques Rousseau (1712-1778) French philosopher, author of Social Contract\",\n",
        "    \"Peter the Great (1721-1725) Russian Emperor, expanded the Tsarist Empire\",\n",
        "    \"Adam Smith (1723-1790) Scottish social philosopher, classical economics\",\n",
        "    \"George Washington (1732-1799) 1st President of US\",\n",
        "    \"James Watt (1736-1819) Scottish engineer, steam engine\",\n",
        "    \"Antoine Laurent Lavoisier (1743-1794) French chemist and biologist, chemical revolution\",\n",
        "    \"Thomas Jefferson (1743-1826) 3rd President of US, Declaration of Independence\",\n",
        "    \"Edward Jenner (1749-1823) Developed the world's first vaccine\",\n",
        "    \"John Dalton (1766-1844) English chemist and physicist, atomic theory\",\n",
        "    \"Thomas Malthus (1766-1834) English scholar who raised concern over growing population\",\n",
        "    \"Napoleon Bonaparte (1769-1821) French military and political leader\",\n",
        "    \"Ludwig van Beethoven (1770-1827) German composer\",\n",
        "    \"Louis Daguerre (1780-1851) French artist and photographer, inventor of the camera\",\n",
        "    \"Simon Bolivar (1783-1830) Liberator of Latin American countries\",\n",
        "    \"Michael Faraday (1791-1867) English scientist, electromagnetism and electrochemistry\",\n",
        "    \"Charles Darwin (1809-1882) Scientist who proposed theory of evolution\",\n",
        "    \"Karl Marx (1818-1883) German Communist philosopher\",\n",
        "    \"William T.G. Morton (1819-1868) American dentist, pioneered anaesthetic\",\n",
        "    \"Louis Pasteur (1822-1895) French biologist, cure for rabies and other infectious diseases\",\n",
        "    \"Gregor Mendel (1822-1884) Czech/Austrian scientist, founded science of genetics\",\n",
        "    \"Joseph Lister (1827-1912) British surgeon, sterilisation and antiseptic surgery\",\n",
        "    \"James Clerk Maxwell (1831-1879) Scottish physicist, electromagnetism\",\n",
        "    \"Nikolaus August Otto (1832-1891) German engineer, petrol engine\",\n",
        "    \"Wilhelm Conrad Roentgen (1845-1923) German physicist, electromagnetic waves or X-rays\",\n",
        "    \"Thomas Edison (1847-1931) Inventor and businessman, helped introduce electricity and light bulbs\",\n",
        "    \"Alexander Graham Bell (1847-1922) Scottish inventor, telephone\",\n",
        "    \"Sigmund Freud (1856-1939) Austrian neurologist, psychoanalysis and subconscious\",\n",
        "    \"Max Planck (1858-1947) German theoretical physicist, Quantum physics\",\n",
        "    \"Henry Ford (1863-1947) Owner of Ford, revolutionised mass-production\",\n",
        "    \"Lenin (1870-1924) Leader of the Russian Revolution\",\n",
        "    \"Orville and Wilbur Wright Orville (1871-1948) Created and flew the first aeroplane\",\n",
        "    \"Ernest Rutherford (1871-1937) NZ born British physicist, atomic physics\",\n",
        "    \"Guglielmo Marconi (1874-1937) Italian engineer, radio transmission\",\n",
        "    \"Joseph Stalin (1878-1953) Absolute ruler of the Soviet Union\",\n",
        "    \"Albert Einstein (1879-1955) German/ US scientist, Theory of Relativity\",\n",
        "    \"Alexander Fleming (1881-1955) Scottish biologist who discovered penicillin\",\n",
        "    \"Adolf Hitler (1889-1945) Dictator of Nazi Germany\",\n",
        "    \"Mao Zedong (1893-1976) Leader of the Communist Revolution\",\n",
        "    \"Werner Heisenberg (1900-1976) German theoretical physicist, Quantum mechanics\",\n",
        "    \"Enrico Fermi (1901-1954) Italian-American physicist, nuclear reactor\",\n",
        "    \"Gregory Pincus (1903-1967) American biologist, oral contraceptive pill\",\n",
        "    \"John F. Kennedy (1917-1963) 38th President of the US, Cuban Missile Crisis\",\n",
        "    \"Mikhail Gorbachev (1931-) Leader of Soviet Communist Party\"\n",
        "]\n",
        "\n",
        "\n",
        "custom_overlay_text = {}\n",
        "\n",
        "    \n",
        "for i in range(400):\n",
        "      if i % 4 == 0:\n",
        "          custom_overlay_text[i] = famous_characters_text[i//4]  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzdGU8_qgw_G"
      },
      "source": [
        "## **FILM Interpolation**\n",
        "### **1. Setup FILM Interpolation Workspace**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFFyDnMtgw_G"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@markdown Run this cell to perform setup.\n",
        "\n",
        "#@markdown Mounting your google drive is optional.\n",
        "#@markdown If you mount your drive, code and models will be downloaded to it.\n",
        "#@markdown This should reduce setup time after your first run.\n",
        "\n",
        "mount_gdrive = True # @param{type:\"boolean\"}\n",
        "\n",
        "frames_to_interpolate = \"custom text\" #@param [\"standard\", \"custom text\"]\n",
        "if make_custom_text_overlay_video==False:\n",
        "    frames_to_interpolate = \"standard\"\n",
        "gdrive_mountpoint = '/content/drive/' #@param{type:\"string\"}\n",
        "gdrive_subdirectory = 'MyDrive/interpolation' #@param{type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "#@markdown \n",
        "\n",
        "#@markdown Specify the directory containing your video frames with the `frames_dir` parameter.\n",
        "\n",
        "\n",
        "\n",
        "#@markdown A single pass of the interpolation procedure adds a frame between each contiguous pair of frames in `frames_dir`.\n",
        "\n",
        "#@markdown If you start with $n$ frames in `frames_dir` and set `recursive_interpolation_passes` to $k$, your total number of frames\n",
        "#@markdown after interpolation will be: \n",
        "#@markdown $$2^k (n-1) -1$$\n",
        "\n",
        "#recursive_interpolation_passes = 1 #@param{'type':'integer'}\n",
        "number_of_interpolations =  4#@param{'type':'integer'}\n",
        "#@markdown Check this box to generate a video output. If no output video will be generated, the FPS option can be ignored.\n",
        "output_video = True #@param{'type':'boolean'}\n",
        "output_video_fps = 30 #@param{'type':'number'}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQAAnokLgw_H"
      },
      "source": [
        "\n",
        "## **Create Gist**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTV5kHTsgw_H"
      },
      "outputs": [],
      "source": [
        "should_create_gist=True #@param{type:'boolean'}\n",
        "github_username=\"tjthejuggler\" #@param{type:'string'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otZK_CbVgw_H"
      },
      "source": [
        "## **Get Music**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6ytirtUgw_H"
      },
      "outputs": [],
      "source": [
        "ccmixter_search_url_or_keyword=\"http://dig.ccmixter.org/search?searchp=spider\" #@param{type:'string'}\n",
        "automatically_choose_music=True #@param{type:'boolean'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsl0px-vgw_H"
      },
      "source": [
        "## **Change Speed of a Video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oCIjj99gw_H"
      },
      "outputs": [],
      "source": [
        "finished_video_length=60 #@param{type:\"raw\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L2RVNMsgw_H"
      },
      "source": [
        "\n",
        "## **Create Youtube Video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EpjhnQk2OlU"
      },
      "outputs": [],
      "source": [
        "show_first_prompt_in_youtube_description=True #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkttXyIxgw_I"
      },
      "source": [
        "## Video Settings Overwrite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEqxtgM9gw_I"
      },
      "outputs": [],
      "source": [
        "if 'settings.txt' in overwrite_settings_file:\n",
        "  json_file=open(overwrite_settings_file)\n",
        "  data = json.load(json_file)\n",
        "  if 'skip_video_for_run_all' in data: skip_video_for_run_all = data['skip_video_for_run_all']\n",
        "  if 'make_regular_video' in data: make_regular_video = data['make_regular_video']\n",
        "  if 'make_info_video' in data: make_info_video = data['make_info_video']\n",
        "  if 'make_unique_prompt_overlay_video' in data: make_unique_prompt_overlay_video = data['make_unique_prompt_overlay_video'],\n",
        "  if 'make_prompt_overlay_video' in data: make_prompt_overlay_video = data['make_prompt_overlay_video']\n",
        "  if 'make_interpolation_video' in data: make_interpolation_video = data['make_interpolation_video']\n",
        "  if 'fps' in data: fps = data['fps'] \n",
        "#skip_video_for_run_all = \"\"\n",
        "#make_regular_video = \"\"\n",
        "#make_info_video = \"\"\n",
        "#make_unique_prompt_overlay_video = \"\"\n",
        "#make_prompt_overlay_video = \"\"\n",
        "#make_interpolation_video = \"\"\n",
        "#fps = \"\"\n",
        "save_settings() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR9rKptjkWep"
      },
      "source": [
        "# Show Diffuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHKs41hYgw_G"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  do_run()\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "finally:\n",
        "    print('Seed used:', seed)\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouojILLiQHGt"
      },
      "source": [
        "# Save Promt Generation Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZyZ4oQ0QHGt"
      },
      "outputs": [],
      "source": [
        "from requests import get\n",
        "import os\n",
        "import json\n",
        "dir = os.path.dirname(os.path.realpath(\"_file_\"))\n",
        "dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "filename = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n",
        "json_file=open(dir +'/' + filename)\n",
        "data = json.load(json_file)\n",
        "full_source_code_list=[]\n",
        "for i in range(len(data[\"cells\"])):\n",
        "  for line in data[\"cells\"][i][\"source\"]:\n",
        "    full_source_code_list.append(line)\n",
        "prompt_generator_list=[]\n",
        "should_append_string_to_prompt_generator_list=False\n",
        "for string in full_source_code_list:\n",
        "  if '#this comment is used to indicate the end of prompt creation DONT CHANGE!!!!' in string:\n",
        "    break\n",
        "  if should_append_string_to_prompt_generator_list:\n",
        "    prompt_generator_list.append(string)\n",
        "  elif '#this comment is used to indicate the beginning of prompt creation DONT CHANGE!!!!' in string:\n",
        "      should_append_string_to_prompt_generator_list=True\n",
        "prompt_generator_file_path=f\"{batchFolder}/{batch_name}({batchNum})_prompt_generator.txt\"\n",
        "with open(prompt_generator_file_path, \"w\") as output:\n",
        "    output.writelines(prompt_generator_list)\n",
        "    output.close()\n",
        "if not os.path.exists(\"/content/drive/MyDrive/AI/Pop_Diffusion/all_prompt_generator.txt\"):\n",
        " f=open(\"/content/drive/MyDrive/AI/Pop_Diffusion/all_prompt_generator.txt\",\"w\")\n",
        "\n",
        "#check if the prompt generator is empty\n",
        "if len(prompt_generator_list)>0:\n",
        "  new_data_to_write = \"---------------------------------------------\\n\"\n",
        "  new_data_to_write += batch_name+\"\\n\"\n",
        "  new_data_to_write += \"---------------------------------------------\\n\"\n",
        "  for line in prompt_generator_list:\n",
        "    new_data_to_write += line\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/AI/Pop_Diffusion/all_prompt_generator.txt\", \"r\") as output:\n",
        "      f=output.read()\n",
        "  if not new_data_to_write in f:\n",
        "    all_data_to_write = f + \"\\n\"+new_data_to_write\n",
        "    with open(\"/content/drive/MyDrive/AI/Pop_Diffusion/all_prompt_generator.txt\", \"w\") as output:\n",
        "      output.write(all_data_to_write)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw4rngvygw_I"
      },
      "source": [
        "# Video and post-production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CreateVidTop"
      },
      "source": [
        "## 11. Create the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CreateVid"
      },
      "outputs": [],
      "source": [
        "from PIL import Image,ImageFont,ImageDraw\n",
        "\n",
        "print('text_prompts_begin', text_prompts)\n",
        "\n",
        "def get_unique_text_prompts(orig_dict):\n",
        "    original_dict = orig_dict.copy()\n",
        "    def find_largest_common_substring(arr):\n",
        "        list_length = len(arr)\n",
        "        reference_item = arr[0]\n",
        "        reference_item_length = len(reference_item) \n",
        "        res = \"\"\n",
        "        for i in range(reference_item_length):\n",
        "            for j in range(i + 1, reference_item_length + 1):\n",
        "                stem = reference_item[i:j]\n",
        "                k = 1\n",
        "                for k in range(1, list_length):\n",
        "                    if stem not in arr[k]:\n",
        "                        break\n",
        "                if (k + 1 == list_length and len(res) < len(stem)):\n",
        "                    res = stem \n",
        "        return res\n",
        "    unique_text_prompts = {}\n",
        "    for prompt in original_dict:\n",
        "        original_dict[prompt] = ' '.join(original_dict[prompt])\n",
        "    for i in range(2):\n",
        "        text_prompts_list = list(original_dict.values())\n",
        "        largest_common = find_largest_common_substring(text_prompts_list)\n",
        "        for frame in original_dict:\n",
        "            unique_text_prompts[frame] = original_dict[frame].replace(largest_common, '')\n",
        "        original_dict = unique_text_prompts.copy()\n",
        "    for item in unique_text_prompts:\n",
        "        unique_text_prompts[item] = [unique_text_prompts[item]]\n",
        "    return unique_text_prompts\n",
        "\n",
        "using_labels = False\n",
        "if skip_video_for_run_all == True:\n",
        "  print('Skipping video creation, uncheck skip_video_for_run_all if you want to run it')\n",
        "else:\n",
        "  # import subprocess in case this cell is run without the above cells\n",
        "  import subprocess\n",
        "  from base64 import b64encode\n",
        "  latest_run = batchNum\n",
        "  \n",
        "  label_folder = 'label'\n",
        "  prompt_folder = 'prompt'\n",
        "  unique_prompt_folder = 'unique_prompt'\n",
        "  custom_overlay_folder='custom_overlay'\n",
        " \n",
        "  final_frame = 'final_frame'\n",
        "  frames = []\n",
        "\n",
        "\n",
        "  if last_frame == 'final_frame':\n",
        "    last_frame = len(glob(batchFolder+f\"/{folder}({run})_*.png\"))\n",
        "    print(f'Total frames: {last_frame}')\n",
        "\n",
        "  dict_of_series = {\n",
        "    'cutn_batches': args.new_cutn_batches_series, \n",
        "    'clip_guidance': args.new_clip_guidance_scale_series,\n",
        "    'tv_scale': args.new_tv_scale_series,\n",
        "    'range_scale': args.new_range_scale_series,\n",
        "    'sat_scale': args.new_sat_scale_series,  \n",
        "    'midas_weight': args.new_midas_weight_series, \n",
        "    'frames_scale': args.new_frames_scale_series, \n",
        "    'eta': args.new_eta_series,\n",
        "    'clamp_max': args.new_clamp_max_series,\n",
        "    'cut_ic_pow': args.new_cut_ic_pow_series,\n",
        "    'frames_skip_steps': calc_frames_skip_steps_series,\n",
        "    'text_prompts': text_prompts,\n",
        "  }\n",
        "  print('text_prompts_begin2', text_prompts)\n",
        "  custom_overlay_text = custom_overlay_text.copy()\n",
        "  if make_custom_text_overlay_video and make_unique_prompt_overlay_video:\n",
        "    custom_overlay_text = get_unique_text_prompts(text_prompts).copy()\n",
        "    print('custom_overlay_text', custom_overlay_text)\n",
        "    \n",
        "  #print('text_prompts', text_prompts)\n",
        "  #print('updated_antarctic_text_prompts',updated_antarctic_text_prompts)\n",
        "  prompt_object = text_prompts.copy()\n",
        "  if os.path.isfile(f'{batchFolder}/antarctic_text_prompts.json'):\n",
        "    print('setting this')\n",
        "    prompt_object = {int(key):value for key, value in updated_antarctic_text_prompts.items()}\n",
        "    dict_of_series['text_prompts'] = prompt_object\n",
        "  print('text_prompts', text_prompts)\n",
        "  print('prompt_object', prompt_object)\n",
        "  \n",
        "  def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
        "    # initialize the dimensions of the image to be resized and grab the image size\n",
        "    dim = None\n",
        "    (h, w) = image.shape[:2]\n",
        "    # if both the width and height are None, then return the original image\n",
        "    if width is None and height is None:\n",
        "        return image\n",
        "    # check to see if the width is None\n",
        "    if width is None:\n",
        "        # calculate the ratio of the height and construct the dimensions\n",
        "        r = height / float(h)\n",
        "        dim = (int(w * r), height)\n",
        "    # otherwise, the height is None\n",
        "    else:\n",
        "        # calculate the ratio of the width and construct the dimensions\n",
        "        r = width / float(w)\n",
        "        dim = (width, int(h * r))\n",
        "    # resize the image\n",
        "    resized = cv2.resize(image, dim, interpolation = inter)\n",
        "    # return the resized image\n",
        "    return resized\n",
        "  \n",
        "  def add_padding_to_image(image_name, image_index):\n",
        "      image = cv2.imread(f\"{outDirPath}/{folder}/{image_name}\")\n",
        "      old_image_height, old_image_width, channels = image.shape\n",
        "      #if the image is not a perfect square then add padding to it\n",
        "      if old_image_width > 500 :\n",
        "        image = image_resize(image, width = 500)\n",
        "        old_image_height, old_image_width, channels = image.shape\n",
        "      if old_image_height != old_image_width:\n",
        "        new_image_height = old_image_width\n",
        "        new_image_width = old_image_width\n",
        "        new_image = np.zeros((new_image_height, new_image_width, channels), np.uint8)\n",
        "        new_image[0:old_image_height, 0:old_image_width] = image\n",
        "        image = new_image\n",
        "      old_image_height, old_image_width, channels = image.shape\n",
        "      # create new image of desired size and color (blue) for padding        \n",
        "      new_image_height = old_image_height + (old_image_height//24)\n",
        "      new_image_width = 16*(new_image_height//9)\n",
        "      print(old_image_height, old_image_width, new_image_height, new_image_width)\n",
        "      color = (255,0,0)\n",
        "      result = np.full((new_image_height,new_image_width, channels), color, dtype=np.uint8)\n",
        "      # compute center offset\n",
        "      x_center = (new_image_width - old_image_width) // 2 + (new_image_width-new_image_height) // 2\n",
        "      y_center = (new_image_height - old_image_height) // 2\n",
        "      # copy image image into center of result image\n",
        "      result[y_center:y_center+old_image_height, x_center:x_center+old_image_width] = image      \n",
        "      for dict_index, (key, value) in enumerate(dict_of_series.items()):\n",
        "          if key == 'text_prompts':\n",
        "            #get the highest key in text_prompts that is lower than image_index\n",
        "            #key_index = next(key for key, value in dict_of_series[\"text_prompts\"].items() if key <= image_index)\n",
        "            this_value = dict_of_series[\"text_prompts\"].get(max(k for k in dict_of_series[\"text_prompts\"] if k <= image_index))\n",
        "            this_prompt = this_value\n",
        "            color_coverted = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
        "            pil_image=Image.fromarray(color_coverted)\n",
        "            draw = ImageDraw.Draw(pil_image)\n",
        "            # font = ImageFont.truetype(<font-file>, <font-size>)\n",
        "            font = ImageFont.truetype(\"arial.ttf\", 36)\n",
        "            # draw.text((x, y),\"Sample Text\",(r,g,b))\n",
        "            draw.text((0, 0),str(this_prompt),(255,255,255),font=font)\n",
        "         \n",
        "            # cv2.putText(result, text=str(this_prompt), org=(10,30),\n",
        "            #     fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.4, color=(255,255,255),\n",
        "            #     thickness=1, lineType=cv2.LINE_AA)\n",
        "          else:\n",
        "            raw_value = value[image_index]\n",
        "            #limit raw_value to 4 decimal places\n",
        "            raw_value = round(raw_value, 4) \n",
        "            color_coverted = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
        "            pil_image=Image.fromarray(color_coverted)\n",
        "            draw = ImageDraw.Draw(pil_image)\n",
        "            # font = ImageFont.truetype(<font-file>, <font-size>)\n",
        "            font = ImageFont.truetype(\"arial.ttf\", 36)\n",
        "            # draw.text((x, y),\"Sample Text\",(r,g,b))\n",
        "            draw.text((0, 0),key+ \": \" + str(raw_value),(255,255,255),font=font)            \n",
        "            # cv2.putText(result, text=key+ \": \" + str(raw_value), org=(10,60+(dict_index*30)),\n",
        "            #     fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, color=(255,255,255),\n",
        "            #     thickness=2, lineType=cv2.LINE_AA)\n",
        "      # plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "      # cv2.imwrite(f\"{outDirPath}/{folder}/{label_folder}/{image_name}\", result)\n",
        "      pil_image.show()\n",
        "      pil_image.save(f\"{outDirPath}/{folder}/{label_folder}/{image_name}\")\n",
        "  def add_prompt_to_image(image_name, image_index, unique_prompt):\n",
        "    image = cv2.imread(f\"{outDirPath}/{folder}/{image_name}\")\n",
        "    print('image_index', image_index)\n",
        "    if unique_prompt:\n",
        "      this_prompt = custom_overlay_text.get(max(k for k in dict_of_series[\"text_prompts\"] if k <= image_index))\n",
        "    else:\n",
        "      this_prompt = dict_of_series[\"text_prompts\"].get(max(k for k in dict_of_series[\"text_prompts\"] if k <= image_index))\n",
        "    print('this_prompt', this_prompt)\n",
        "    for prompt_item_index, prompt in enumerate(this_prompt) :\n",
        "      color_coverted = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      pil_image=Image.fromarray(color_coverted)\n",
        "      draw = ImageDraw.Draw(pil_image)\n",
        "      # font = ImageFont.truetype(<font-file>, <font-size>)\n",
        "      font = ImageFont.truetype(\"/content/drive/MyDrive/AI/font/arial.ttf\", 36)\n",
        "      # draw.text((x, y),\"Sample Text\",(r,g,b))\n",
        "      print(\"type\", type(this_prompt))\n",
        "      draw.text((0, 0),' '.join(this_prompt),(255,255,255),font=font)            \n",
        "      # cv2.putText(result, text=key+ \": \" + str(raw_value), org=(10,60+(dict_index*30)),\n",
        "      #     fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, color=(255,255,255),\n",
        "      #     thickness=2, lineType=cv2.LINE_AA)\n",
        "      # plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "      # cv2.imwrite(f\"{outDirPath}/{folder}/{label_folder}/{image_name}\", result)\n",
        "      \n",
        "      \n",
        "    if unique_prompt:\n",
        "      pil_image.save(f\"{outDirPath}/{folder}/{unique_prompt_folder}/{image_name}\")\n",
        "      \n",
        "    else:\n",
        "      pil_image.save(f\"{outDirPath}/{folder}/{prompt_folder}/{image_name}\")\n",
        "\n",
        "    \n",
        "  for video_index in range(4):\n",
        "    if video_index == 0:\n",
        "      if not make_regular_video:\n",
        "        continue\n",
        "      image_path = f\"{outDirPath}/{folder}/{folder}({run})_%04d.png\"\n",
        "      filepath = f\"{outDirPath}/{folder}/{folder}({run}).mp4\"\n",
        "    if video_index == 1:\n",
        "      if not make_info_video:\n",
        "        continue\n",
        "      fps = 1\n",
        "      isExist = os.path.exists(f\"{outDirPath}/{folder}/{label_folder}\")\n",
        "      if not isExist:\n",
        "        os.makedirs(f\"{outDirPath}/{folder}/{label_folder}\")\n",
        "      image_count = 0\n",
        "      for index,imagename in enumerate(os.listdir(f\"{outDirPath}/{folder}\")):\n",
        "          if imagename.endswith(\".png\"):\n",
        "            add_padding_to_image(imagename,image_count)\n",
        "            image_count += 1\n",
        "      image_path = f\"{outDirPath}/{folder}/{label_folder}/{folder}({run})_%04d.png\"\n",
        "      filepath = f\"{outDirPath}/{folder}/{folder}({run})_info.mp4\"\n",
        "    if video_index == 2: \n",
        "      if not make_prompt_overlay_video:\n",
        "        continue\n",
        "      isExist = os.path.exists(f\"{outDirPath}/{folder}/{prompt_folder}\")\n",
        "      if not isExist:\n",
        "        os.makedirs(f\"{outDirPath}/{folder}/{prompt_folder}\")\n",
        "      image_count = 0\n",
        "      for index,imagename in enumerate(os.listdir(f\"{outDirPath}/{folder}\")):\n",
        "          if imagename.endswith(\".png\"):\n",
        "            add_prompt_to_image(imagename,image_count,unique_prompt = False)\n",
        "            image_count += 1\n",
        "      image_path = f\"{outDirPath}/{folder}/{prompt_folder}/{folder}({run})_%04d.png\"\n",
        "      filepath = f\"{outDirPath}/{folder}/{folder}({run})_prompts.mp4\"  \n",
        "    if video_index == 3: \n",
        "      if not make_unique_prompt_overlay_video:\n",
        "        continue\n",
        "      isExist = os.path.exists(f\"{outDirPath}/{folder}/{unique_prompt_folder}\")\n",
        "      if not isExist:\n",
        "        os.makedirs(f\"{outDirPath}/{folder}/{unique_prompt_folder}\")\n",
        "      image_count_unique = 0\n",
        "      for index,imagename in enumerate(os.listdir(f\"{outDirPath}/{folder}\")):\n",
        "          print('loopin', index)\n",
        "          if imagename.endswith(\".png\"):\n",
        "            add_prompt_to_image(imagename,image_count_unique,unique_prompt = True)\n",
        "            image_count_unique += 1\n",
        "      image_path = f\"{outDirPath}/{folder}/{unique_prompt_folder}/{folder}({run})_%04d.png\"\n",
        "      filepath = f\"{outDirPath}/{folder}/{folder}({run})_unique_prompts.mp4\"\n",
        "    cmd = [\n",
        "        'ffmpeg',\n",
        "        '-y',\n",
        "        '-vcodec',\n",
        "        'png',\n",
        "        '-r',\n",
        "        str(fps),\n",
        "        '-start_number',\n",
        "        str(init_frame),\n",
        "        '-i',\n",
        "        image_path,\n",
        "        '-frames:v',\n",
        "        str(last_frame+1),\n",
        "        '-c:v',\n",
        "        'libx264',\n",
        "        '-vf',\n",
        "        f'fps={fps}',\n",
        "        '-pix_fmt',\n",
        "        'yuv420p',\n",
        "        '-crf',\n",
        "        '17',\n",
        "        '-preset',\n",
        "        'veryslow',\n",
        "        filepath\n",
        "    ]\n",
        "    print('video_index1', video_index)\n",
        "    process = subprocess.Popen(cmd, cwd=f'{batchFolder}', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "    try:\n",
        "      if process.returncode != 0:\n",
        "          print(stderr)\n",
        "          raise RuntimeError(stderr)\n",
        "          #delete all images from label folder\n",
        "          if video_index == 1:\n",
        "            for file in os.listdir(f\"{outDirPath}/{folder}/{label_folder}\"):\n",
        "              if file.endswith(\".png\"):\n",
        "                os.remove(f\"{outDirPath}/{folder}/{label_folder}/{file}\")\n",
        "      else:\n",
        "          print('video_index2', video_index)\n",
        "          print(\"The video is ready and saved to the images folder\")\n",
        "          #delete all images from label folder\n",
        "          if video_index == 1:\n",
        "            for file in os.listdir(f\"{outDirPath}/{folder}/{label_folder}\"):\n",
        "              if file.endswith(\".png\"):\n",
        "                os.remove(f\"{outDirPath}/{folder}/{label_folder}/{file}\")\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    # if view_video_in_cell:\n",
        "    #     mp4 = open(filepath,'rb').read()\n",
        "    #     data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    #     display.HTML(f'<video width=400 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>')\n",
        "  \n",
        "print('text_prompts_end', text_prompts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YoTrbE8Fl8B"
      },
      "source": [
        "## 11. FILM Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O8Y_lB9pGK1"
      },
      "outputs": [],
      "source": [
        "# This top bit only neneds to be run once\n",
        "# ...may as well set up the notebook to \n",
        "# assume the user is only going to run all\n",
        "# of setup once.\n",
        "\n",
        "\n",
        "##############################\n",
        "\n",
        "\n",
        "\n",
        "###################\n",
        "def predict_number_of_interpolation_frames(frame_number,interpolation_number):\n",
        "    for i in range(interpolation_number):\n",
        "        prediction_number=(frame_number)*2-1\n",
        "        frame_number=prediction_number\n",
        "    return prediction_number\n",
        "print(predict_number_of_interpolation_frames(2,17))\n",
        "\n",
        "# Optionally Mount GDrive\n",
        "frames_dir = f'{outDirPath}/{folder}'\n",
        "if frames_to_interpolate == \"unique prompts\":\n",
        "  frames_dir = f'{outDirPath}/{folder}/{unique_prompt_folder}'\n",
        "\n",
        "number_of_interpolated_frames = 0\n",
        "\n",
        "if os.path.exists(frames_dir+'/interpolated_frames'):\n",
        "  number_of_interpolated_frames=len(os.listdir(frames_dir+'/interpolated_frames'))\n",
        "  print('real_frame_number', number_of_interpolated_frames)\n",
        "#/content/drive/MyDrive/AI/Disco_Diffusion/images_out/testing_many/unique_prompt/interpolated_frames\n",
        "numbers_of_frames_to_be_interpolated=0\n",
        "for imagename in os.listdir(frames_dir):\n",
        "    print(imagename)\n",
        "    if imagename.endswith(\".png\"):\n",
        "      print(\"has png\")\n",
        "      numbers_of_frames_to_be_interpolated += 1\n",
        "should_do_interpolation=False\n",
        "if predict_number_of_interpolation_frames(numbers_of_frames_to_be_interpolated,number_of_interpolations) > number_of_interpolated_frames:\n",
        "    should_do_interpolation=True\n",
        "print(\"predict_number_of_interpolation_frames(numbers_of_frames_to_be_interpolated,number_of_interpolations\",predict_number_of_interpolation_frames(numbers_of_frames_to_be_interpolated,number_of_interpolations))\n",
        "print(\"number_of_interpolated_frames\",number_of_interpolated_frames)\n",
        "if make_interpolation_video and should_do_interpolation:\n",
        "  interpolation_start_time = time.time()\n",
        "  from pathlib import Path\n",
        "  import os\n",
        "\n",
        "  drive_mounted = False\n",
        "  gdrive_fpath = '.'\n",
        "  local_path = '/content/'\n",
        "\n",
        "\n",
        "  if mount_gdrive and not drive_mounted:\n",
        "      from google.colab import drive\n",
        "     \n",
        "\n",
        "     \n",
        "      gdrive_fpath = str(Path(gdrive_mountpoint) / gdrive_subdirectory)\n",
        "      try:\n",
        "          if not mounted_at_beginning:\n",
        "            drive.mount(gdrive_mountpoint, force_remount = True)\n",
        "          !mkdir -p {gdrive_fpath}\n",
        "          %cd {gdrive_fpath}\n",
        "          local_path = gdrive_fpath\n",
        "          drive_mounted = True\n",
        "      except OSError:\n",
        "          print(\n",
        "              \"\\n\\n-----[PYTTI-TOOLS]-------\\n\\n\"\n",
        "              \"If you received a scary OSError and your drive\"\n",
        "              \" was already mounted, ignore it.\"\n",
        "              \"\\n\\n-----[PYTTI-TOOLS]-------\\n\\n\"\n",
        "              )\n",
        "          raise\n",
        "\n",
        "  #####################\n",
        "\n",
        "  # Perform rest of setup\n",
        "\n",
        "  if not Path('./frame-interpolation').exists():\n",
        "      !git clone https://github.com/pytti-tools/frame-interpolation\n",
        "\n",
        "  try:\n",
        "      import frame_interpolation\n",
        "  except ModuleNotFoundError:\n",
        "      %pip install -r ./frame-interpolation/requirements_colab.txt\n",
        "      %pip install ./frame-interpolation\n",
        "\n",
        "  #url = \"https://drive.google.com/drive/folders/1GhVNBPq20X7eaMsesydQ774CgGcDGkc6?usp=sharing\"\n",
        "  share_id = \"1GhVNBPq20X7eaMsesydQ774CgGcDGkc6\"\n",
        "\n",
        "  if not (Path(local_path) / 'saved_model').exists():\n",
        "      !pip install --upgrade gdown\n",
        "      !gdown --folder {share_id}\n",
        "\n",
        "  # create default frame\n",
        "  !mkdir -p frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz3jjuY2QHGw"
      },
      "source": [
        "### Record Interpolation Observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HghLSyKrQHGw"
      },
      "outputs": [],
      "source": [
        "#chooose a random number up to a billion\n",
        "interpolation_random_number = random.randint(0,10000000000)\n",
        "\n",
        "\n",
        "this_runs_interpolation_observation_dict={  \n",
        "    \"gpu\":gpu_name,\n",
        "    \"width\":width_height[0],\n",
        "    \"height\": width_height[1],\n",
        "    \"frames\": numbers_of_frames_to_be_interpolated,\n",
        "    \"interp_num\": number_of_interpolations,\n",
        "    \"status\": \"failed\",\n",
        "}\n",
        "with open(\"/content/drive/MyDrive/AI/Pop_Diffusion/interpolation_observations.txt\") as f:\n",
        "    full_interpolation_observation_dict = json.load(f)\n",
        "full_interpolation_observation_dict[batch_name+\"*\"+str(interpolation_random_number)]=this_runs_interpolation_observation_dict\n",
        "with open(\"/content/drive/MyDrive/AI/Pop_Diffusion/interpolation_observations.txt\", \"w+\") as ant: \n",
        "    json.dump(full_interpolation_observation_dict, ant, ensure_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJx8tRJuf9YK"
      },
      "source": [
        " ### 11.2. Interpolate!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6z4PbBHpWC2"
      },
      "outputs": [],
      "source": [
        "if make_interpolation_video and should_do_interpolation:\n",
        "  #@markdown\n",
        "  #@markdown Results will be saved in a subdirectory of `frames_dir` named \"interpolated_frames\"\n",
        "\n",
        "  from loguru import logger\n",
        "\n",
        "  logger.info(\"Beginning interpolation...\")\n",
        "  \n",
        "  if output_video:\n",
        "    !python -m frame_interpolation.eval.interpolator_cli \\\n",
        "        --model_path ./saved_model \\\n",
        "        --pattern {frames_dir} \\\n",
        "        --fps {output_video_fps} \\\n",
        "        --times_to_interpolate {number_of_interpolations} \\\n",
        "        --output_video\n",
        "        \n",
        "\n",
        "  else:\n",
        "      !python -m frame_interpolation.eval.interpolator_cli \\\n",
        "        --model_path ./saved_model \\\n",
        "        --pattern {frames_dir} \\\n",
        "\n",
        "  logger.info(\"Interpolation comlpete.\")\n",
        "\n",
        "  while not os.path.exists(f'{frames_dir}/interpolated.mp4'):\n",
        "    time.sleep(1)\n",
        "\n",
        "  if os.path.isfile(f'{frames_dir}/interpolated.mp4'):\n",
        "      os.rename(f'{frames_dir}/interpolated.mp4', f'{frames_dir}/{folder}({run})_interp.mp4')\n",
        "  else:\n",
        "      raise ValueError(\"%s isn't a file!\" % file_path)\n",
        "\n",
        "  interpolation_end_time = time.time()\n",
        "  interpolation_duration = interpolation_end_time - interpolation_start_time\n",
        "  #add interpolation duration to stats dictionary\n",
        "  stats['interpolation_duration'] = interpolation_duration\n",
        "  with open(f'{batchFolder}/{batch_name}({batchNum})_stats.txt', 'w') as fp:\n",
        "    json.dump(stats, fp, ensure_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UonyNSZuQHGx"
      },
      "source": [
        "### Update Interpolation Observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzdS1JWIQHGx"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open(\"/content/drive/MyDrive/AI/Pop_Diffusion/interpolation_observations.txt\") as f:\n",
        "    full_interpolation_observation_dict = json.load(f)\n",
        "full_interpolation_observation_dict[batch_name+\"*\"+str(interpolation_random_number)][\"status\"]=\"success\"\n",
        "with open(\"/content/drive/MyDrive/AI/Pop_Diffusion/interpolation_observations.txt\", \"w+\") as ant: \n",
        "    json.dump(full_interpolation_observation_dict, ant, ensure_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zILrbsCSf9YL"
      },
      "source": [
        "## 12. Create gist "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNEJCPjkf9YL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "HTTP Reuests has following parameters: \n",
        "1)Request URL \n",
        "2)Header Fields\n",
        "3)Parameter \n",
        "4)Request body\n",
        "'''\n",
        "#!/usr/bin/env python\n",
        "\n",
        "if should_create_gist:\n",
        "\n",
        "    import requests\n",
        "    import json\n",
        "\n",
        "    GITHUB_API=\"https://api.github.com\"\n",
        "    #open \"/content/drive/MyDrive/AI/youtube/gist_api.json\" and get the string from it\n",
        "    API_TOKEN = '' #this api is in github_api.txt\n",
        "    with open(\"/content/drive/MyDrive/AI/gist/gist_api.txt\") as f:\n",
        "        API_TOKEN = f.read().strip()\n",
        "    #form a request URL\n",
        "    url=GITHUB_API+\"/gists\"\n",
        "    print (\"Request URL: %s\"%url)\n",
        "    #print headers,parameters,payload\n",
        "    headers={'Authorization':'token %s'%API_TOKEN}\n",
        "    params={'scope':'gist'}\n",
        "    #with open(settings) as file:\n",
        "    #   lines = str(file.readlines())\n",
        "    def create_gist(gist_type):\n",
        "        \n",
        "        if gist_type == \"prompt_generator\":\n",
        "          with open(f\"{batchFolder}/{batch_name}({batchNum})_{gist_type}.txt\") as f:\n",
        "            payload_content = f.read()\n",
        "        else:\n",
        "          file=open(f\"{batchFolder}/{batch_name}({batchNum})_{gist_type}.txt\")\n",
        "          lines=json.load(file)\n",
        "          payload_content=json.dumps(lines,indent=4)\n",
        "        payload={\"description\":\"GIST created by python code\",\"public\":True,\"files\":{name:{\"content\":payload_content}}}\n",
        "        #make a requests\n",
        "        res=requests.post(url,headers=headers,params=params,data=json.dumps(payload))\n",
        "        #print response --> JSON\n",
        "        j=json.loads(res.text)\n",
        "        print('j',j)\n",
        "        # Print created GIST's details\n",
        "        print (\"Gist URL : %s\"%(j['url']))\n",
        "        print (\"GIST ID: %s\"%(j['id']))\n",
        "        gist_URL=str(\"https://gist.github.com/\"+github_username+\"/%s\"%(j['id']))\n",
        "        print(gist_type , gist_URL)\n",
        "        return gist_URL\n",
        "    settings_gist_URL=create_gist(\"settings\")\n",
        "    stats_gist_URL=create_gist(\"stats\")\n",
        "    prompt_generator_gist_URL=create_gist(\"prompt_generator\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yfmv5LFf9YL"
      },
      "source": [
        "## 13. Get Music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--KWF09Ff9YL"
      },
      "outputs": [],
      "source": [
        "#check if there are any mp3 in frames_dir\n",
        "import os\n",
        "def check_mp3_file():\n",
        "    mp3_file_exists=False\n",
        "    for file in os.listdir(frames_dir):\n",
        "        if file.endswith(\".mp3\"):\n",
        "            mp3_file_exists=True\n",
        "            break\n",
        "    return mp3_file_exists\n",
        "mp3_name=\"\"\n",
        "music_credits=\"\"\n",
        "\n",
        "if check_mp3_file():\n",
        "    for file in os.listdir(frames_dir):\n",
        "        if file.endswith(\".mp3\"):\n",
        "            mp3_name=file\n",
        "            break\n",
        "    music_credits_filename=mp3_name.replace('.mp3','')+\"_credits.txt\"\n",
        "    with open(frames_dir+'/'+music_credits_filename) as f:\n",
        "        music_credits=f.read()\n",
        "else:\n",
        "    !pip install selenium\n",
        "    !apt-get update # to update ubuntu to correctly run apt install\n",
        "    !apt install chromium-chromedriver\n",
        "    !pip3 install  pyvirtualdisplay selenium webdriver_manager  > /dev/null\n",
        "    !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "   \n",
        "    import sys\n",
        "    sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver.common.by import By\n",
        "    from selenium.webdriver.support.ui import WebDriverWait\n",
        "    from selenium.webdriver.support import expected_conditions as EC\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    import time\n",
        "    import random\n",
        "\n",
        "    if 'http://dig.ccmixter.org' not in ccmixter_search_url_or_keyword:\n",
        "        ccmixter_search_url_or_keyword=\"http://dig.ccmixter.org/search?searchp=\"+ccmixter_search_url_or_keyword\n",
        "\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument(\"window-size=1400,600\")\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome('chromedriver',chrome_options=options)\n",
        "\n",
        "    driver.get(ccmixter_search_url_or_keyword)\n",
        "    original_html = driver.page_source\n",
        "    time.sleep(2)\n",
        "    #print(\"original_html\", original_html)\n",
        "    number_of_songs_at_page=str(original_html).split('1 - ')[1].split(' of ')[0]\n",
        "    print (\"number_of_songs_at_page\", number_of_songs_at_page)\n",
        "    x=str(original_html).split('1 - '+number_of_songs_at_page+' of ')[1].split('</div><label')\n",
        "    number_of_songs = x[0].replace(',','')\n",
        "    #print(\"number_of_songs\", number_of_songs)\n",
        "    url_with_all_songs=ccmixter_search_url_or_keyword.replace('search?',\"search?limit=\"+number_of_songs+\"&\")\n",
        "    #url_with_all_songs=\"http://dig.ccmixter.org/search?limit=\"+number_of_songs+\"&searchp=instrumental\"\n",
        "    #new_url=\"http://dig.ccmixter.org/search?limit=10&searchp=instrumental\"\n",
        "    #print(\"url_with_all_songs\", url_with_all_songs)\n",
        "\n",
        "    driver.get(url_with_all_songs)\n",
        "    all_songs_html = driver.page_source\n",
        "    time.sleep(10)\n",
        "    y=str(all_songs_html).split('<a class=\"upload-link song-title\" href=\"/files/')\n",
        "    song_list = []\n",
        "    for i in range(1,len(y)): #use split to get all song names\n",
        "        song_list.append(y[i].split('</a> <a class=\"people-link artist-name light-color\" ')[0].split('/')[1].split('\">')[0])\n",
        "    random_song=random.sample(song_list,1)\n",
        "    print(\"random_song: \", random_song)\n",
        "    random_song_id = random_song[0].split('\"')[0]\n",
        "\n",
        "\n",
        "    #button_element = driver.find_element_by_xpath(\"//button[@class='btn btn-warning  btn-lg' and @data-reactid='.0.1.6.0.2.0.0.1.$\"+random_song_id+\".2.0']\")\n",
        "\n",
        "    button_element = driver.find_element(by=By.XPATH, value=\"//button[@class='btn btn-warning  btn-lg' and @data-reactid='.0.1.6.0.2.0.0.1.$\"+random_song_id+\".2.0']\")\n",
        "    WebDriverWait(driver,10).until(EC.element_to_be_clickable(button_element)).click()\n",
        "    driver.switch_to.active_element\n",
        "    time.sleep(2)\n",
        "    html_of_song_popup = driver.page_source\n",
        "    time.sleep(12)\n",
        "    #open text file\n",
        "    text_file = open(\"data.txt\", \"w\")\n",
        "    \n",
        "    #write string to file\n",
        "    text_file.write(html_of_song_popup)\n",
        "    \n",
        "\n",
        "\n",
        "    #close file\n",
        "    text_file.close()\n",
        "    music_credits=str(html_of_song_popup).split('textarea readonly')[1].split('\">')[1].split('</textarea>')[0]\n",
        "    print(music_credits)\n",
        "\n",
        "\n",
        "\n",
        "    mp3_url = 'http://ccmixter.org/' + str(html_of_song_popup).split('.mp3\" download=\"\"')[0].split('http://ccmixter.org/')[1] + '.mp3'\n",
        "    print(mp3_url)\n",
        "    mp3_name = \"None\"\n",
        "    if 'content' in mp3_url:\n",
        "        mp3_name = mp3_url.split('http://ccmixter.org/content/')[1].replace('/','_')\n",
        "    elif 'contests' in mp3_url:\n",
        "        mp3_name = mp3_url.split('http://ccmixter.org/contests/')[1].replace('/','_')\n",
        "    with open(frames_dir+'/'+mp3_name.split('.mp3')[0]+\"_credits.txt\", \"w\") as text_file:\n",
        "        text_file.write(music_credits)\n",
        "        \n",
        "    import os\n",
        "    import requests\n",
        "    full_save_path = os.path.join(os.getcwd())\n",
        "    base_path = os.path.dirname(full_save_path)\n",
        "    if not os.path.isdir(base_path):\n",
        "        os.makedirs(base_path)\n",
        "\n",
        "    r = requests.get(mp3_url, headers={'referer': 'http://ccmixter.org/'})\n",
        "    if r.ok:\n",
        "        with open(frames_dir+'/'+mp3_name, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "    else:\n",
        "        r.raise_for_status()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80Q_jsTQQHGz"
      },
      "source": [
        "## Manual Music Selection "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyq6tynfQHGz"
      },
      "outputs": [],
      "source": [
        "# !pip install jp_proxy_widget\n",
        "# from jupyter_bbox_widget import BBoxWidget\n",
        "if not automatically_choose_music:\n",
        "  !pip install ipython_blocking\n",
        "  import ipython_blocking\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import Audio\n",
        "from IPython.core.display import display\n",
        "def audio_player():\n",
        "  display(Audio(frames_dir+'/'+mp3_name,autoplay=True))\n",
        "# import os\n",
        "# import json\n",
        "# import base64\n",
        "\n",
        "\n",
        "def continue_button_pressed(args):\n",
        "  print(\"continur\")\n",
        "def play_music_button_pressed(args):\n",
        "  print(\"music ply\")\n",
        "def delete_song_button_pressed(args):\n",
        "  print(\"choose\")\n",
        "  global mp3_name\n",
        "  os.remove(frames_dir+'/'+mp3_name)\n",
        "  os.remove(frames_dir+'/'+mp3_name.split('.mp3')[0]+\"_credits.txt\")\n",
        "  # dir = os.path.dirname(os.path.realpath(\"_file_\"))\n",
        "  # print(dir)\n",
        "  # dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "  # filename = get('http://\n",
        "\n",
        "continue_button = widgets.Button(\n",
        "    description='Continue',\n",
        "    disabled=False,\n",
        "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click me',\n",
        "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        ")\n",
        "continue_button\n",
        "\n",
        "delete_song_button = widgets.Button(\n",
        "    description='New Song',\n",
        "    disabled=False,\n",
        "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Play Music',\n",
        "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        ")\n",
        "delete_song_button\n",
        "continue_button.on_click(continue_button_pressed)\n",
        "# play_music_button.on_click(play_music_button_pressed) \n",
        "delete_song_button.on_click(delete_song_button_pressed)\n",
        "items = [continue_button,delete_song_button]\n",
        "box = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(2, 600px)\"), disabled =False)\n",
        "\n",
        "\n",
        "if automatically_choose_music:\n",
        "  box.layout.visibility=\"hidden\"\n",
        "else:\n",
        "  box.layout.visibility==\"visible\"\n",
        "  audio_player()\n",
        "box\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9tZSQq3QHG0"
      },
      "outputs": [],
      "source": [
        "if not automatically_choose_music:\n",
        "  %block continue_button"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XefZjyH2f9YM"
      },
      "source": [
        "## 13. Video functions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABW6iNInf9YM"
      },
      "outputs": [],
      "source": [
        "def get_video_length(video_name):\n",
        "    duration = subprocess.check_output(['ffprobe', '-i', video_name, '-show_entries', 'format=duration', '-v', 'quiet', '-of', 'csv=%s' % (\"p=0\")])\n",
        "    duration2=str(duration).replace(\"b'\",\"\")\n",
        "    duration2=str(duration2).replace(\"\\\\r\\\\n'\",\"\")\n",
        "    duration2=str(duration2).replace(\"\\\\n'\",\"\")\n",
        "    intduration=round(float(duration2))+1\n",
        "    return intduration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJdAszSBf9YM"
      },
      "source": [
        "## 14. Change Speed of a Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jKSUJYGf9YM"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "#vf = f'select=not(mod(n\\,{extract_nth_frame}))'\n",
        "\n",
        "current_video_length=get_video_length(f'{frames_dir}/{folder}({run})_interp.mp4')\n",
        "length_multiplier=str(finished_video_length/current_video_length)\n",
        "vf=\"setpts=\"+length_multiplier+\"*PTS\"\n",
        "# subprocess.run(['ffmpeg', '-i', f'{video}', '-vf', f'{vf}', f'{speed_changed_video_name}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "#   #!ffmpeg -i {video_init_path} -vf {vf} -vsync vfr -q:v 2 -loglevel error -stats {videoFramesFolder}/%04d.jpg\n",
        "\n",
        "cmd = ['ffmpeg', '-i',f'{frames_dir}/{folder}({run})_interp.mp4', '-vf', f'{vf}', f'{frames_dir}/{batch_name}({batchNum})_speed_changed_output.mp4']\n",
        "\n",
        "process = subprocess.Popen(cmd, cwd=\"./\", stdout=subprocess.PIPE, stderr=subprocess.PIPE) \n",
        "stdout, stderr = process.communicate()\n",
        "try:\n",
        "  if process.returncode != 0:\n",
        "      print(stderr)\n",
        "      raise RuntimeError(stderr)\n",
        "\n",
        "  else:\n",
        "\n",
        "      print(\"The video is ready and saved to the images folder\")\n",
        "      #delete all images from label folder\n",
        "\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZg-y_4Cf9YM"
      },
      "source": [
        "## 15. Add audio to video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP5mcGwrf9YM"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile(f'{frames_dir}/{batch_name}({batchNum})_finished_video.mp4'):\n",
        "    !pip install ffmpeg-python\n",
        "    import ffmpeg\n",
        "    import sys\n",
        "    duration = subprocess.check_output(['ffprobe', '-i', f'{frames_dir}/{batch_name}({batchNum})_speed_changed_output.mp4', '-show_entries', 'format=duration', '-v', 'quiet', '-of', 'csv=%s' % (\"p=0\")])\n",
        "    duration2=str(duration).replace(\"b'\",\"\")\n",
        "    duration2=str(duration2).replace(\"\\\\r\\\\n'\",\"\")\n",
        "    duration2=str(duration2).replace(\"\\\\n'\",\"\")\n",
        "    intduration=round(float(duration2))+1\n",
        "\n",
        "    print(intduration)\n",
        "    input_video = ffmpeg.input(f'{frames_dir}/{batch_name}({batchNum})_speed_changed_output.mp4')\n",
        "    input_audio = ffmpeg.input(frames_dir+'/'+mp3_name, t=intduration)\n",
        "    #check to see if finished_video.mp4 exists\n",
        "\n",
        "\n",
        "\n",
        "    ffmpeg.concat(input_video, input_audio, v=1, a=1).output(f'{frames_dir}/{batch_name}({batchNum})_finished_video.mp4').run()\n",
        "    #how to delete a video file\n",
        "    os.remove(f'{frames_dir}/{batch_name}({batchNum})_speed_changed_output.mp4')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#subprocess.run(ffmpeg -i \"video.mp4\" -i \"song.mp3\" -shortest outPutFile.mp4)\n",
        "\n",
        "#ffmpeg -i \"video.mp4\" -i \"song.mp3\" -shortest outPutFile.mp4\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMN9F6y9QHG2"
      },
      "source": [
        "## Upscale Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGgoRb3LQHG2"
      },
      "outputs": [],
      "source": [
        "cmd = ['ffmpeg', '-i',f'{frames_dir}/{folder}({run})_finished_video.mp4', '-vf', 'scale=-1:1080:flags=lanczos', '-c:v','libx264', '-crf','21',f'{frames_dir}/{batch_name}({batchNum})_upscaled.mp4']\n",
        "#ffmpeg -i colortree.mp4 -vf scale=-1:1080:flags=lanczos -c:v libx264 -crf 21 out.mp4\n",
        "process = subprocess.Popen(cmd, cwd=\"./\", stdout=subprocess.PIPE, stderr=subprocess.PIPE) \n",
        "stdout, stderr = process.communicate()\n",
        "try:\n",
        "  if process.returncode != 0:\n",
        "      print(stderr)\n",
        "      raise RuntimeError(stderr)\n",
        "\n",
        "  else:\n",
        "\n",
        "      print(\"The video is ready and saved to the images folder\")\n",
        "      #delete all images from label folder\n",
        "\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eayX5o4Gf9YN"
      },
      "source": [
        "## 16. Create Youtube Video "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71g4pbJif9YN"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/python\n",
        "should_upload_video=True #@param{type,'boolean'}\n",
        "if should_upload_video:\n",
        "  import httplib2\n",
        "  import os\n",
        "  import random\n",
        "  import sys\n",
        "  import time\n",
        "\n",
        "  from apiclient.discovery import build\n",
        "  from apiclient.errors import HttpError\n",
        "  from apiclient.http import MediaFileUpload\n",
        "  from oauth2client.client import flow_from_clientsecrets\n",
        "  from oauth2client.file import Storage\n",
        "  from oauth2client.tools import argparser, run_flow\n",
        "\n",
        "\n",
        "  # Explicitly tell the underlying HTTP transport library not to retry, since\n",
        "  # we are handling retry logic ourselves.\n",
        "  httplib2.RETRIES = 1\n",
        "\n",
        "  # Maximum number of times to retry before giving up.\n",
        "  MAX_RETRIES = 10\n",
        "\n",
        "  # Always retry when these exceptions are raised.\n",
        "  RETRIABLE_EXCEPTIONS = (httplib2.HttpLib2Error, IOError)\n",
        "\n",
        "  # Always retry when an apiclient.errors.HttpError with one of these status\n",
        "  # codes is raised.\n",
        "  RETRIABLE_STATUS_CODES = [500, 502, 503, 504]\n",
        "\n",
        "  # The CLIENT_SECRETS_FILE variable specifies the name of a file that contains\n",
        "  # the OAuth 2.0 information for this application, including its client_id and\n",
        "  # client_secret. You can acquire an OAuth 2.0 client ID and client secret from\n",
        "  # the Google API Console at\n",
        "  # https://console.developers.google.com/.\n",
        "  # Please ensure that you have enabled the YouTube Data API for your project.\n",
        "  # For more information about using OAuth2 to access the YouTube Data API, see:\n",
        "  #   https://developers.google.com/youtube/v3/guides/authentication\n",
        "  # For more information about the client_secrets.json file format, see:\n",
        "  #   https://developers.google.com/api-client-library/python/guide/aaa_client_secrets\n",
        "  CLIENT_SECRETS_FILE = \"/content/drive/MyDrive/AI/youtube/client_secrets.json\"\n",
        "\n",
        "  # This OAuth 2.0 access scope allows an application to upload files to the\n",
        "  # authenticated user's YouTube channel, but doesn't allow other types of access.\n",
        "  YOUTUBE_UPLOAD_SCOPE = \"https://www.googleapis.com/auth/youtube.upload\"\n",
        "  YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "  YOUTUBE_API_VERSION = \"v3\"\n",
        "\n",
        "  # This variable defines a message to display if the CLIENT_SECRETS_FILE is\n",
        "  # missing.\n",
        "  MISSING_CLIENT_SECRETS_MESSAGE = \"Error\"\n",
        "\n",
        "  VALID_PRIVACY_STATUSES = (\"public\", \"private\", \"unlisted\")\n",
        "\n",
        "  class ourargs:\n",
        "    def __init__(self, **kwargs):\n",
        "      self.__dict__.update(kwargs)\n",
        "\n",
        "  def get_authenticated_service(args):\n",
        "    flow = flow_from_clientsecrets(CLIENT_SECRETS_FILE,\n",
        "      scope=YOUTUBE_UPLOAD_SCOPE,\n",
        "      message=MISSING_CLIENT_SECRETS_MESSAGE)\n",
        "\n",
        "    #storage = Storage(\"%s-oauth2.json\" % sys.argv[0])\n",
        "    storage = Storage(\"/content/drive/MyDrive/AI/youtube/upload_video.py-oauth2.json\")\n",
        "    credentials = storage.get()\n",
        "\n",
        "    # if credentials is None or credentials.invalid:\n",
        "    #   credentials = run_flow(flow, storage, args)\n",
        "\n",
        "    return build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
        "      http=credentials.authorize(httplib2.Http()))\n",
        "\n",
        "  def initialize_upload(youtube, options):\n",
        "    tags = None\n",
        "    if options.keywords:\n",
        "      tags = options.keywords.split(\",\")\n",
        "\n",
        "    body=dict(\n",
        "      snippet=dict(\n",
        "        title=options.title,\n",
        "        description=options.description,\n",
        "        tags=tags,\n",
        "        categoryId=options.category\n",
        "      ),\n",
        "      status=dict(\n",
        "        privacyStatus=options.privacyStatus\n",
        "      )\n",
        "    )\n",
        "\n",
        "    # Call the API's videos.insert method to create and upload the video.\n",
        "    insert_request = youtube.videos().insert(\n",
        "      part=\",\".join(body.keys()),\n",
        "      body=body,\n",
        "      # The chunksize parameter specifies the size of each chunk of data, in\n",
        "      # bytes, that will be uploaded at a time. Set a higher value for\n",
        "      # reliable connections as fewer chunks lead to faster uploads. Set a lower\n",
        "      # value for better recovery on less reliable connections.\n",
        "      #\n",
        "      # Setting \"chunksize\" equal to -1 in the code below means that the entire\n",
        "      # file will be uploaded in a single HTTP request. (If the upload fails,\n",
        "      # it will still be retried where it left off.) This is usually a best\n",
        "      # practice, but if you're using Python older than 2.6 or if you're\n",
        "      # running on App Engine, you should set the chunksize to something like\n",
        "      # 1024 * 1024 (1 megabyte).\n",
        "      media_body=MediaFileUpload(options.file, chunksize=-1, resumable=True)\n",
        "    )\n",
        "\n",
        "    resumable_upload(insert_request)\n",
        "\n",
        "  # This method implements an exponential backoff strategy to resume a\n",
        "  # failed upload.\n",
        "  def resumable_upload(insert_request):\n",
        "    response = None\n",
        "    error = None\n",
        "    retry = 0\n",
        "    while response is None:\n",
        "      try:\n",
        "        print (\"Uploading file...\")\n",
        "        status, response = insert_request.next_chunk()\n",
        "        if response is not None:\n",
        "          if 'id' in response:\n",
        "            print (\"Video id '%s' was successfully uploaded.\" % response['id'])\n",
        "          else:\n",
        "            exit(\"The upload failed with an unexpected response: %s\" % response)\n",
        "      except HttpError as e:\n",
        "        if e.resp.status in RETRIABLE_STATUS_CODES:\n",
        "          error = \"A retriable HTTP error %d occurred:\\n%s\" % (e.resp.status,\n",
        "                                                              e.content)\n",
        "        else:\n",
        "          raise\n",
        "      except RETRIABLE_EXCEPTIONS as e:\n",
        "        error = \"A retriable error occurred: %s\" % e\n",
        "\n",
        "      if error is not None:\n",
        "        print (error)\n",
        "        retry += 1\n",
        "        if retry > MAX_RETRIES:\n",
        "          exit(\"No longer attempting to retry.\")\n",
        "\n",
        "        max_sleep = 2 ** retry\n",
        "        sleep_seconds = random.random() * max_sleep\n",
        "        print (\"Sleeping %f seconds and then retrying...\" % sleep_seconds)\n",
        "        time.sleep(sleep_seconds)\n",
        "\n",
        "  if __name__ == '__main__':\n",
        "    # argparser.add_argument(\"--file\", required=True, help=\"Video file to upload\")\n",
        "    # argparser.add_argument(\"--title\", help=\"Video title\", default=\"Test Title\")\n",
        "    # argparser.add_argument(\"--description\", help=\"Video description\",\n",
        "    #   default=\"Test Description\")\n",
        "    # argparser.add_argument(\"--category\", default=\"22\",\n",
        "    #   help=\"Numeric video category. \" +\n",
        "    #     \"See https://developers.google.com/youtube/v3/docs/videoCategories/list\")\n",
        "    # argparser.add_argument(\"--keywords\", help=\"Video keywords, comma separated\",\n",
        "    #   default=\"\")\n",
        "    # argparser.add_argument(\"--privacyStatus\", choices=VALID_PRIVACY_STATUSES,\n",
        "    #   default=VALID_PRIVACY_STATUSES[0], help=\"Video privacy status.\")\n",
        "    # args = argparser.parse_args()\n",
        "\n",
        "    if show_first_prompt_in_youtube_description:\n",
        "      youtube_description=\"Created by Pop Diffusion: (https://github.com/tjthejuggler/Pop-Diffusion)\\n\\n \\\n",
        "        A fork of Disco Diffusion: (https://github.com/alembics/disco-diffusion)\\n\\n \\\n",
        "        Film Interpolation: (https://github.com/google-research/frame-interpolation)\\n\\n \\\n",
        "        First prompt:\\n\"+str(text_prompts[0])+\"\\n\\n \\\n",
        "        Settings:\\n\"+settings_gist_URL+\"\\n\\n \\\n",
        "        Stats:\\n\"+stats_gist_URL+\"\\n\\n \\\n",
        "        Prompt Generator:\\n\"+prompt_generator_gist_URL+\"\\n\\n \\\n",
        "        Music:\\n\"+music_credits \n",
        "    else:\n",
        "      youtube_description=\"Created by Pop Diffusion: (https://github.com/tjthejuggler/Pop-Diffusion)\\n\\n \\\n",
        "        A fork of Disco Diffusion: (https://github.com/alembics/disco-diffusion)\\n\\n \\\n",
        "        Film Interpolation: (https://github.com/google-research/frame-interpolation)\\n\\n \\\n",
        "        Settings:\\n\"+settings_gist_URL+\"\\n\\n \\\n",
        "        Stats:\\n\"+stats_gist_URL+\"\\n\\n \\\n",
        "        Prompt Generator:\\n\"+prompt_generator_gist_URL+\"\\n\\n \\\n",
        "        Music:\\n\"+music_credits\n",
        "                  \n",
        "    #put a variable into a text file\n",
        "    with open(f'{frames_dir}/{batch_name}({batchNum})_youtube_description.txt', \"w\") as text_file:\n",
        "      text_file.write(youtube_description)\n",
        "\n",
        "    args=ourargs(file=f'{frames_dir}/{batch_name}({batchNum})_upscaled.mp4',title=batch_name,description=youtube_description,category=\"22\",keywords=\"Disco Diffusion, AI Art\",privacyStatus=\"private\")\n",
        "    # args.file=video\n",
        "    # args.title=name\n",
        "    # args.description=URL\n",
        "    \n",
        "\n",
        "\n",
        "    if not os.path.exists(args.file):\n",
        "      exit(\"Please specify a valid file using the --file= parameter.\")\n",
        "\n",
        "    youtube = get_authenticated_service(args)\n",
        "    try:\n",
        "      initialize_upload(youtube, args)\n",
        "    except HttpError as e:\n",
        "      print (\"An HTTP error %d occurred:\\n%s\" % (e.resp.status, e.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqJLG343QHG4"
      },
      "source": [
        "### Save settings.txt to the settings_files folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KzvCh11QHG4"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "\n",
        "def clicked(arg):\n",
        "    save_settings(False)\n",
        "    print(\"button has been clicked!\")\n",
        "\n",
        "button_download = widgets.Button(description = 'Save')   \n",
        "button_download.on_click(clicked)\n",
        "display(button_download)\n",
        "\n",
        "#play an mp3 file\n",
        "def play_mp3(mp3_file):\n",
        "    import pyglet\n",
        "    music = pyglet.media.load(mp3_file)\n",
        "    music.play()\n",
        "    pyglet.app.run()\n",
        "    music.stop()\n",
        "    pyglet.app.exit()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "main.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
